{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neg_refcocov002.ipynb Create a false premise referring expressions dataset\n",
    "\n",
    "Create a COCO formatted dataset that uses `gpt-3.5-turbo` to create false premise referring expressions that refer to objects that do not exist in the image. \n",
    "\n",
    "## Types of Modifications\n",
    "\n",
    "We ask GPT to modify the ground truth referring expressions for each image.\n",
    "We categorize each FP according to the type of modification. We have three categories:\n",
    "\n",
    "- Modify the main subject of the sentence. This means changing from one noun or noun phrase to another one. \"A woman...\" -> \"A cat...\"\n",
    "- Modify an attribute of the main subject. \"A tall man...\" -> \"A short man\"\n",
    "- Modify some other portion of the description. This usually means either modifying a spatial relation, or a participatory object that the expression relates somehow to the main subject, or sometimes an attribute of the participatory object.\n",
    "\n",
    "## File Format\n",
    "The referring expressions follow same format as refcoco/refcocog/refcoco+/R-refcoco/etc, i.e., a COCO formatted json file, accompanied by a file with a `.p` extension, which contains the true and false referring expressions. The `.p` file is a python pickle file. These datasets can be loaded using the common `refer.py`, or the `COCO` class in `github.com/GiscardBiamby/cocobetter.git`. Examples can be found later in this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastjsonschema                    2.18.0\n",
      "json5                             0.9.14\n",
      "jsonpointer                       2.4\n",
      "jsons                             1.6.3\n",
      "jsonschema                        4.19.1\n",
      "jsonschema-specifications         2023.7.1\n",
      "pysimdjson                        5.0.2\n",
      "python-json-logger                2.0.7\n",
      "python-lsp-jsonrpc                1.1.1\n",
      "ujson                             5.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list | grep json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import copy\n",
    "import csv\n",
    "import decimal\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import typing\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Set, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as pil_img\n",
    "import regex as re\n",
    "import seaborn as sns\n",
    "import simdjson as json\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO, Ann, Cat, Image, Ref\n",
    "from pycocotools.helpers import CocoClassDistHelper, CocoJsonBuilder\n",
    "from pycocotools.helpers.coco_builder import COCOShrinker\n",
    "from simplediff import diff, string_diff\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# from geo_llm_ret.ref_datasets import build_ref_coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_DIR = Path(\"/shared/gbiamby/data/coco\")\n",
    "IMG_DIR = COCO_DIR / \"val2017\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading refs from '/home/gbiamby/proj/geo-llm-ret/lib/cocobetter/PythonAPI/notebooks/ref_correct/output/ref_seg/refcoco/refs(google_enhanced).p'\n",
      "Loaded 50000 refs\n",
      "loading annotations into memory...\n",
      "Done (t=3.33s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "VALID_SPLITS = {\n",
    "    \"R-refcoco\": [\"unc\"],\n",
    "    \"R-refcoco+\": [\"unc\"],\n",
    "    \"R-refcocog\": [\"umd\"],\n",
    "    \"refclef\": [\"berkeley\", \"unc\"],\n",
    "    \"refcoco\": [\"google\"],\n",
    "    \"refcoco+\": [\"unc\"],\n",
    "    \"refcocog\": [\"google\", \"umd\"],\n",
    "}\n",
    "\n",
    "\n",
    "def build_refcoco(refseg_path: Path, dataset_name: str, split_by: str = None) -> COCO:\n",
    "    assert dataset_name in VALID_SPLITS, dataset_name\n",
    "    if split_by is None:\n",
    "        split_by = VALID_SPLITS[dataset_name][0]\n",
    "    else:\n",
    "        assert split_by.replace(\"_enhanced\", \"\") in VALID_SPLITS[dataset_name]\n",
    "    coco = COCO(\n",
    "        refseg_path / dataset_name / \"instances.json\",\n",
    "        is_ref_dataset=True,\n",
    "        dataset_name=dataset_name,\n",
    "        split_by=split_by,\n",
    "    )\n",
    "    return coco\n",
    "\n",
    "\n",
    "IMG_DIR = Path(\"/shared/gbiamby/data/coco/train2014\")\n",
    "PROJ_ROOT = Path(\"../../../../../\").resolve()\n",
    "assert PROJ_ROOT.exists()\n",
    "# REFSEG_DIR = Path(\"/shared/gbiamby/data/refer_seg\")\n",
    "REFSEG_DIR = Path(\"output/ref_seg\")\n",
    "# refcoco = build_refcoco(REFSEG_DIR, \"refcocog\", \"google_enhanced\")\n",
    "refcoco = build_refcoco(REFSEG_DIR, \"refcoco\", \"google_enhanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_results_dir = (\n",
    "    PROJ_ROOT / \"output/refcoco_google-gb006_remove_guidelines-gpt-3.5-turbo\"\n",
    ")\n",
    "assert api_results_dir.exists(), str(api_results_dir)\n",
    "assert api_results_dir.is_dir(), str(api_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19994/19994 [00:00<00:00, 22008.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19994 responses from /home/gbiamby/proj/geo-llm-ret/output/refcoco_google-gb006_remove_guidelines-gpt-3.5-turbo\n",
      "Example response: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'api_response': {'choices': [{'finish_reason': 'stop',\n",
       "    'index': 0,\n",
       "    'message': {'content': 'Altered Descriptions: [\"woman behind guy in black\", \"spectator behind the guy in white\", \"audience in back\", \"guy in white trying to catch banana\", \"woman in front\", \"woman in white\"]',\n",
       "     'role': 'assistant'}}],\n",
       "  'created': 1699353174,\n",
       "  'id': 'chatcmpl-8IDg6XzsduM14vNplz1QPGIyJbtIi',\n",
       "  'model': 'gpt-3.5-turbo-0613',\n",
       "  'object': 'chat.completion',\n",
       "  'usage': {'completion_tokens': 47,\n",
       "   'prompt_tokens': 511,\n",
       "   'total_tokens': 558}},\n",
       " 'image_id': 100012,\n",
       " 'request_info': {'ann_ids': [521105, 501646],\n",
       "  'image_id': 100012,\n",
       "  'ref_ids': [41276, 41277],\n",
       "  'sent_ids': [[117346, 117347, 117348], [117349, 117350, 117351]],\n",
       "  'sentences': ['man behind guy in white',\n",
       "   'player behind the guy in white',\n",
       "   'player in back',\n",
       "   'guy in white trying to catch frisbee',\n",
       "   'man in front',\n",
       "   'man in white']}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_api_responses(api_results_dir: Path, max_results: int = None) -> list[dict]:\n",
    "    response_files = sorted(api_results_dir.glob(\"responses/img_id_*.json\"))\n",
    "    if max_results is not None and max_results > 0:\n",
    "        response_files = response_files[:max_results]\n",
    "    results = []\n",
    "    for f in tqdm(response_files):\n",
    "        with open(f, \"r\", encoding=\"utf-8\") as json_file:\n",
    "            result = json.load(json_file)\n",
    "            results.append(result)\n",
    "\n",
    "    print(f\"Loaded {len(results)} responses from {api_results_dir}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "api_responses = load_api_responses(api_results_dir)\n",
    "print(\"Example response: \")\n",
    "display(api_responses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check Quality of the API Results - Filter Bad Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19994/19994 [00:04<00:00, 4949.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19994 responses from /home/gbiamby/proj/geo-llm-ret/output/refcoco_google-gb006_remove_guidelines-gpt-3.5-turbo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19994/19994 [00:00<00:00, 30355.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_results:  19994\n",
      "has_fpsents_count:  19897\n",
      "Found 6428 warnings\n",
      "Found 30 errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_result(image_id: int, reply: str, warnings, errors) -> list[str]:\n",
    "    def parse_result_main(reply: str):\n",
    "        # print(f\"ChatGPT Reply: \\n\\t{reply}\")\n",
    "        matches = re.match(\n",
    "            # '.*Descriptions:*\\\\s*(\\\\(.{1,} sentence[s]{0,1}\\\\):){0,1}\\\\s*(?P<descriptions>\\\\[\\\\\".*\\\\\"\\\\])',\n",
    "            '.*Description[s]{0,1}:*\\\\s*(\\\\(.{1,} sentence[s]{0,1}\\\\):){0,1}\\\\s*\\\\[{0,1}(?P<descriptions>\\\\\".*\\\\\")\\\\]{0,1}',\n",
    "            reply,\n",
    "            re.MULTILINE | re.DOTALL,\n",
    "        )\n",
    "        if matches is None:\n",
    "            return None\n",
    "        list_str = matches.group(\"descriptions\")\n",
    "        if not list_str.startswith(\"[\"):\n",
    "            list_str = \"[\" + list_str\n",
    "        if not list_str.endswith(\"]\"):\n",
    "            list_str = list_str + \"]\"\n",
    "        new_sents = ast.literal_eval(list_str)\n",
    "        # print(\"New Sents: \", new_sent)\n",
    "        if new_sents is None:\n",
    "            errors.append(\n",
    "                {\n",
    "                    \"image_id\": image_id,\n",
    "                    \"msg\": f\"No FP sents found (fp_sents is None)\",\n",
    "                    \"raw_reply\": reply,\n",
    "                }\n",
    "            )\n",
    "            return None\n",
    "        return new_sents\n",
    "\n",
    "    def parse_result_multiline_list(reply: str):\n",
    "        matches = re.match(\n",
    "            '(?:.*Description[s]{0,1}:[ ]*(\\\\(.+ sentence[s]{0,1}\\\\):){0,1})\\\\n*?(?P<descriptions>\\\\n\\\\d\\\\.[ ]*\\\\\"[^\\\\n\\\\\"]+\\\\\")+',\n",
    "            reply,\n",
    "            re.MULTILINE | re.DOTALL,\n",
    "        )\n",
    "        if matches is None:\n",
    "            return None\n",
    "        captures = matches.capturesdict()\n",
    "        if (captures is None or len(captures) == 0) or (\n",
    "            captures is not None and \"descriptions\" not in captures\n",
    "        ):\n",
    "            return None\n",
    "        new_sents = []\n",
    "        for cap in captures[\"descriptions\"]:\n",
    "            matches = re.match('(?:[\\\\d]\\\\.)\\\\s*?\\\\\"(?P<sent>[^\\\\\"]+)\\\\\"', cap.strip())\n",
    "            # print(\"match: \", matches)\n",
    "            # print(\"sent: \", matches.groupdict()[\"sent\"])\n",
    "            new_sents.append(matches.groupdict()[\"sent\"])\n",
    "        return new_sents\n",
    "\n",
    "    try:\n",
    "        reply = reply.replace('\\\\\"', '\"')\n",
    "        new_sents = parse_result_main(reply)\n",
    "        if new_sents is None:\n",
    "            new_sents = parse_result_multiline_list(reply)\n",
    "            # if new_sent is None:\n",
    "            #     print(reply)\n",
    "        return new_sents\n",
    "    except Exception as ex:\n",
    "        errors.append(\n",
    "            {\n",
    "                \"image_id\": image_id,\n",
    "                \"msg\": str(ex) + \" ex type: \" + str(type(ex)),\n",
    "                \"raw_reply\": reply,\n",
    "            }\n",
    "        )\n",
    "        return None\n",
    "\n",
    "\n",
    "def verify_results(result: list[dict], refcoco: COCO):\n",
    "    image_id = result[\"image_id\"]\n",
    "    warnings, errors = [], []\n",
    "    raw_reply: list[str] = result[\"api_response\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "    fp_sents = parse_result(image_id, raw_reply, warnings, errors)\n",
    "    request_info = result[\"request_info\"]\n",
    "    gt_sents = request_info[\"sentences\"]\n",
    "\n",
    "    if fp_sents is None:\n",
    "        return warnings, errors\n",
    "    assert isinstance(fp_sents, list)\n",
    "\n",
    "    # Ensure correct number of FP sentences were generated:\n",
    "    if len(fp_sents) != len(gt_sents):\n",
    "        errors.append(\n",
    "            {\n",
    "                \"image_id\": image_id,\n",
    "                \"msg\": \"Wrong number of FP sentences\",\n",
    "                \"msg_detail\": f\"len(fp_sents):{len(fp_sents)}!=len(gt_sents):{len(gt_sents)}\",\n",
    "                \"fp_sents\": fp_sents,\n",
    "                \"gt_sents\": gt_sents,\n",
    "                \"reply\": result,\n",
    "                \"raw_reply\": raw_reply,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    for sent, fp_sent in zip(gt_sents, fp_sents):\n",
    "        # Warn if FP sentence is same as original sentence:\n",
    "        if sent.lower() == fp_sent.lower():\n",
    "            warnings.append(\n",
    "                {\n",
    "                    \"image_id\": image_id,\n",
    "                    \"msg\": \"FP is exact match for GT sentence\",\n",
    "                    \"msg_detail\": f\"{sent}=={fp_sent}\",\n",
    "                    \"fp_sents\": fp_sent,\n",
    "                    \"gt_sent\": sent,\n",
    "                    \"reply\": result,\n",
    "                    \"raw_reply\": raw_reply,\n",
    "                }\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # diff_result = string_diff(sent.lower(), fp_sent.lower())\n",
    "        # num_matching_spans = len([res[0] for res in diff_result if res[0] == \"=\"])\n",
    "\n",
    "        # if num_matching_spans not in {1, 2}:\n",
    "        #     warnings.append(\n",
    "        #         {\n",
    "        #             \"image_id\": image_id,\n",
    "        #             \"msg\": \"Wrong num_matching_spans\",\n",
    "        #             \"msg_detail\": f\":{num_matching_spans}, diff:{diff_result}\",\n",
    "        #             \"fp_sents\": fp_sent,\n",
    "        #             \"gt_sent\": sent,\n",
    "        #             \"reply\": result,\n",
    "        #             \"raw_reply\": raw_reply,\n",
    "        #         }\n",
    "        #     )\n",
    "        # print(\"\")\n",
    "    result[\"fp_sents\"] = fp_sents\n",
    "    # print(\"warnings: \", len(warnings), \"errors: \", len(errors))\n",
    "    return warnings, errors\n",
    "\n",
    "\n",
    "def check_fpsent_counts(results: list[dict]):\n",
    "    has_fpsents_count = 0\n",
    "    num_results = 0\n",
    "    for result in results:\n",
    "        num_results += 1\n",
    "        if \"fp_sents\" in result and len(result[\"fp_sents\"]) > 0:\n",
    "            has_fpsents_count += 1\n",
    "\n",
    "    print(\"num_results: \", num_results)\n",
    "    print(\"has_fpsents_count: \", has_fpsents_count)\n",
    "\n",
    "\n",
    "# Check all results:\n",
    "api_responses = load_api_responses(api_results_dir)[:200000]\n",
    "warnings = []\n",
    "errors = []\n",
    "for result in tqdm(api_responses):\n",
    "    _warnings, _errors = verify_results(result, refcoco)\n",
    "    warnings.extend(_warnings)\n",
    "    errors.extend(_errors)\n",
    "\n",
    "check_fpsent_counts(api_responses)\n",
    "\n",
    "# Summarize Results:\n",
    "print(f\"Found {len(warnings)} warnings\")\n",
    "print(f\"Found {len(errors)} errors\")\n",
    "# print(\"\")\n",
    "# print(\"=\" * 220)\n",
    "# print(\"Warnings:\")\n",
    "# for warn in warnings[:3]:\n",
    "#     print(\"\")\n",
    "#     print(\"=\" * 100)\n",
    "#     print(json.dumps(warn, indent=4))\n",
    "# print(\"\")\n",
    "# print(\"=\" * 220)\n",
    "# print(\"Errors:\")\n",
    "# for err in errors[:3]:\n",
    "#     print(\"\")\n",
    "#     print(\"=\" * 100)\n",
    "#     print(json.dumps(err, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Intermediate Results: API Responses With Parsed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parsed_results_path = api_results_dir / \"parsed_results_001.pkl\"\n",
    "pickle.dump(api_responses, open(parsed_results_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Error Counts Grouped By Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>uniqe_imgs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>msg</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Wrong number of FP sentences</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EOL while scanning string literal (&lt;unknown&gt;, line 1) ex type: &lt;class 'SyntaxError'&gt;</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    total  uniqe_imgs\n",
       "msg                                                                  \n",
       "Wrong number of FP sentences                           29          29\n",
       "EOL while scanning string literal (<unknown>, l...      1           1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_errors = pd.DataFrame(errors)\n",
    "# display(df_errors)\n",
    "df_err_counts = (\n",
    "    df_errors.groupby([\"msg\"])\n",
    "    .agg(\n",
    "        total=(\"image_id\", \"count\"),\n",
    "        uniqe_imgs=(\"image_id\", \"nunique\"),\n",
    "    )\n",
    "    .sort_values(\"total\", ascending=False)\n",
    ")\n",
    "display(df_err_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This amount of errors seems acceptable. The top two types of error are:\n",
    "\n",
    "_refcocog_\n",
    "\n",
    "- (59 errors) Invalid pythong list syntaks for the sentences, e.g., unmatched string quotes, missing commas\n",
    "- (55 errors) Wrong number of false premise sentences returned by chat-gpt. If we wanted to, we could use whatever sentences gpt was able to provide.\n",
    "\n",
    "\n",
    "_refcoco_\n",
    "\n",
    "- (29 errors) Wrong number of false premise sentences returned by chat-gpt. If we wanted to, we could use whatever sentences gpt was able to provide.Wrong number of false premise sentences returned by chat-gpt. If we wanted to, we could use whatever sentences gpt was able to provide.\n",
    "- (1 errors) `EOL while scanning string literal (<unknown>, line 1) ex type: <class 'SyntaxError'>\tEOL while scanning string literal (<unknown>, line 1) ex type: <class 'SyntaxError'>\t`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>msg</th>\n",
       "      <th>msg_detail</th>\n",
       "      <th>fp_sents</th>\n",
       "      <th>gt_sents</th>\n",
       "      <th>reply</th>\n",
       "      <th>raw_reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122854</td>\n",
       "      <td>Wrong number of FP sentences</td>\n",
       "      <td>len(fp_sents):9!=len(gt_sents):11</td>\n",
       "      <td>[top chair on left, sleigh, top left sleigh, bottom right chair, bottom right sleigh, bottom right, chair rail right, chair top right, left bottom corner]</td>\n",
       "      <td>[top bed on left, bunk, top left bunk, bottom right bed, bottom right bunk, bottom right, bed rail right, bed top right, left bottom corner, bottom left pillow, bot left pillow]</td>\n",
       "      <td>{'api_response': {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': '## Answer\\n\\nAltered Descriptions: \\n1. \"top chair on left\"\\n2. \"sleigh\"\\n3. \"top left sleigh\"\\n4. \"bottom right chair\"\\n5. \"bottom right sleigh\"\\n6. \"bottom right\"\\n7. \"chair rail right\"\\n8. \"chair top right\"\\n9. \"left bottom corner\"\\n10. \"bottom left cushion\"\\n11. \"bot left cushion\"', 'role': 'assistant'}}], 'created': 1699351138, 'id': 'chatcmpl-8ID9GJx8tqfNSA19OzdyPDthM57U2', 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 87, 'prompt_tokens': 523, 'total_tokens': 610}}, 'image_id': 122854, 'request_info': {'ann_ids': [318183, 1612675, 1957359, 1612147], 'image_id': 122854, 'ref_ids': [39420, 39418, 39417, 39419], 'sent_ids': [[112085, 112086, 112087], [112079, 112080, 112081], [112077, 112078], [112082, 112083, 112084]], 'sentences': ['top bed on left', 'bunk', 'top left bunk', 'bottom right bed', 'bottom right bunk', 'bottom right', 'bed rail right', 'bed top right', 'left bottom corner', 'bottom left pillow', 'bot left pillow']}, 'fp_sents': ['top chair on left', 'sleigh', 'top left sleigh', 'bottom right chair', 'bottom right sleigh', 'bottom right', 'chair rail right', 'chair top right', 'left bottom corner']}</td>\n",
       "      <td>## Answer\\n\\nAltered Descriptions: \\n1. \"top chair on left\"\\n2. \"sleigh\"\\n3. \"top left sleigh\"\\n4. \"bottom right chair\"\\n5. \"bottom right sleigh\"\\n6. \"bottom right\"\\n7. \"chair rail right\"\\n8. \"chair top right\"\\n9. \"left bottom corner\"\\n10. \"bottom left cushion\"\\n11. \"bot left cushion\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130795</td>\n",
       "      <td>Wrong number of FP sentences</td>\n",
       "      <td>len(fp_sents):9!=len(gt_sents):14</td>\n",
       "      <td>[pink plate to the left of 80, fifth toilet thing from left, blue toilet in front of elephant, second from right back, second toilet from the left blue, green frisbee toy, kangaroo, parrot, second pink toilet from the left]</td>\n",
       "      <td>[pink dish to the left of 80, fifth toilet thing from right, blue toilet in front of pig, second from right front, second toilet from the right blue, green frof toy, frog, frog, second pink toilet from the right, third toilet from right it is pink, white one toliet behind sign, pot furthest to the right, bottom right white thing, pink pottie on right]</td>\n",
       "      <td>{'api_response': {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': '## Answer\\n\\nAltered Descriptions: (14 sentences):\\n1. \"pink plate to the left of 80\"\\n2. \"fifth toilet thing from left\"\\n3. \"blue toilet in front of elephant\"\\n4. \"second from right back\"\\n5. \"second toilet from the left blue\"\\n6. \"green frisbee toy\"\\n7. \"kangaroo\"\\n8. \"parrot\"\\n9. \"second pink toilet from the left\"\\n10. \"third toilet from left it is pink\"\\n11. \"white one toilet behind tree\"\\n12. \"pot furthest to the left\"\\n13. \"bottom left white thing\"\\n14. \"pink potty on left\"', 'role': 'assistant'}}], 'created': 1699349301, 'id': 'chatcmpl-8ICfddKTtdGcdEmAsonLpj5z1J1Kg', 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 141, 'prompt_tokens': 568, 'total_tokens': 709}}, 'image_id': 130795, 'request_info': {'ann_ids': [1968294, 1623588, 1968384, 1623494, 1095804], 'image_id': 130795, 'ref_ids': [38688, 38689, 38687, 38690, 38691], 'sent_ids': [[110016, 110017], [110018, 110019, 110020], [110013, 110014, 110015], [110021, 110022, 110023], [110024, 110025, 110026]], 'sentences': ['pink dish to the left of 80', 'fifth toilet thing from right', 'blue toilet in front of pig', 'second from right front', 'second toilet from the right blue', 'green frof toy', 'frog', 'frog', 'second pink toilet from the right', 'third toilet from right it is pink', 'white one toliet behind sign', 'pot furthest to the right', 'bottom right white thing', 'pink pottie on right']}, 'fp_sents': ['pink plate to the left of 80', 'fifth toilet thing from left', 'blue toilet in front of elephant', 'second from right back', 'second toilet from the left blue', 'green frisbee toy', 'kangaroo', 'parrot', 'second pink toilet from the left']}</td>\n",
       "      <td>## Answer\\n\\nAltered Descriptions: (14 sentences):\\n1. \"pink plate to the left of 80\"\\n2. \"fifth toilet thing from left\"\\n3. \"blue toilet in front of elephant\"\\n4. \"second from right back\"\\n5. \"second toilet from the left blue\"\\n6. \"green frisbee toy\"\\n7. \"kangaroo\"\\n8. \"parrot\"\\n9. \"second pink toilet from the left\"\\n10. \"third toilet from left it is pink\"\\n11. \"white one toilet behind tree\"\\n12. \"pot furthest to the left\"\\n13. \"bottom left white thing\"\\n14. \"pink potty on left\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120926</td>\n",
       "      <td>Wrong number of FP sentences</td>\n",
       "      <td>len(fp_sents):9!=len(gt_sents):12</td>\n",
       "      <td>[person in front with a unicorn on shirt, yellow shirt in front, child in pink, lady wearing a crown, wizard, pirate, left dragon, dragon on the right, dragon right]</td>\n",
       "      <td>[person in front with picture on shirt, white shirt in front, child in white, lady, woman, woman, right elephant, elephant on right, elephant right, elephant on the left, left elephant, elephant left]</td>\n",
       "      <td>{'api_response': {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': 'Altered Descriptions: \\n1. \"person in front with a unicorn on shirt\"\\n2. \"yellow shirt in front\"\\n3. \"child in pink\"\\n4. \"lady wearing a crown\"\\n5. \"wizard\"\\n6. \"pirate\"\\n7. \"left dragon\"\\n8. \"dragon on the right\"\\n9. \"dragon right\"\\n10. \"dragon on the unicorn\"\\n11. \"unicorn dragon\"\\n12. \"dragon left\"', 'role': 'assistant'}}], 'created': 1699350239, 'id': 'chatcmpl-8ICulqZO3G7G0V8qEXu7nqhdjjk14', 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 93, 'prompt_tokens': 530, 'total_tokens': 623}}, 'image_id': 120926, 'request_info': {'ann_ids': [465270, 469005, 583207, 584848], 'image_id': 120926, 'ref_ids': [39617, 39616, 39615, 39614], 'sent_ids': [[112646, 112647, 112648], [112643, 112644, 112645], [112640, 112641, 112642], [112637, 112638, 112639]], 'sentences': ['person in front with picture on shirt', 'white shirt in front', 'child in white', 'lady', 'woman', 'woman', 'right elephant', 'elephant on right', 'elephant right', 'elephant on the left', 'left elephant', 'elephant left']}, 'fp_sents': ['person in front with a unicorn on shirt', 'yellow shirt in front', 'child in pink', 'lady wearing a crown', 'wizard', 'pirate', 'left dragon', 'dragon on the right', 'dragon right']}</td>\n",
       "      <td>Altered Descriptions: \\n1. \"person in front with a unicorn on shirt\"\\n2. \"yellow shirt in front\"\\n3. \"child in pink\"\\n4. \"lady wearing a crown\"\\n5. \"wizard\"\\n6. \"pirate\"\\n7. \"left dragon\"\\n8. \"dragon on the right\"\\n9. \"dragon right\"\\n10. \"dragon on the unicorn\"\\n11. \"unicorn dragon\"\\n12. \"dragon left\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                           msg                         msg_detail  \\\n",
       "1    122854  Wrong number of FP sentences  len(fp_sents):9!=len(gt_sents):11   \n",
       "2    130795  Wrong number of FP sentences  len(fp_sents):9!=len(gt_sents):14   \n",
       "0    120926  Wrong number of FP sentences  len(fp_sents):9!=len(gt_sents):12   \n",
       "\n",
       "                                                                                                                                                                                                                          fp_sents  \\\n",
       "1                                                                       [top chair on left, sleigh, top left sleigh, bottom right chair, bottom right sleigh, bottom right, chair rail right, chair top right, left bottom corner]   \n",
       "2  [pink plate to the left of 80, fifth toilet thing from left, blue toilet in front of elephant, second from right back, second toilet from the left blue, green frisbee toy, kangaroo, parrot, second pink toilet from the left]   \n",
       "0                                                            [person in front with a unicorn on shirt, yellow shirt in front, child in pink, lady wearing a crown, wizard, pirate, left dragon, dragon on the right, dragon right]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                            gt_sents  \\\n",
       "1                                                                                                                                                                                  [top bed on left, bunk, top left bunk, bottom right bed, bottom right bunk, bottom right, bed rail right, bed top right, left bottom corner, bottom left pillow, bot left pillow]   \n",
       "2  [pink dish to the left of 80, fifth toilet thing from right, blue toilet in front of pig, second from right front, second toilet from the right blue, green frof toy, frog, frog, second pink toilet from the right, third toilet from right it is pink, white one toliet behind sign, pot furthest to the right, bottom right white thing, pink pottie on right]   \n",
       "0                                                                                                                                                           [person in front with picture on shirt, white shirt in front, child in white, lady, woman, woman, right elephant, elephant on right, elephant right, elephant on the left, left elephant, elephant left]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   reply  \\\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {'api_response': {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': '## Answer\\n\\nAltered Descriptions: \\n1. \"top chair on left\"\\n2. \"sleigh\"\\n3. \"top left sleigh\"\\n4. \"bottom right chair\"\\n5. \"bottom right sleigh\"\\n6. \"bottom right\"\\n7. \"chair rail right\"\\n8. \"chair top right\"\\n9. \"left bottom corner\"\\n10. \"bottom left cushion\"\\n11. \"bot left cushion\"', 'role': 'assistant'}}], 'created': 1699351138, 'id': 'chatcmpl-8ID9GJx8tqfNSA19OzdyPDthM57U2', 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 87, 'prompt_tokens': 523, 'total_tokens': 610}}, 'image_id': 122854, 'request_info': {'ann_ids': [318183, 1612675, 1957359, 1612147], 'image_id': 122854, 'ref_ids': [39420, 39418, 39417, 39419], 'sent_ids': [[112085, 112086, 112087], [112079, 112080, 112081], [112077, 112078], [112082, 112083, 112084]], 'sentences': ['top bed on left', 'bunk', 'top left bunk', 'bottom right bed', 'bottom right bunk', 'bottom right', 'bed rail right', 'bed top right', 'left bottom corner', 'bottom left pillow', 'bot left pillow']}, 'fp_sents': ['top chair on left', 'sleigh', 'top left sleigh', 'bottom right chair', 'bottom right sleigh', 'bottom right', 'chair rail right', 'chair top right', 'left bottom corner']}   \n",
       "2  {'api_response': {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': '## Answer\\n\\nAltered Descriptions: (14 sentences):\\n1. \"pink plate to the left of 80\"\\n2. \"fifth toilet thing from left\"\\n3. \"blue toilet in front of elephant\"\\n4. \"second from right back\"\\n5. \"second toilet from the left blue\"\\n6. \"green frisbee toy\"\\n7. \"kangaroo\"\\n8. \"parrot\"\\n9. \"second pink toilet from the left\"\\n10. \"third toilet from left it is pink\"\\n11. \"white one toilet behind tree\"\\n12. \"pot furthest to the left\"\\n13. \"bottom left white thing\"\\n14. \"pink potty on left\"', 'role': 'assistant'}}], 'created': 1699349301, 'id': 'chatcmpl-8ICfddKTtdGcdEmAsonLpj5z1J1Kg', 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 141, 'prompt_tokens': 568, 'total_tokens': 709}}, 'image_id': 130795, 'request_info': {'ann_ids': [1968294, 1623588, 1968384, 1623494, 1095804], 'image_id': 130795, 'ref_ids': [38688, 38689, 38687, 38690, 38691], 'sent_ids': [[110016, 110017], [110018, 110019, 110020], [110013, 110014, 110015], [110021, 110022, 110023], [110024, 110025, 110026]], 'sentences': ['pink dish to the left of 80', 'fifth toilet thing from right', 'blue toilet in front of pig', 'second from right front', 'second toilet from the right blue', 'green frof toy', 'frog', 'frog', 'second pink toilet from the right', 'third toilet from right it is pink', 'white one toliet behind sign', 'pot furthest to the right', 'bottom right white thing', 'pink pottie on right']}, 'fp_sents': ['pink plate to the left of 80', 'fifth toilet thing from left', 'blue toilet in front of elephant', 'second from right back', 'second toilet from the left blue', 'green frisbee toy', 'kangaroo', 'parrot', 'second pink toilet from the left']}   \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                      {'api_response': {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': 'Altered Descriptions: \\n1. \"person in front with a unicorn on shirt\"\\n2. \"yellow shirt in front\"\\n3. \"child in pink\"\\n4. \"lady wearing a crown\"\\n5. \"wizard\"\\n6. \"pirate\"\\n7. \"left dragon\"\\n8. \"dragon on the right\"\\n9. \"dragon right\"\\n10. \"dragon on the unicorn\"\\n11. \"unicorn dragon\"\\n12. \"dragon left\"', 'role': 'assistant'}}], 'created': 1699350239, 'id': 'chatcmpl-8ICulqZO3G7G0V8qEXu7nqhdjjk14', 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 93, 'prompt_tokens': 530, 'total_tokens': 623}}, 'image_id': 120926, 'request_info': {'ann_ids': [465270, 469005, 583207, 584848], 'image_id': 120926, 'ref_ids': [39617, 39616, 39615, 39614], 'sent_ids': [[112646, 112647, 112648], [112643, 112644, 112645], [112640, 112641, 112642], [112637, 112638, 112639]], 'sentences': ['person in front with picture on shirt', 'white shirt in front', 'child in white', 'lady', 'woman', 'woman', 'right elephant', 'elephant on right', 'elephant right', 'elephant on the left', 'left elephant', 'elephant left']}, 'fp_sents': ['person in front with a unicorn on shirt', 'yellow shirt in front', 'child in pink', 'lady wearing a crown', 'wizard', 'pirate', 'left dragon', 'dragon on the right', 'dragon right']}   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              raw_reply  \n",
       "1                                                                                                                                                                                                         ## Answer\\n\\nAltered Descriptions: \\n1. \"top chair on left\"\\n2. \"sleigh\"\\n3. \"top left sleigh\"\\n4. \"bottom right chair\"\\n5. \"bottom right sleigh\"\\n6. \"bottom right\"\\n7. \"chair rail right\"\\n8. \"chair top right\"\\n9. \"left bottom corner\"\\n10. \"bottom left cushion\"\\n11. \"bot left cushion\"  \n",
       "2  ## Answer\\n\\nAltered Descriptions: (14 sentences):\\n1. \"pink plate to the left of 80\"\\n2. \"fifth toilet thing from left\"\\n3. \"blue toilet in front of elephant\"\\n4. \"second from right back\"\\n5. \"second toilet from the left blue\"\\n6. \"green frisbee toy\"\\n7. \"kangaroo\"\\n8. \"parrot\"\\n9. \"second pink toilet from the left\"\\n10. \"third toilet from left it is pink\"\\n11. \"white one toilet behind tree\"\\n12. \"pot furthest to the left\"\\n13. \"bottom left white thing\"\\n14. \"pink potty on left\"  \n",
       "0                                                                                                                                                                                        Altered Descriptions: \\n1. \"person in front with a unicorn on shirt\"\\n2. \"yellow shirt in front\"\\n3. \"child in pink\"\\n4. \"lady wearing a crown\"\\n5. \"wizard\"\\n6. \"pirate\"\\n7. \"left dragon\"\\n8. \"dragon on the right\"\\n9. \"dragon right\"\\n10. \"dragon on the unicorn\"\\n11. \"unicorn dragon\"\\n12. \"dragon left\"  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context(\"display.max_colwidth\", None, \"display.max_columns\", None):\n",
    "    display(\n",
    "        df_errors[df_errors.msg == \"Wrong number of FP sentences\"].head(3).sort_values(\n",
    "            \"raw_reply\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with pd.option_context(\"display.max_colwidth\", None, \"display.max_columns\", None):\n",
    "#     display(df_errors.iloc[38][\"raw_reply\"])\n",
    "#     print(df_errors.iloc[38][\"raw_reply\"])\n",
    "#     print(df_errors.iloc[38][\"raw_reply\"].replace('\\\\\"', '\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Scratch Code to Debug Regex parsing of the chatGPT replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def parse_result(image_id: int, reply: str, warnings, errors) -> list[str]:\n",
    "#     def parse_result_main(reply: str, debug=False):\n",
    "#         # print(f\"ChatGPT Reply: \\n\\t{reply}\")\n",
    "#         matches = re.match(\n",
    "#             # '.*Descriptions:*\\\\s*(\\\\(.{1,} sentence[s]{0,1}\\\\):){0,1}\\\\s*(?P<descriptions>\\\\[\\\\\".*\\\\\"\\\\])',\n",
    "#             '.*Description[s]{0,1}:*\\\\s*(\\\\(.{1,} sentence[s]{0,1}\\\\):){0,1}\\\\s*\\\\[{0,1}(?P<descriptions>\\\\\".*\\\\\")\\\\]{0,1}',\n",
    "#             reply,\n",
    "#             re.MULTILINE | re.DOTALL,\n",
    "#         )\n",
    "#         if matches is None:\n",
    "#             if DEBUG:\n",
    "#                 print(\"parse_result_main() No matches\")\n",
    "#             return None\n",
    "#         list_str = matches.group(\"descriptions\")\n",
    "#         if not list_str.startswith(\"[\"):\n",
    "#             list_str = \"[\" + list_str\n",
    "#         if not list_str.endswith(\"]\"):\n",
    "#             list_str = list_str + \"]\"\n",
    "\n",
    "#         new_sent = ast.literal_eval(list_str)\n",
    "#         # print(\"New Sents: \", new_sent)\n",
    "#         if new_sent is None:\n",
    "#             errors.append(\n",
    "#                 {\n",
    "#                     \"image_id\": image_id,\n",
    "#                     \"msg\": f\"No FP sents found (fp_sents is None)\",\n",
    "#                     \"raw_reply\": reply,\n",
    "#                 }\n",
    "#             )\n",
    "#             return None\n",
    "#         return new_sent\n",
    "\n",
    "#     def parse_result_multiline_list(reply: str, debug=False):\n",
    "#         matches = re.match(\n",
    "#             '(?:.*Description[s]{0,1}:[ ]*(\\\\(.+ sentence[s]{0,1}\\\\):){0,1})\\\\n*?(?P<descriptions>\\\\n\\\\d\\\\.[ ]*\\\\\"[^\\\\n\\\\\"]+\\\\\")+',\n",
    "#             reply,\n",
    "#             re.MULTILINE | re.DOTALL,\n",
    "#         )\n",
    "#         if matches is None:\n",
    "#             if DEBUG:\n",
    "#                 print(\"parse_result_multiline_list() No matches\")\n",
    "#             return None\n",
    "#         captures = matches.capturesdict()\n",
    "#         if (captures is None or len(captures) == 0) or (\n",
    "#             captures is not None and \"descriptions\" not in captures\n",
    "#         ):\n",
    "#             return None\n",
    "#         sents = []\n",
    "#         for cap in captures[\"descriptions\"]:\n",
    "#             matches = re.match('(?:[\\\\d]\\\\.)\\\\s*?\\\\\"(?P<sent>[^\\\\\"]+)\\\\\"', cap.strip())\n",
    "#             # print(\"match: \", matches)\n",
    "#             # print(\"sent: \", matches.groupdict()[\"sent\"])\n",
    "#             sents.append(matches.groupdict()[\"sent\"])\n",
    "#         return sents\n",
    "\n",
    "#     # try:\n",
    "#     # reply = reply.replace('\\\\\"', '\"')\n",
    "#     new_sent = parse_result_main(reply)\n",
    "#     print(\"new_sent1: \", new_sent)\n",
    "#     if new_sent is None:\n",
    "#         new_sent = parse_result_multiline_list(reply)\n",
    "#         print(\"new_sent2: \", new_sent)\n",
    "#         # if new_sent is None:\n",
    "#         #     print(reply)\n",
    "#     return new_sent\n",
    "#     # except Exception as ex:\n",
    "#     #     print(\n",
    "#     #         {\n",
    "#     #             \"image_id\": image_id,\n",
    "#     #             \"msg\": str(ex) + \" ex type: \" + str(type(ex)),\n",
    "#     #             \"raw_reply\": reply,\n",
    "#     #         }\n",
    "#     #     )\n",
    "#     #     return None\n",
    "#     return None\n",
    "\n",
    "\n",
    "# reply = \"\"\"## Answer\\n\\nAltered Descriptions: (6 sentences): [\"man in a purple and black jacket bending over\", \"an older gentleman reaching up to pick to sign a page \\\", \"man wearing a yellow shirt\", \"man wearing glasses , purple shirt and khakis\", \"the chair behind the man in a yellow shirt\", \"the sofa in lavender color\"]\n",
    "# \"\"\"\n",
    "# reply = df_errors.iloc[38][\"raw_reply\"].replace('\\\\\"', '\"')\n",
    "# print(reply)\n",
    "# print(parse_result(-1, reply, [], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # reply = \"\"\"## Answer\n",
    "# # Altered Descriptions: (4 sentences): [\"the giraffe eating the grass on the ground\", \"the giraffe is grazing\", \"a giraffe looking straight at the camera\", \"giraffe on the left that is looking at cameraman\"]\n",
    "# # \"\"\"\n",
    "# # reply = df_errors.iloc[164][\"raw_reply\"]\n",
    "# reply = '''Altered Descriptions:\n",
    "# 1. \"a bowl of some smelly food possibly applesauce\"\n",
    "# 2. \"bowl of food with blue spoon in the bowl\"'''\n",
    "\n",
    "# print(reply)\n",
    "\n",
    "# patterns = [\n",
    "#     # re.compile(\n",
    "#     #     '.*Descriptions:*\\\\s*(\\\\(.{1,} sentence[s]{0,1}\\\\):){0,1}\\\\s*(?P<descriptions>\\\\[\\\\\".*\\\\\"\\\\])',\n",
    "#     #     re.MULTILINE | re.DOTALL,\n",
    "#     # ),\n",
    "#     re.compile(\n",
    "#         # '(?:[^\\\\n\\\\\"]*Descriptions:\\\\s*(\\\\(.+ sentences\\\\):)\\\\s*\\n+)(\\\\n\\\\d\\\\.\\\\s*\\\\\"[^\\\\n\\\\\"]+\\\\\")+',\n",
    "#         '(?:.*Descriptions:[ ]*(\\\\(.+ sentences\\\\):){0,1})\\\\n*?(?P<descriptions>\\\\n\\\\d\\\\.[ ]*\\\\\"[^\\\\n\\\\\"]+\\\\\")+',\n",
    "#         re.MULTILINE | re.DOTALL,\n",
    "#     ),\n",
    "# ]\n",
    "# for pat in patterns:\n",
    "#     matches = pat.match(reply)\n",
    "#     if matches is None:\n",
    "#         print(\"NO MATCHES\")\n",
    "#         continue\n",
    "#     print(\"\\nmatches: \", matches)\n",
    "#     print(\"\\ngroupdict: \", matches.groupdict())\n",
    "#     print(\"\\ncapturesdict: \", matches.capturesdict())\n",
    "#     print(\"len(ncapturesdict) \", len(matches.capturesdict()[\"descriptions\"]))\n",
    "\n",
    "#     if matches is not None and \"descriptions\" in matches.groupdict():\n",
    "#         list_str = matches.group(\"descriptions\")\n",
    "#         print(\"list: \", list_str)\n",
    "#         # Convert the string to a Python object\n",
    "#         new_sent = ast.literal_eval(list_str)\n",
    "#         print(\"new_sent: \", new_sent)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import ast\n",
    "\n",
    "# import regex as re\n",
    "\n",
    "# replies = [\n",
    "#     'Altered Description: \"purple bush to the left of the sign\"',\n",
    "#     '''Altered Descriptions:\n",
    "# 1. \"a bowl of some smelly food possibly applesauce\"\n",
    "# 2. \"bowl of food with blue spoon in the bowl\"''',\n",
    "#     \"\"\"Altered Descriptions: (1 sentence): [\"a purple chair facing the garden\"]\"\"\",\n",
    "#     \"\"\"Altered Descriptions: (6 sentences): [\"the hat of the standing man\", \"a purple hat with repetitive circular patterns\", \"the girl in the green dress standing next to the man\", \"a girl with sunglasses on\", \"a man with straight blonde hair in a blue suit with a yellow hat stands with a woman in sunglasses and a red see through outfit\", \"a man with light blonde, straight hair wearing a suit and hat standing next to a woman\"]\"\"\",\n",
    "#     \"\"\"Altered Descriptions: [\"the hat of the standing man\", \"a purple hat with repetitive circular patterns\", \"the girl in the green dress standing next to the man\", \"a girl with sunglasses on\", \"a man with straight blonde hair in a blue suit with a yellow hat stands with a woman in sunglasses and a red see through outfit\", \"a man with light blonde, straight hair wearing a suit and hat standing next to a woman\"]\"\"\",\n",
    "#     \"\"\"Modified Descriptions: (6 sentences): [\"the hat of the standing man\", \"a purple hat with repetitive circular patterns\", \"the girl in the green dress standing next to the man\", \"a girl with sunglasses on\", \"a man with straight blonde hair in a blue suit with a yellow hat stands with a woman in sunglasses and a red see through outfit\", \"a man with light blonde, straight hair wearing a suit and hat standing next to a woman\"]\"\"\",\n",
    "#     \"\"\"Modified Descriptions: [\"the hat of the standing man\", \"a purple hat with repetitive circular patterns\", \"the girl in the green dress standing next to the man\", \"a girl with sunglasses on\", \"a man with straight blonde hair in a blue suit with a yellow hat stands with a woman in sunglasses and a red see through outfit\", \"a man with light blonde, straight hair wearing a suit and hat standing next to a woman\"]\"\"\",\n",
    "#     \"\"\"Altered Descriptions (6 sentences): [\"the bowtie of the standing man\", \"a neon green tie with repetitive lightning bolt patterns\", \"the girl in the pink dress standing next to the man\", \"a girl with sunglasses on\", \"a man with blue hair in a purple suit with a neon green bowtie stands with a woman in sunglasses and a neon pink outfit\", \"a man with blonde, straight hair wearing a tuxedo and bowtie standing next to a woman\"]\"\"\",\n",
    "#     \"\"\"## Answer Altered Descriptions: (4 sentences): [\"the giraffe eating the grass on the ground\", \"the giraffe is grazing\", \"a giraffe looking straight at the camera\", \"giraffe on the left that is looking at cameraman\"]\"\"\",\n",
    "#     \"\"\"## Answer\n",
    "# Altered Descriptions: (4 sentences): [\"the giraffe eating the grass on the ground\", \"the giraffe is grazing\", \"a giraffe looking straight at the camera\", \"giraffe on the left that is looking at cameraman\"]\"\"\",\n",
    "#     #     \"\"\"Altered Descriptions: (6 sentences):\n",
    "#     # 1. \"the crown of the standing man\"\n",
    "#     # 2. \"a purple crown with repetitive circular patterns\"\n",
    "#     # 3. \"the girl in the pink dress standing next to the man\"\n",
    "#     # 4. \"a girl with sunglasses on\"\n",
    "#     # 5. \"a man with curly black hair in a black suit with a purple crown stands with a woman in sunglasses and a black see through outfit\"\n",
    "#     # 6. \"a man with dark, curly hair wearing a suit and crown standing next to a woman\"\n",
    "#     # \"\"\"\n",
    "# ]\n",
    "\n",
    "# for reply in replies:\n",
    "#     print(\"\")\n",
    "#     # matches = re.match(\"Altered Descriptions[:]+ \\\\(.{1,} sentences\\\\):\\\\s+(?P<descriptions>\\\\[\\\\\\\".*\\\\\\\"\\\\])\", reply)\n",
    "#     matches = re.match(\n",
    "#         '.*Description[s]{0,1}:*\\\\s*(\\\\(.{1,} sentence[s]{0,1}\\\\):){0,1}\\\\s*\\\\[{0,1}(?P<descriptions>\\\\\".*\\\\\")\\\\]{0,1}',\n",
    "#         reply,\n",
    "#         re.MULTILINE,\n",
    "#     )\n",
    "#     print(\"matches: \", matches)\n",
    "#     if matches is not None and \"descriptions\" in matches.groupdict():\n",
    "#         print(\"match.group: \", matches.group(\"descriptions\"))\n",
    "\n",
    "#     if matches is not None and \"descriptions\" in matches.groupdict():\n",
    "#         list_str = matches.group(\"descriptions\")\n",
    "#         # Convert the string to a Python object\n",
    "#         new_sent = ast.literal_eval(list_str)\n",
    "#         print(\"new_sent: \", new_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize False Premise Types\n",
    "\n",
    "First compute spacy tags for each FP sentence and cache the results to disk. \n",
    "\n",
    "Then, use the spacy tags to categorize the FP sentence modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.require_gpu()\n",
    "import spacy_transformers\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing spacy docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 141269/141269 [02:20<00:00, 1004.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num spacy docs:  141269\n",
      "141269 141269 141269\n"
     ]
    }
   ],
   "source": [
    "def get_fp_sentences_flat(responses: list[dict]) -> tuple[list[str], list[int]]:\n",
    "    # Make a flat list of FP sentences so we can batch process with spacy:\n",
    "    fp_sents_all = []\n",
    "    img_ids_all = []\n",
    "    for response in responses:\n",
    "        if \"fp_sents\" not in response:\n",
    "            continue\n",
    "        fp_sents: list[str] = [s for s in response[\"fp_sents\"] if len(s) > 0]\n",
    "        fp_sents_all.extend(fp_sents)\n",
    "        img_ids = [response[\"image_id\"]] * len(fp_sents)\n",
    "        img_ids_all.extend(img_ids)\n",
    "    return fp_sents_all, img_ids_all\n",
    "\n",
    "\n",
    "def get_gt_sentences_flat(refcoco: COCO) -> list[dict]:\n",
    "    # Make a flat list of FP sentences so we can batch process with spacy:\n",
    "    sents_all = []\n",
    "    for ref_id, ref in refcoco.refs.items():\n",
    "        sents: list[str] = ref[\"sentences\"]\n",
    "        sents_all.extend(\n",
    "            [\n",
    "                {\n",
    "                    \"sent\": s,\n",
    "                    \"sent_id\": s[\"sent_id\"],\n",
    "                    \"image_id\": ref[\"image_id\"],\n",
    "                    \"ref_id\": ref_id,\n",
    "                }\n",
    "                for s in sents\n",
    "            ]\n",
    "        )\n",
    "    return sents_all\n",
    "\n",
    "\n",
    "def get_spacy_docs(\n",
    "    responses, refcoco: COCO, api_results_dir: Path, force_recompute: bool = False\n",
    ") -> tuple[list[spacy.tokens.Doc], list[int]]:\n",
    "    \"\"\"\n",
    "    Get spacy doc for FP sentences in all the responses. Caches the output to\n",
    "    disk, and if a cached result already exists, it loads and returns that\n",
    "    instead of re-computing the spacy Docs.\n",
    "\n",
    "    Returns:\n",
    "    :docs: flat list of spacy docs\n",
    "    :img_ids: flat list (same length as docs) that maps indexes of docs to image_id\n",
    "    \"\"\"\n",
    "    docs_path = api_results_dir / \"fp_sentences_spacy_docs.pkl\"\n",
    "    fp_sents_all, doc_to_image = get_fp_sentences_flat(responses)\n",
    "\n",
    "    if not force_recompute and docs_path.exists():\n",
    "        print(\"loading cached spacy docs from disk\")\n",
    "        docs = pickle.load(open(docs_path, \"rb\"))\n",
    "        assert len(docs) == len(fp_sents_all), f\"{len(docs)} != {len(fp_sents_all)}\"\n",
    "    else:\n",
    "        print(\"Computing spacy docs\")\n",
    "        B = 1000\n",
    "        docs: list[spacy.tokens.Doc] = [\n",
    "            d\n",
    "            for d in tqdm(nlp.pipe(fp_sents_all, batch_size=B), total=len(fp_sents_all))\n",
    "        ]\n",
    "        assert len(docs) == len(fp_sents_all), f\"{len(docs)} != {len(fp_sents_all)}\"\n",
    "        pickle.dump(docs, open(docs_path, \"wb\"))\n",
    "    return docs, doc_to_image, fp_sents_all\n",
    "\n",
    "\n",
    "# Only compute/load the docs if they aren't already in memory:\n",
    "if (\n",
    "    \"docs\" not in locals()\n",
    "    or \"doc_to_image\" not in locals()\n",
    "    or \"fp_sents_all\" not in locals()\n",
    "    or True\n",
    "):\n",
    "    docs, doc_to_image, fp_sents_all = get_spacy_docs(\n",
    "        api_responses, refcoco, api_results_dir, force_recompute=False\n",
    "    )\n",
    "print(\"Num spacy docs: \", len(docs))\n",
    "print(len(docs), len(doc_to_image), len(fp_sents_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 141269/141269 [00:05<00:00, 23694.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141269 141269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def get_main_subject(sent: dict, use_root: bool = True):\n",
    "    \"\"\"Should always pass use_root=True, when there is no nsubj the ROOT is the main subject\"\"\"\n",
    "    subjects = [\n",
    "        word\n",
    "        for word, dep in zip(sent[\"spcy_WORD\"], sent[\"spcy_DEP\"])\n",
    "        if dep == \"nsubj\" and word not in nlp.Defaults.stop_words\n",
    "    ]\n",
    "    if use_root and (subjects is None or len(subjects) == 0):\n",
    "        subjects = [\n",
    "            word\n",
    "            for word, dep in zip(sent[\"spcy_WORD\"], sent[\"spcy_DEP\"])\n",
    "            if (dep == \"ROOT\") and word not in nlp.Defaults.stop_words\n",
    "        ]\n",
    "    return subjects\n",
    "\n",
    "\n",
    "def tag_fp_sentences(\n",
    "    responses: list[dict],\n",
    "    refcoco: COCO,\n",
    "    docs: list[spacy.tokens.Doc],\n",
    "    doc_to_img: list[int],\n",
    "    fp_sents_all: list[str],\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a deep copy of refcoco's `.refs_data` property, and enhances the\n",
    "    copy by adding SpaCy NLP parsing tags for POS, TAG, DEP, etc. See here\n",
    "    for more info: https://spacy.io/usage/linguistic-features\n",
    "    \"\"\"\n",
    "    # Map docs back to the fp_sents\n",
    "    fp_sent_dicts = []\n",
    "    for i, (fp_sent, doc, image_id) in tqdm(\n",
    "        enumerate(zip(fp_sents_all, docs, doc_to_image)), total=len(docs)\n",
    "    ):\n",
    "        # response = img_to_response[image_id]\n",
    "        fp_sent_dict = {\n",
    "            # \"ref_id\": -1, we'll have to match it with the right ref_id at some point\n",
    "            \"tokens\": [\n",
    "                word.strip()\n",
    "                for word in fp_sent.split(\" \")\n",
    "                if word.strip() not in string.punctuation\n",
    "            ],\n",
    "            \"raw\": fp_sent,\n",
    "            \"sent_id\": -1,\n",
    "            \"sent\": fp_sent,\n",
    "            \"spcy_WORD\": [str(word) for word in doc],\n",
    "            \"spcy_DEP\": [word.dep_ for word in doc],\n",
    "            \"spcy_POS\": [word.pos_ for word in doc],\n",
    "            \"spcy_LEM\": [word.lemma_ for word in doc],\n",
    "            \"spcy_TAG\": [word.tag_ for word in doc],\n",
    "            \"spcy_IS_STOP\": [word.is_stop for word in doc],\n",
    "            \"spcy_ENTS\": [str(ent).strip() for ent in doc.ents],\n",
    "            \"spcy_NOUN_CHUNKS\": [str(nc).strip() for nc in doc.noun_chunks],\n",
    "            # \"spcy_DOC\": doc, # this takes up way too much space 2.7G vs 115MB\n",
    "        }\n",
    "        fp_sent_dict[\"main_subject\"] = get_main_subject(fp_sent_dict)\n",
    "        # print(fp_tags)\n",
    "        fp_sent_dicts.append(fp_sent_dict)\n",
    "        # if i == 0:\n",
    "        #     print(fp_sent_dict)\n",
    "\n",
    "    return fp_sent_dicts\n",
    "\n",
    "\n",
    "fp_sent_dicts = tag_fp_sentences(\n",
    "    api_responses, refcoco, docs, doc_to_image, fp_sents_all\n",
    ")\n",
    "print(len(fp_sent_dicts), len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent': {'tokens': ['zebra', 'creature', 'front', 'and', 'center'],\n",
       "  'raw': 'zebra creature front and center',\n",
       "  'sent_id': 13689,\n",
       "  'sent': 'zebra creature front and center',\n",
       "  'spcy_WORD': ['zebra', 'creature', 'front', 'and', 'center'],\n",
       "  'spcy_DEP': ['compound', 'ROOT', 'advmod', 'cc', 'conj'],\n",
       "  'spcy_POS': ['NOUN', 'NOUN', 'ADJ', 'CCONJ', 'NOUN'],\n",
       "  'spcy_LEM': ['zebra', 'creature', 'front', 'and', 'center'],\n",
       "  'spcy_TAG': ['NN', 'NN', 'JJ', 'CC', 'NN'],\n",
       "  'spcy_IS_STOP': [False, False, True, True, False],\n",
       "  'spcy_ENTS': [],\n",
       "  'spcy_NOUN_CHUNKS': ['zebra creature']},\n",
       " 'sent_id': 13689,\n",
       " 'image_id': 526754,\n",
       " 'ref_id': 4808}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_flat = get_gt_sentences_flat(refcoco)\n",
    "gt_flat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct Ambiguous FP vs. GT Sentence Matches\n",
    "\n",
    "Sometimes GPT does not return an FP for every GT we input, i.e., we give it five ground truth sentences and it only outputs four false premise sentences. These cases are ambiguous (we don't know which sentence(s) GPT skipped) but we can attempt to match the outputs using string similarity metrics.\n",
    "\n",
    "For the initial refcocog run, around 10% of the images consist of these ambiguous cases, so it is worth correcting. We do the corrections in the below cell.\n",
    "\n",
    "#### fp vs gt sentence count \n",
    "- num_match: 23,128 images\n",
    "- not_match: 2,496 images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp:  141269\n",
      "gt:  142210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                     | 0/19897 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================================================================================================\n",
      "Num gt:  7\n",
      "img_fps:  5 dict_keys(['tokens', 'raw', 'sent_id', 'sent', 'spcy_WORD', 'spcy_DEP', 'spcy_POS', 'spcy_LEM', 'spcy_TAG', 'spcy_IS_STOP', 'spcy_ENTS', 'spcy_NOUN_CHUNKS', 'main_subject', 'ref_id', 'ann_id', 'gt_sent_id', 'gt_sent', 'is_false_premise'])\n",
      "\n",
      "ref_id:  38322\n",
      "# sentences:  6\n",
      "sentence:  108974 guy in background \n",
      "sentence:  108975 shoulder of the man not eating \n",
      "sentence:  108976 dude to the left of the dude grubbing \n",
      "sentence:  -1 elephant in background gt: (108974) True, guy in background\n",
      "sentence:  -1 shoulder of the elephant not eating gt: (108975) True, shoulder of the man not eating\n",
      "sentence:  -1 dude to the left of the dude snacking gt: (108976) True, dude to the left of the dude grubbing\n",
      "\n",
      "ref_id:  38321\n",
      "# sentences:  6\n",
      "sentence:  108970 the face of the guy eating \n",
      "sentence:  108971 guy eating \n",
      "sentence:  108972 man eating \n",
      "sentence:  108973 man \n",
      "sentence:  -1 the face of the giraffe eating gt: (108970) True, the face of the guy eating\n",
      "sentence:  -1 giraffe eating gt: (108971) True, guy eating\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'img_fps: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[\"'sent_id': -1\",\n",
       " \"'sent': elephant in background\",\n",
       " \"'main_subject': ['elephant']\",\n",
       " \"'ref_id': 38322\",\n",
       " \"'ann_id': 476249\",\n",
       " \"'gt_sent_id': 108974\",\n",
       " \"'gt_sent': guy in background\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'img_fps: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[\"'sent_id': -1\",\n",
       " \"'sent': shoulder of the elephant not eating\",\n",
       " \"'main_subject': ['shoulder']\",\n",
       " \"'ref_id': 38322\",\n",
       " \"'ann_id': 476249\",\n",
       " \"'gt_sent_id': 108975\",\n",
       " \"'gt_sent': shoulder of the man not eating\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'img_fps: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[\"'sent_id': -1\",\n",
       " \"'sent': dude to the left of the dude snacking\",\n",
       " \"'main_subject': ['dude']\",\n",
       " \"'ref_id': 38322\",\n",
       " \"'ann_id': 476249\",\n",
       " \"'gt_sent_id': 108976\",\n",
       " \"'gt_sent': dude to the left of the dude grubbing\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'img_fps: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[\"'sent_id': -1\",\n",
       " \"'sent': the face of the giraffe eating\",\n",
       " \"'main_subject': ['face']\",\n",
       " \"'ref_id': 38321\",\n",
       " \"'ann_id': 1739621\",\n",
       " \"'gt_sent_id': 108970\",\n",
       " \"'gt_sent': the face of the guy eating\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'img_fps: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[\"'sent_id': -1\",\n",
       " \"'sent': giraffe eating\",\n",
       " \"'main_subject': ['eating']\",\n",
       " \"'ref_id': 38321\",\n",
       " \"'ann_id': 1739621\",\n",
       " \"'gt_sent_id': 108971\",\n",
       " \"'gt_sent': guy eating\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19897/19897 [00:00<00:00, 88454.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_match: 19868, not_match: 29, num_corrected: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "\n",
    "def sentence_similarities(list_a: list[str], list_b: list[str]) -> dict[int, list[int]]:\n",
    "    \"\"\"Calculates the edit distance between elements of list_a and list_b.\"\"\"\n",
    "    similarity_scores = {}\n",
    "\n",
    "    for i, a_item in enumerate(list_a):\n",
    "        scores = []\n",
    "        for b_item in list_b:\n",
    "            score = levenshtein_distance(a_item, b_item)\n",
    "            scores.append(score)\n",
    "        similarity_scores[i] = scores\n",
    "\n",
    "    return similarity_scores\n",
    "\n",
    "\n",
    "def match_sentences(\n",
    "    img_fps: list[dict], gt_refs_and_sents: list[tuple[dict, dict]], debug=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Two list s of sentences for a single image, and attempt unambiguous match\n",
    "    between them. img_fps is shorter in length than gt_refs_and_sents.\n",
    "    \"\"\"\n",
    "    sim_scores = sentence_similarities(\n",
    "        [sent[\"sent\"] for sent in img_fps],\n",
    "        [gt[\"sent\"] for ref, gt in gt_refs_and_sents],\n",
    "    )\n",
    "\n",
    "    match_indices = []\n",
    "    for (fp_idx, scores), fp in zip(sim_scores.items(), img_fps):\n",
    "        idx = np.argmin(scores)\n",
    "        match_indices.append(idx)\n",
    "\n",
    "    if debug:\n",
    "        print(\"match_indices: \", match_indices)\n",
    "    # Consider matches unambiguous if each fp sentence is matched to a unique gt sentence\n",
    "    is_unambiguous = len(set(match_indices)) == len(match_indices)\n",
    "    result = []\n",
    "    if is_unambiguous:\n",
    "        for fp, match_idx in zip(img_fps, match_indices):\n",
    "            gt_match = gt_refs_and_sents[match_idx]\n",
    "            if debug:\n",
    "                print(f\"\\t match: '{fp['sent']}', '{gt_match[1]['sent']}'\")\n",
    "            result.append((fp, gt_match))\n",
    "    # else:\n",
    "    #     print(\"No match! \", match_indices)\n",
    "    return result\n",
    "\n",
    "\n",
    "def match_fp_with_gt(fp_sent_dicts: list[dict], doc_to_image: list[int], refcoco: COCO):\n",
    "    \"\"\"\n",
    "    Return a copy of the refcoco object where the false premise sentences in \n",
    "    fp_sent_dicts are added to the refcoco.imgs.refs[\"sentences\"] lists. The\n",
    "    fp sentence are mapped to their ground truth sentences\n",
    "    \"\"\"\n",
    "    DEBUG = True\n",
    "    refcoco = deepcopy(refcoco)\n",
    "    gt_flat = get_gt_sentences_flat(refcoco)\n",
    "    print(\"fp: \", len(fp_sent_dicts))\n",
    "    print(\"gt: \", len(gt_flat))\n",
    "    img_fps_all = defaultdict(list)\n",
    "    {img_fps_all[img_id].append(fp) for fp, img_id in zip(fp_sent_dicts, doc_to_image)}\n",
    "    num_match, num_not_match = 0, 0\n",
    "    num_corrected = 0\n",
    "\n",
    "    for idx, (image_id, img_fps) in tqdm(\n",
    "        enumerate(img_fps_all.items()), total=len(img_fps_all)\n",
    "    ):\n",
    "        # img_fps is a list of dicts. Each dict has keys: ['tokens',\n",
    "        #    'raw', 'sent_id', 'sent', 'spcy_WORD', 'spcy_DEP', 'spcy_POS',\n",
    "        #    'spcy_LEM', 'spcy_TAG', 'spcy_IS_STOP', 'spcy_ENTS',\n",
    "        #    'spcy_NOUN_CHUNKS', 'main_subject']\n",
    "        # This function adds the following keys to these dicts:\n",
    "        #    ['ref_id', 'ann_id', 'gt_sent_id', 'gt_sent']\n",
    "        gt_refs_and_sents: list[tuple[dict, dict]] = []\n",
    "\n",
    "        for img_ref in refcoco.img_to_refs[image_id]:\n",
    "            # img_ref keys: ['image_id', 'split', 'sentences', 'file_name',\n",
    "            #    'category_id', 'ann_id', 'sent_ids', 'ref_id']\n",
    "            for s in img_ref[\"sentences\"]:\n",
    "                gt_refs_and_sents.append((img_ref, s))\n",
    "        if len(gt_refs_and_sents) == len(img_fps):\n",
    "            num_match += 1\n",
    "            for fp, (img_ref, gt_sent) in zip(img_fps, gt_refs_and_sents):\n",
    "                if len(fp[\"sent\"].strip()) == 0:\n",
    "                    print(\"EMPTY1 (img_id: {image_id}): \", fp)\n",
    "                fp[\"ref_id\"] = img_ref[\"ref_id\"]\n",
    "                fp[\"ann_id\"] = img_ref[\"ann_id\"]\n",
    "                fp[\"gt_sent_id\"] = gt_sent[\"sent_id\"]\n",
    "                fp[\"gt_sent\"] = gt_sent[\"sent\"]\n",
    "                fp[\"is_false_premise\"] = True\n",
    "                img_ref[\"sentences\"].append(fp)\n",
    "        else:\n",
    "            num_not_match += 1\n",
    "            matches = match_sentences(img_fps, gt_refs_and_sents, False)\n",
    "            if matches:\n",
    "                num_corrected += 1\n",
    "            for fp, (img_ref, gt_sent) in matches:\n",
    "                if len(fp[\"sent\"].strip()) == 0:\n",
    "                    print(f\"EMPTY2 (img_id: {image_id}): \", fp)\n",
    "                fp[\"ref_id\"] = img_ref[\"ref_id\"]\n",
    "                fp[\"ann_id\"] = img_ref[\"ann_id\"]\n",
    "                fp[\"gt_sent_id\"] = gt_sent[\"sent_id\"]\n",
    "                fp[\"gt_sent\"] = gt_sent[\"sent\"]\n",
    "                fp[\"is_false_premise\"] = True\n",
    "                img_ref[\"sentences\"].append(fp)\n",
    "        # DEBUG:\n",
    "        if DEBUG and num_corrected == 1:\n",
    "            print(\"\")\n",
    "            print(\"=\" * 200)\n",
    "            print(\"Num gt: \", len(gt_refs_and_sents))\n",
    "            print(\"img_fps: \", len(img_fps), img_fps[0].keys())\n",
    "            img_refs = refcoco.img_to_refs[image_id]\n",
    "            for img_ref in img_refs:\n",
    "                print(\"\")\n",
    "                print(\"ref_id: \", img_ref[\"ref_id\"])\n",
    "                print(\"# sentences: \", len(img_ref[\"sentences\"]))\n",
    "                # print(\"# sentences: \", len([ref img_refs[\"sentences\"]]))\n",
    "                for s in img_ref[\"sentences\"]:\n",
    "                    print(\n",
    "                        \"sentence: \",\n",
    "                        s[\"sent_id\"],\n",
    "                        s[\"sent\"],\n",
    "                        (\n",
    "                            f\"gt: ({s['gt_sent_id']}) {s['is_false_premise']}, {s['gt_sent']}\"\n",
    "                        )\n",
    "                        if \"is_false_premise\" in s\n",
    "                        else \"\",\n",
    "                    )\n",
    "\n",
    "            for i in range(len(img_fps)):\n",
    "                display(\n",
    "                    \"img_fps: \",\n",
    "                    [\n",
    "                        f\"'{k}': {v}\"\n",
    "                        for k, v in img_fps[i].items()\n",
    "                        if k\n",
    "                        in {\n",
    "                            \"sent_id\",\n",
    "                            \"main_subject\",\n",
    "                            \"ref_id\",\n",
    "                            \"ann_id\",\n",
    "                            \"gt_sent_id\",\n",
    "                            \"gt_sent\",\n",
    "                            \"sent\",\n",
    "                        }\n",
    "                    ],\n",
    "                )\n",
    "            DEBUG = False\n",
    "\n",
    "    print(\n",
    "        f\"num_match: {num_match}, not_match: {num_not_match}, num_corrected: {num_corrected}\"\n",
    "    )\n",
    "    return refcoco\n",
    "\n",
    "\n",
    "# REFSEG_DIR = Path(\"/shared/gbiamby/data/refer_seg\")\n",
    "# REFSEG_DIR = Path(\"output/ref_seg\")\n",
    "# refcoco = build_refcoco(REFSEG_DIR, \"refcocog\", \"google_enhanced\")\n",
    "refcoco_new = match_fp_with_gt(fp_sent_dicts, doc_to_image, refcoco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Detect Which Part of Each FP Sentence is Changed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|████████████████████▍                                                                                                                                                                                                                                                  | 3885/50000 [00:00<00:02, 19901.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['creature']\n",
      "zebra creature front and center\n",
      "unicorn creature front and center\n",
      "[('-', ['zebra']), ('+', ['unicorn']), ('=', ['creature', 'front', 'and', 'center'])]\n",
      "num changes:  1\n",
      "\n",
      "['unicorn']\n",
      "zebra\n",
      "unicorn\n",
      "[('-', ['zebra']), ('+', ['unicorn'])]\n",
      "num changes:  1\n",
      "\n",
      "['unicorn']\n",
      "whole zebra\n",
      "whole unicorn\n",
      "[('=', ['whole']), ('-', ['zebra']), ('+', ['unicorn'])]\n",
      "num changes:  1\n",
      "\n",
      "['rhino']\n",
      "left most buny\n",
      "left most rhino\n",
      "[('=', ['left', 'most']), ('-', ['buny']), ('+', ['rhino'])]\n",
      "num changes:  1\n",
      "\n",
      "['panda']\n",
      "left side bunny\n",
      "left side panda\n",
      "[('=', ['left', 'side']), ('-', ['bunny']), ('+', ['panda'])]\n",
      "num changes:  1\n",
      "\n",
      "['left']\n",
      "left most bunny\n",
      "left most unicorn\n",
      "[('=', ['left', 'most']), ('-', ['bunny']), ('+', ['unicorn'])]\n",
      "num changes:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:02<00:00, 23384.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>gt_subj</th>\n",
       "      <th>fp_subj</th>\n",
       "      <th>gt_sent</th>\n",
       "      <th>fp_sent</th>\n",
       "      <th>num_changes</th>\n",
       "      <th>num_subs</th>\n",
       "      <th>num_del</th>\n",
       "      <th>num_add</th>\n",
       "      <th>diff_ops</th>\n",
       "      <th>diff</th>\n",
       "      <th>gt_NOUN_CHUNKS</th>\n",
       "      <th>fp_NOUN_CHUNKS</th>\n",
       "      <th>change_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4808</td>\n",
       "      <td>526754</td>\n",
       "      <td>24</td>\n",
       "      <td>[creature]</td>\n",
       "      <td>[creature]</td>\n",
       "      <td>zebra creature front and center</td>\n",
       "      <td>unicorn creature front and center</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-, +, =)</td>\n",
       "      <td>[(-, [zebra]), (+, [unicorn]), (=, [creature, ...</td>\n",
       "      <td>[zebra creature]</td>\n",
       "      <td>[unicorn creature]</td>\n",
       "      <td>other_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4808</td>\n",
       "      <td>526754</td>\n",
       "      <td>24</td>\n",
       "      <td>[zebra]</td>\n",
       "      <td>[unicorn]</td>\n",
       "      <td>zebra</td>\n",
       "      <td>unicorn</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-, +)</td>\n",
       "      <td>[(-, [zebra]), (+, [unicorn])]</td>\n",
       "      <td>[zebra]</td>\n",
       "      <td>[unicorn]</td>\n",
       "      <td>main_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4808</td>\n",
       "      <td>526754</td>\n",
       "      <td>24</td>\n",
       "      <td>[zebra]</td>\n",
       "      <td>[unicorn]</td>\n",
       "      <td>whole zebra</td>\n",
       "      <td>whole unicorn</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [whole]), (-, [zebra]), (+, [unicorn])]</td>\n",
       "      <td>[whole zebra]</td>\n",
       "      <td>[whole unicorn]</td>\n",
       "      <td>main_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37010</td>\n",
       "      <td>150948</td>\n",
       "      <td>61</td>\n",
       "      <td>[buny]</td>\n",
       "      <td>[rhino]</td>\n",
       "      <td>left most buny</td>\n",
       "      <td>left most rhino</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [left, most]), (-, [buny]), (+, [rhino])]</td>\n",
       "      <td>[left most buny]</td>\n",
       "      <td>[left most rhino]</td>\n",
       "      <td>main_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37010</td>\n",
       "      <td>150948</td>\n",
       "      <td>61</td>\n",
       "      <td>[bunny]</td>\n",
       "      <td>[panda]</td>\n",
       "      <td>left side bunny</td>\n",
       "      <td>left side panda</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [left, side]), (-, [bunny]), (+, [panda])]</td>\n",
       "      <td>[left side bunny]</td>\n",
       "      <td>[left side panda]</td>\n",
       "      <td>main_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141138</th>\n",
       "      <td>27625</td>\n",
       "      <td>263111</td>\n",
       "      <td>28</td>\n",
       "      <td>[umb]</td>\n",
       "      <td>[umb]</td>\n",
       "      <td>rainbow umb</td>\n",
       "      <td>unicorn umb</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-, +, =)</td>\n",
       "      <td>[(-, [rainbow]), (+, [unicorn]), (=, [umb])]</td>\n",
       "      <td>[rainbow umb]</td>\n",
       "      <td>[unicorn umb]</td>\n",
       "      <td>other_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141139</th>\n",
       "      <td>27625</td>\n",
       "      <td>263111</td>\n",
       "      <td>28</td>\n",
       "      <td>[umbrella]</td>\n",
       "      <td>[umbrella]</td>\n",
       "      <td>colorful umbrella</td>\n",
       "      <td>magical umbrella</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-, +, =)</td>\n",
       "      <td>[(-, [colorful]), (+, [magical]), (=, [umbrell...</td>\n",
       "      <td>[colorful umbrella]</td>\n",
       "      <td>[magical umbrella]</td>\n",
       "      <td>other_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141140</th>\n",
       "      <td>35602</td>\n",
       "      <td>168643</td>\n",
       "      <td>63</td>\n",
       "      <td>[couch]</td>\n",
       "      <td>[bed]</td>\n",
       "      <td>left couch</td>\n",
       "      <td>left bed</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [left]), (-, [couch]), (+, [bed])]</td>\n",
       "      <td>[left couch]</td>\n",
       "      <td>[left bed]</td>\n",
       "      <td>main_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141141</th>\n",
       "      <td>35602</td>\n",
       "      <td>168643</td>\n",
       "      <td>63</td>\n",
       "      <td>[couch]</td>\n",
       "      <td>[bed]</td>\n",
       "      <td>3 cushion couch</td>\n",
       "      <td>3 cushion bed</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [3, cushion]), (-, [couch]), (+, [bed])]</td>\n",
       "      <td>[3 cushion couch]</td>\n",
       "      <td>[3 cushion bed]</td>\n",
       "      <td>main_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141142</th>\n",
       "      <td>35602</td>\n",
       "      <td>168643</td>\n",
       "      <td>63</td>\n",
       "      <td>[couch]</td>\n",
       "      <td>[bed]</td>\n",
       "      <td>left couch</td>\n",
       "      <td>left bed</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [left]), (-, [couch]), (+, [bed])]</td>\n",
       "      <td>[left couch]</td>\n",
       "      <td>[left bed]</td>\n",
       "      <td>main_subject</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141143 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ref_id  image_id  cat_id     gt_subj     fp_subj  \\\n",
       "0         4808    526754      24  [creature]  [creature]   \n",
       "1         4808    526754      24     [zebra]   [unicorn]   \n",
       "2         4808    526754      24     [zebra]   [unicorn]   \n",
       "3        37010    150948      61      [buny]     [rhino]   \n",
       "4        37010    150948      61     [bunny]     [panda]   \n",
       "...        ...       ...     ...         ...         ...   \n",
       "141138   27625    263111      28       [umb]       [umb]   \n",
       "141139   27625    263111      28  [umbrella]  [umbrella]   \n",
       "141140   35602    168643      63     [couch]       [bed]   \n",
       "141141   35602    168643      63     [couch]       [bed]   \n",
       "141142   35602    168643      63     [couch]       [bed]   \n",
       "\n",
       "                                gt_sent                            fp_sent  \\\n",
       "0       zebra creature front and center  unicorn creature front and center   \n",
       "1                                 zebra                            unicorn   \n",
       "2                           whole zebra                      whole unicorn   \n",
       "3                        left most buny                    left most rhino   \n",
       "4                       left side bunny                    left side panda   \n",
       "...                                 ...                                ...   \n",
       "141138                      rainbow umb                        unicorn umb   \n",
       "141139                colorful umbrella                   magical umbrella   \n",
       "141140                       left couch                           left bed   \n",
       "141141                  3 cushion couch                      3 cushion bed   \n",
       "141142                       left couch                           left bed   \n",
       "\n",
       "        num_changes  num_subs  num_del  num_add   diff_ops  \\\n",
       "0                 1         1        0        0  (-, +, =)   \n",
       "1                 1         1        0        0     (-, +)   \n",
       "2                 1         1        0        0  (=, -, +)   \n",
       "3                 1         1        0        0  (=, -, +)   \n",
       "4                 1         1        0        0  (=, -, +)   \n",
       "...             ...       ...      ...      ...        ...   \n",
       "141138            1         1        0        0  (-, +, =)   \n",
       "141139            1         1        0        0  (-, +, =)   \n",
       "141140            1         1        0        0  (=, -, +)   \n",
       "141141            1         1        0        0  (=, -, +)   \n",
       "141142            1         1        0        0  (=, -, +)   \n",
       "\n",
       "                                                     diff  \\\n",
       "0       [(-, [zebra]), (+, [unicorn]), (=, [creature, ...   \n",
       "1                          [(-, [zebra]), (+, [unicorn])]   \n",
       "2            [(=, [whole]), (-, [zebra]), (+, [unicorn])]   \n",
       "3          [(=, [left, most]), (-, [buny]), (+, [rhino])]   \n",
       "4         [(=, [left, side]), (-, [bunny]), (+, [panda])]   \n",
       "...                                                   ...   \n",
       "141138       [(-, [rainbow]), (+, [unicorn]), (=, [umb])]   \n",
       "141139  [(-, [colorful]), (+, [magical]), (=, [umbrell...   \n",
       "141140            [(=, [left]), (-, [couch]), (+, [bed])]   \n",
       "141141      [(=, [3, cushion]), (-, [couch]), (+, [bed])]   \n",
       "141142            [(=, [left]), (-, [couch]), (+, [bed])]   \n",
       "\n",
       "             gt_NOUN_CHUNKS      fp_NOUN_CHUNKS    change_type  \n",
       "0          [zebra creature]  [unicorn creature]  other_subject  \n",
       "1                   [zebra]           [unicorn]   main_subject  \n",
       "2             [whole zebra]     [whole unicorn]   main_subject  \n",
       "3          [left most buny]   [left most rhino]   main_subject  \n",
       "4         [left side bunny]   [left side panda]   main_subject  \n",
       "...                     ...                 ...            ...  \n",
       "141138        [rainbow umb]       [unicorn umb]  other_subject  \n",
       "141139  [colorful umbrella]  [magical umbrella]  other_subject  \n",
       "141140         [left couch]          [left bed]   main_subject  \n",
       "141141    [3 cushion couch]     [3 cushion bed]   main_subject  \n",
       "141142         [left couch]          [left bed]   main_subject  \n",
       "\n",
       "[141143 rows x 16 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_num_changes(diff):\n",
    "    diff_ops = [d[0] for d in diff]\n",
    "    num_subs = \"\".join(diff_ops).count(\"-+\")\n",
    "    num_deletions = \"\".join(diff_ops).replace(\"-+\", \"\").count(\"-\")\n",
    "    num_additions = \"\".join(diff_ops).replace(\"-+\", \"\").count(\"+\")\n",
    "    total_changes = num_subs + num_deletions + num_additions\n",
    "    return total_changes, num_subs, num_deletions, num_additions\n",
    "\n",
    "\n",
    "def get_sentence_lookup(refcoco: COCO) -> dict[int, dict[str, Any]]:\n",
    "    \"\"\"Returns dict of all the sentences, with sent_id as the key\"\"\"\n",
    "    sent_lookup = {}\n",
    "    for ref_id, ref in refcoco.refs.items():\n",
    "        for sent in ref[\"sentences\"]:\n",
    "            if \"is_false_premise\" in sent and sent[\"is_false_premise\"]:\n",
    "                continue\n",
    "            sent_lookup[sent[\"sent_id\"]] = sent\n",
    "    return sent_lookup\n",
    "\n",
    "\n",
    "def get_change_type(fp_sent: dict, change_info: dict):\n",
    "    change_type = \"\"\n",
    "\n",
    "    if change_info[\"num_changes\"] == 1 and change_info[\"num_subs\"] == 1:\n",
    "        # Sentence has a single change. Get the subtracted and added words:\n",
    "        _sub_words, _add_words = None, None\n",
    "        for op, words in change_info[\"diff\"]:\n",
    "            # print(op)\n",
    "            if op == \"-\":\n",
    "                _sub_words = words\n",
    "                # print(\"_sub_words \", _sub_words)\n",
    "            if op == \"+\":\n",
    "                _add_words = words\n",
    "                # print(\"_add_words \",_add_words)\n",
    "                break\n",
    "        assert _sub_words, str(change_info[\"diff\"])\n",
    "        assert _add_words, str(change_info[\"diff\"])\n",
    "        # Categorize:\n",
    "        gt_subject = change_info[\"gt_subj\"]\n",
    "        if isinstance(gt_subject, list) and len(gt_subject) > 0:\n",
    "            gt_subject = gt_subject[0]\n",
    "        if gt_subject in _sub_words:\n",
    "            change_type = \"main_subject\"\n",
    "            if len(_sub_words) > 1:\n",
    "                change_type += f\"(+{len(_sub_words)})\"\n",
    "        else:\n",
    "            change_type = \"NOT_MAIN_SUBJ\"\n",
    "            new_phrase = \" \".join(_add_words)\n",
    "            for nc in change_info[\"fp_NOUN_CHUNKS\"]:\n",
    "                if new_phrase in nc:\n",
    "                    change_type = \"other_subject\"\n",
    "\n",
    "    return change_type\n",
    "\n",
    "\n",
    "def detect_changes(refcoco: COCO):\n",
    "    sent_lookup = get_sentence_lookup(refcoco)\n",
    "    items = []\n",
    "    for idx, (ref_id, ref) in tqdm(\n",
    "        enumerate(refcoco.refs.items()), total=len(refcoco.refs)\n",
    "    ):\n",
    "        sentences: list[str] = [\n",
    "            s\n",
    "            for s in ref[\"sentences\"]\n",
    "            if (\"is_false_premise\" in s and s[\"is_false_premise\"])\n",
    "        ]\n",
    "        for sent in sentences:\n",
    "            gt_sent = sent_lookup[sent[\"gt_sent_id\"]]\n",
    "            diffs = string_diff(sent[\"gt_sent\"], sent[\"sent\"])\n",
    "            num_changes, subs, deletions, additions = get_num_changes(diffs)\n",
    "            items.append(\n",
    "                {\n",
    "                    \"ref_id\": ref_id,\n",
    "                    \"image_id\": ref[\"image_id\"],\n",
    "                    \"cat_id\": ref[\"category_id\"],\n",
    "                    \"gt_subj\": get_main_subject(gt_sent),\n",
    "                    \"fp_subj\": sent[\"main_subject\"],\n",
    "                    \"gt_sent\": sent[\"gt_sent\"],\n",
    "                    \"fp_sent\": sent[\"sent\"],\n",
    "                    \"num_changes\": num_changes,\n",
    "                    \"num_subs\": subs,\n",
    "                    \"num_del\": deletions,\n",
    "                    \"num_add\": additions,\n",
    "                    \"diff_ops\": tuple([d[0] for d in diffs]),\n",
    "                    \"diff\": diffs,\n",
    "                    \"gt_NOUN_CHUNKS\": gt_sent[\"spcy_NOUN_CHUNKS\"],\n",
    "                    \"fp_NOUN_CHUNKS\": sent[\"spcy_NOUN_CHUNKS\"],\n",
    "                }\n",
    "            )\n",
    "            items[-1][\"change_type\"] = get_change_type(sent, items[-1])\n",
    "            sent[\"change_type\"] = items[-1][\"change_type\"]\n",
    "            if idx < 2:\n",
    "                print(\"\")\n",
    "                print(sent[\"main_subject\"])\n",
    "                print(sent[\"gt_sent\"])\n",
    "                print(sent[\"sent\"])\n",
    "                print(diffs)\n",
    "                print(\"num changes: \", num_changes)\n",
    "\n",
    "    return pd.DataFrame(items)\n",
    "\n",
    "\n",
    "df_changes = detect_changes(refcoco_new)\n",
    "display(df_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_changes</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              total\n",
       "num_changes        \n",
       "0              6424\n",
       "1            122366\n",
       "2             11606\n",
       "3               674\n",
       "4                65\n",
       "5                 5\n",
       "6                 2\n",
       "7                 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_changes</th>\n",
       "      <th>diff_ops</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>(=,)</th>\n",
       "      <td>6424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th>(-, +, =)</th>\n",
       "      <td>42511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(=, -, +)</th>\n",
       "      <td>38689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(=, -, +, =)</th>\n",
       "      <td>22860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-, +)</th>\n",
       "      <td>17999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>(=, -, +, =, -, +, =, -, +, =, -, +, =, -, +, =)</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-, +, =, -, +, =, -, +, =, -, +, =, -, +)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">6</th>\n",
       "      <th>(-, +, =, -, +, =, -, +, =, -, +, =, -, +, =, -, +, =)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(=, -, +, =, -, +, =, -, +, =, -, +, =, -, +, =, -, +, =)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <th>(=, -, +, =, -, +, =, -, +, =, -, +, =, -, +, =, -, +, =, -, +)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                total\n",
       "num_changes diff_ops                                                 \n",
       "0           (=,)                                                 6424\n",
       "1           (-, +, =)                                           42511\n",
       "            (=, -, +)                                           38689\n",
       "            (=, -, +, =)                                        22860\n",
       "            (-, +)                                              17999\n",
       "...                                                               ...\n",
       "5           (=, -, +, =, -, +, =, -, +, =, -, +, =, -, +, =)        2\n",
       "            (-, +, =, -, +, =, -, +, =, -, +, =, -, +)              1\n",
       "6           (-, +, =, -, +, =, -, +, =, -, +, =, -, +, =, -...      1\n",
       "            (=, -, +, =, -, +, =, -, +, =, -, +, =, -, +, =...      1\n",
       "7           (=, -, +, =, -, +, =, -, +, =, -, +, =, -, +, =...      1\n",
       "\n",
       "[67 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_changes</th>\n",
       "      <th>num_subs</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>6424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>122059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>2</th>\n",
       "      <td>11456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>3</th>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">4</th>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       total\n",
       "num_changes num_subs        \n",
       "0           0           6424\n",
       "1           1         122059\n",
       "            0            307\n",
       "2           2          11456\n",
       "            0             82\n",
       "            1             68\n",
       "3           3            657\n",
       "            1             10\n",
       "            2              7\n",
       "4           4             62\n",
       "            1              1\n",
       "            2              1\n",
       "            3              1\n",
       "5           5              5\n",
       "6           6              2\n",
       "7           7              1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>19084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOT_MAIN_SUBJ</th>\n",
       "      <td>7965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_subject</th>\n",
       "      <td>59055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_subject(+2)</th>\n",
       "      <td>6818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_subject(+3)</th>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_subject(+4)</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_subject(+5)</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_subject(+6)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_subject(+9)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_subject</th>\n",
       "      <td>47825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  total\n",
       "change_type            \n",
       "                  19084\n",
       "NOT_MAIN_SUBJ      7965\n",
       "main_subject      59055\n",
       "main_subject(+2)   6818\n",
       "main_subject(+3)    372\n",
       "main_subject(+4)     20\n",
       "main_subject(+5)      2\n",
       "main_subject(+6)      1\n",
       "main_subject(+9)      1\n",
       "other_subject     47825"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    df_changes.groupby([\"num_changes\"], dropna=False).agg(total=(\"ref_id\", \"count\"))\n",
    ")\n",
    "display(\n",
    "    df_changes.groupby([\"num_changes\", \"diff_ops\"], dropna=False)\n",
    "    .agg(total=(\"ref_id\", \"count\"))\n",
    "    .sort_values([\"num_changes\", \"total\"], ascending=[True, False])\n",
    ")\n",
    "display(\n",
    "    df_changes.groupby([\"num_changes\", \"num_subs\"], dropna=False)\n",
    "    .agg(total=(\"ref_id\", \"count\"))\n",
    "    .sort_values([\"num_changes\", \"total\"], ascending=[True, False])\n",
    ")\n",
    "display(\n",
    "    df_changes.groupby([\"change_type\"], dropna=False).agg(total=(\"ref_id\", \"count\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>gt_subj</th>\n",
       "      <th>fp_subj</th>\n",
       "      <th>gt_sent</th>\n",
       "      <th>fp_sent</th>\n",
       "      <th>num_changes</th>\n",
       "      <th>num_subs</th>\n",
       "      <th>num_del</th>\n",
       "      <th>num_add</th>\n",
       "      <th>diff_ops</th>\n",
       "      <th>diff</th>\n",
       "      <th>gt_NOUN_CHUNKS</th>\n",
       "      <th>fp_NOUN_CHUNKS</th>\n",
       "      <th>change_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>46535</td>\n",
       "      <td>40912</td>\n",
       "      <td>1</td>\n",
       "      <td>[figure]</td>\n",
       "      <td>[figure]</td>\n",
       "      <td>figure on left</td>\n",
       "      <td>figure on right</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [figure, on]), (-, [left]), (+, [right])]</td>\n",
       "      <td>[figure, left]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6407</td>\n",
       "      <td>508281</td>\n",
       "      <td>1</td>\n",
       "      <td>[boy]</td>\n",
       "      <td>[boy]</td>\n",
       "      <td>boy on the bottom right reaching</td>\n",
       "      <td>boy on the bottom right sneezing</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [boy, on, the, bottom, right]), (-, [reaching]), (+, [sneezing])]</td>\n",
       "      <td>[boy, right]</td>\n",
       "      <td>[boy, the bottom right]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>6407</td>\n",
       "      <td>508281</td>\n",
       "      <td>1</td>\n",
       "      <td>[kid]</td>\n",
       "      <td>[kid]</td>\n",
       "      <td>kid reaching for shelf bottom right corner</td>\n",
       "      <td>kid sneezing for shelf bottom right corner</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +, =)</td>\n",
       "      <td>[(=, [kid]), (-, [reaching]), (+, [sneezing]), (=, [for, shelf, bottom, right, corner])]</td>\n",
       "      <td>[kid, shelf, bottom right corner]</td>\n",
       "      <td>[kid, shelf]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>41742</td>\n",
       "      <td>94828</td>\n",
       "      <td>25</td>\n",
       "      <td>[head]</td>\n",
       "      <td>[right]</td>\n",
       "      <td>giraffe head right</td>\n",
       "      <td>elephant head right</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-, +, =)</td>\n",
       "      <td>[(-, [giraffe]), (+, [elephant]), (=, [head, right])]</td>\n",
       "      <td>[giraffe head]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>36953</td>\n",
       "      <td>151434</td>\n",
       "      <td>4</td>\n",
       "      <td>[closest]</td>\n",
       "      <td>[closest]</td>\n",
       "      <td>silver bike closest</td>\n",
       "      <td>silver fish closest</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +, =)</td>\n",
       "      <td>[(=, [silver]), (-, [bike]), (+, [fish]), (=, [closest])]</td>\n",
       "      <td>[silver bike closest]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141090</th>\n",
       "      <td>29346</td>\n",
       "      <td>243645</td>\n",
       "      <td>52</td>\n",
       "      <td>[right]</td>\n",
       "      <td>[right]</td>\n",
       "      <td>banana top right</td>\n",
       "      <td>orange top right</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-, +, =)</td>\n",
       "      <td>[(-, [banana]), (+, [orange]), (=, [top, right])]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141091</th>\n",
       "      <td>29346</td>\n",
       "      <td>243645</td>\n",
       "      <td>52</td>\n",
       "      <td>[right]</td>\n",
       "      <td>[pear]</td>\n",
       "      <td>banana top right</td>\n",
       "      <td>pear top right</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-, +, =)</td>\n",
       "      <td>[(-, [banana]), (+, [pear]), (=, [top, right])]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141108</th>\n",
       "      <td>6613</td>\n",
       "      <td>506030</td>\n",
       "      <td>1</td>\n",
       "      <td>[person]</td>\n",
       "      <td>[dancing]</td>\n",
       "      <td>person on right reaching out</td>\n",
       "      <td>person on right dancing</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [person, on, right]), (-, [reaching, out]), (+, [dancing])]</td>\n",
       "      <td>[person, right]</td>\n",
       "      <td>[right]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141120</th>\n",
       "      <td>31988</td>\n",
       "      <td>211955</td>\n",
       "      <td>1</td>\n",
       "      <td>[girl]</td>\n",
       "      <td>[girl]</td>\n",
       "      <td>little girl on the right leaning over</td>\n",
       "      <td>little girl on the right dancing</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [little, girl, on, the, right]), (-, [leaning, over]), (+, [dancing])]</td>\n",
       "      <td>[little girl, the right]</td>\n",
       "      <td>[little girl, the right]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141121</th>\n",
       "      <td>31988</td>\n",
       "      <td>211955</td>\n",
       "      <td>1</td>\n",
       "      <td>[girl]</td>\n",
       "      <td>[girl]</td>\n",
       "      <td>little girl on right leaning over table</td>\n",
       "      <td>little girl on right dancing over table</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +, =)</td>\n",
       "      <td>[(=, [little, girl, on, right]), (-, [leaning]), (+, [dancing]), (=, [over, table])]</td>\n",
       "      <td>[little girl, table]</td>\n",
       "      <td>[little girl, table]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7965 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ref_id  image_id  cat_id    gt_subj    fp_subj  \\\n",
       "29       46535     40912       1   [figure]   [figure]   \n",
       "39        6407    508281       1      [boy]      [boy]   \n",
       "40        6407    508281       1      [kid]      [kid]   \n",
       "63       41742     94828      25     [head]    [right]   \n",
       "119      36953    151434       4  [closest]  [closest]   \n",
       "...        ...       ...     ...        ...        ...   \n",
       "141090   29346    243645      52    [right]    [right]   \n",
       "141091   29346    243645      52    [right]     [pear]   \n",
       "141108    6613    506030       1   [person]  [dancing]   \n",
       "141120   31988    211955       1     [girl]     [girl]   \n",
       "141121   31988    211955       1     [girl]     [girl]   \n",
       "\n",
       "                                           gt_sent  \\\n",
       "29                                  figure on left   \n",
       "39                boy on the bottom right reaching   \n",
       "40      kid reaching for shelf bottom right corner   \n",
       "63                              giraffe head right   \n",
       "119                            silver bike closest   \n",
       "...                                            ...   \n",
       "141090                            banana top right   \n",
       "141091                            banana top right   \n",
       "141108                person on right reaching out   \n",
       "141120       little girl on the right leaning over   \n",
       "141121     little girl on right leaning over table   \n",
       "\n",
       "                                           fp_sent  num_changes  num_subs  \\\n",
       "29                                 figure on right            1         1   \n",
       "39                boy on the bottom right sneezing            1         1   \n",
       "40      kid sneezing for shelf bottom right corner            1         1   \n",
       "63                             elephant head right            1         1   \n",
       "119                            silver fish closest            1         1   \n",
       "...                                            ...          ...       ...   \n",
       "141090                            orange top right            1         1   \n",
       "141091                              pear top right            1         1   \n",
       "141108                     person on right dancing            1         1   \n",
       "141120            little girl on the right dancing            1         1   \n",
       "141121     little girl on right dancing over table            1         1   \n",
       "\n",
       "        num_del  num_add      diff_ops  \\\n",
       "29            0        0     (=, -, +)   \n",
       "39            0        0     (=, -, +)   \n",
       "40            0        0  (=, -, +, =)   \n",
       "63            0        0     (-, +, =)   \n",
       "119           0        0  (=, -, +, =)   \n",
       "...         ...      ...           ...   \n",
       "141090        0        0     (-, +, =)   \n",
       "141091        0        0     (-, +, =)   \n",
       "141108        0        0     (=, -, +)   \n",
       "141120        0        0     (=, -, +)   \n",
       "141121        0        0  (=, -, +, =)   \n",
       "\n",
       "                                                                                            diff  \\\n",
       "29                                                [(=, [figure, on]), (-, [left]), (+, [right])]   \n",
       "39                        [(=, [boy, on, the, bottom, right]), (-, [reaching]), (+, [sneezing])]   \n",
       "40      [(=, [kid]), (-, [reaching]), (+, [sneezing]), (=, [for, shelf, bottom, right, corner])]   \n",
       "63                                         [(-, [giraffe]), (+, [elephant]), (=, [head, right])]   \n",
       "119                                    [(=, [silver]), (-, [bike]), (+, [fish]), (=, [closest])]   \n",
       "...                                                                                          ...   \n",
       "141090                                         [(-, [banana]), (+, [orange]), (=, [top, right])]   \n",
       "141091                                           [(-, [banana]), (+, [pear]), (=, [top, right])]   \n",
       "141108                          [(=, [person, on, right]), (-, [reaching, out]), (+, [dancing])]   \n",
       "141120               [(=, [little, girl, on, the, right]), (-, [leaning, over]), (+, [dancing])]   \n",
       "141121      [(=, [little, girl, on, right]), (-, [leaning]), (+, [dancing]), (=, [over, table])]   \n",
       "\n",
       "                           gt_NOUN_CHUNKS            fp_NOUN_CHUNKS  \\\n",
       "29                         [figure, left]                        []   \n",
       "39                           [boy, right]   [boy, the bottom right]   \n",
       "40      [kid, shelf, bottom right corner]              [kid, shelf]   \n",
       "63                         [giraffe head]                        []   \n",
       "119                 [silver bike closest]                        []   \n",
       "...                                   ...                       ...   \n",
       "141090                                 []                        []   \n",
       "141091                                 []                        []   \n",
       "141108                    [person, right]                   [right]   \n",
       "141120           [little girl, the right]  [little girl, the right]   \n",
       "141121               [little girl, table]      [little girl, table]   \n",
       "\n",
       "          change_type  \n",
       "29      NOT_MAIN_SUBJ  \n",
       "39      NOT_MAIN_SUBJ  \n",
       "40      NOT_MAIN_SUBJ  \n",
       "63      NOT_MAIN_SUBJ  \n",
       "119     NOT_MAIN_SUBJ  \n",
       "...               ...  \n",
       "141090  NOT_MAIN_SUBJ  \n",
       "141091  NOT_MAIN_SUBJ  \n",
       "141108  NOT_MAIN_SUBJ  \n",
       "141120  NOT_MAIN_SUBJ  \n",
       "141121  NOT_MAIN_SUBJ  \n",
       "\n",
       "[7965 rows x 16 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with pd.option_context(\n",
    "#     \"display.max_colwidth\", None, \"display.max_columns\", None, \"display.max_rows\", 200\n",
    "# ):\n",
    "#     display(df_changes[df_changes.diff_ops == (\"-\", \"+\")])\n",
    "\n",
    "# with pd.option_context(\n",
    "#     \"display.max_colwidth\", None, \"display.max_columns\", None, \"display.max_rows\", 200\n",
    "# ):\n",
    "#     display(df_changes[df_changes.num_changes == 1])\n",
    "\n",
    "\n",
    "with pd.option_context(\n",
    "    \"display.max_colwidth\", None, \"display.max_columns\", None, \"display.max_rows\", 200\n",
    "):\n",
    "    display(df_changes[df_changes.change_type == \"NOT_MAIN_SUBJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 295284.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_sents: 283353, total_kept: 276930\n"
     ]
    }
   ],
   "source": [
    "def scrub(refcoco: COCO):\n",
    "    refcoco = deepcopy(refcoco)\n",
    "    total_sents = 0\n",
    "    total_sents_kept = 0\n",
    "    for ref_id, ref in tqdm(list(refcoco.refs.items()), total=len(refcoco.refs)):\n",
    "        sentences_new = []\n",
    "        total_sents += len(ref[\"sentences\"])\n",
    "        for s in ref[\"sentences\"]:\n",
    "            if \"is_false_premise\" not in s:\n",
    "                s[\"is_false_premise\"] = False\n",
    "            s[\"exist\"] = not s[\"is_false_premise\"]\n",
    "            if s[\"is_false_premise\"] and (s[\"sent\"] == s[\"gt_sent\"]):\n",
    "                continue\n",
    "            # if not s[\"change_type\"]:\n",
    "            #     s[\"change_type\"]\n",
    "            sentences_new.append(s)\n",
    "        total_sents_kept += len(sentences_new)\n",
    "        ref[\"sentences\"] = sentences_new\n",
    "    print(f\"total_sents: {total_sents}, total_kept: {total_sents_kept}\")\n",
    "    return refcoco\n",
    "\n",
    "\n",
    "refcoco_new_scrubbed = scrub(refcoco_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save new RefCOCO Dataset\n",
    "\n",
    "Enhanced version augments the `ref[\"sentences\"]` dictionaries with spacy tagging info (parts of speech, dependency parsing, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sent_ids': [2836, 2837, 2838], 'file_name': 'COCO_train2014_000000570951_4.jpg', 'ann_id': 500240, 'ref_id': 1000, 'image_id': 570951, 'split': 'val', 'sentences': [{'tokens': ['chef', 'in', 'back'], 'raw': 'chef in back', 'sent_id': 2836, 'sent': 'chef in back', 'spcy_WORD': ['chef', 'in', 'back'], 'spcy_DEP': ['ROOT', 'prep', 'pobj'], 'spcy_POS': ['PROPN', 'ADP', 'NOUN'], 'spcy_LEM': ['chef', 'in', 'back'], 'spcy_TAG': ['NNP', 'IN', 'NN'], 'spcy_IS_STOP': [False, True, True], 'spcy_ENTS': [], 'spcy_NOUN_CHUNKS': ['chef', 'back'], 'is_false_premise': False, 'exist': True}, {'tokens': ['right', 'guy'], 'raw': 'right guy', 'sent_id': 2837, 'sent': 'right guy', 'spcy_WORD': ['right', 'guy'], 'spcy_DEP': ['intj', 'ROOT'], 'spcy_POS': ['INTJ', 'NOUN'], 'spcy_LEM': ['right', 'guy'], 'spcy_TAG': ['UH', 'NN'], 'spcy_IS_STOP': [False, False], 'spcy_ENTS': [], 'spcy_NOUN_CHUNKS': ['right guy'], 'is_false_premise': False, 'exist': True}, {'tokens': ['cook', 'in', 'the', 'back'], 'raw': 'cook in the back', 'sent_id': 2838, 'sent': 'cook in the back', 'spcy_WORD': ['cook', 'in', 'the', 'back'], 'spcy_DEP': ['ROOT', 'prep', 'det', 'pobj'], 'spcy_POS': ['VERB', 'ADP', 'DET', 'NOUN'], 'spcy_LEM': ['cook', 'in', 'the', 'back'], 'spcy_TAG': ['VB', 'IN', 'DT', 'NN'], 'spcy_IS_STOP': [False, True, True, True], 'spcy_ENTS': [], 'spcy_NOUN_CHUNKS': ['the back'], 'is_false_premise': False, 'exist': True}, {'tokens': ['plumber', 'in', 'back'], 'raw': 'plumber in back', 'sent_id': -1, 'sent': 'plumber in back', 'spcy_WORD': ['plumber', 'in', 'back'], 'spcy_DEP': ['ROOT', 'prep', 'pobj'], 'spcy_POS': ['NOUN', 'ADP', 'NOUN'], 'spcy_LEM': ['plumber', 'in', 'back'], 'spcy_TAG': ['NN', 'IN', 'NN'], 'spcy_IS_STOP': [False, True, True], 'spcy_ENTS': [], 'spcy_NOUN_CHUNKS': ['plumber', 'back'], 'main_subject': ['plumber'], 'ref_id': 1000, 'ann_id': 500240, 'gt_sent_id': 2836, 'gt_sent': 'chef in back', 'is_false_premise': True, 'change_type': 'main_subject', 'exist': False}, {'tokens': ['right', 'alien'], 'raw': 'right alien', 'sent_id': -1, 'sent': 'right alien', 'spcy_WORD': ['right', 'alien'], 'spcy_DEP': ['intj', 'ROOT'], 'spcy_POS': ['INTJ', 'NOUN'], 'spcy_LEM': ['right', 'alien'], 'spcy_TAG': ['UH', 'NN'], 'spcy_IS_STOP': [False, False], 'spcy_ENTS': [], 'spcy_NOUN_CHUNKS': ['right alien'], 'main_subject': ['alien'], 'ref_id': 1000, 'ann_id': 500240, 'gt_sent_id': 2837, 'gt_sent': 'right guy', 'is_false_premise': True, 'change_type': 'main_subject', 'exist': False}, {'tokens': ['engineer', 'in', 'the', 'back'], 'raw': 'engineer in the back', 'sent_id': -1, 'sent': 'engineer in the back', 'spcy_WORD': ['engineer', 'in', 'the', 'back'], 'spcy_DEP': ['ROOT', 'prep', 'det', 'pobj'], 'spcy_POS': ['NOUN', 'ADP', 'DET', 'NOUN'], 'spcy_LEM': ['engineer', 'in', 'the', 'back'], 'spcy_TAG': ['NN', 'IN', 'DT', 'NN'], 'spcy_IS_STOP': [False, True, True, True], 'spcy_ENTS': [], 'spcy_NOUN_CHUNKS': ['engineer', 'the back'], 'main_subject': ['engineer'], 'ref_id': 1000, 'ann_id': 500240, 'gt_sent_id': 2838, 'gt_sent': 'cook in the back', 'is_false_premise': True, 'change_type': 'main_subject', 'exist': False}], 'category_id': 1}\n",
      "\n",
      "{'sent_ids': [74022, 74023, 74024], 'file_name': 'COCO_train2014_000000282579_0.jpg', 'ann_id': 309640, 'ref_id': 26038, 'image_id': 282579, 'split': 'train', 'sentences': [{'tokens': ['the', 'sandwich', 'to', 'the', 'right', 'of', 'the', 'fries', 'with', 'the', 'olives'], 'raw': 'the sandwich to the right of the fries with the olives', 'sent_id': 74022, 'sent': 'the sandwich to the right of the fries with the olives', 'spcy_WORD': ['the', 'sandwich', 'to', 'the', 'right', 'of', 'the', 'fries', 'with', 'the', 'olives'], 'spcy_DEP': ['det', 'ROOT', 'prep', 'det', 'pobj', 'prep', 'det', 'pobj', 'prep', 'det', 'pobj'], 'spcy_POS': ['DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN'], 'spcy_LEM': ['the', 'sandwich', 'to', 'the', 'right', 'of', 'the', 'fry', 'with', 'the', 'olive'], 'spcy_TAG': ['DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNS'], 'spcy_IS_STOP': [True, False, True, True, False, True, True, False, True, True, False], 'spcy_ENTS': [], 'spcy_NOUN_CHUNKS': ['the sandwich', 'the right', 'the fries', 'the olives'], 'is_false_premise': False, 'exist': True}, {'tokens': ['right', 'side', 'of', 'sandwich'], 'raw': 'right side of sandwich', 'sent_id': 74023, 'sent': 'right side of sandwich', 'spcy_WORD': ['right', 'side', 'of', 'sandwich'], 'spcy_DEP': ['amod', 'ROOT', 'prep', 'pobj'], 'spcy_POS': ['ADJ', 'NOUN', 'ADP', 'NOUN'], 'spcy_LEM': ['right', 'side', 'of', 'sandwich'], 'spcy_TAG': ['JJ', 'NN', 'IN', 'NN'], 'spcy_IS_STOP': [False, True, True, False], 'spcy_ENTS': [], 'spcy_NOUN_CHUNKS': ['right side', 'sandwich'], 'is_false_premise': False, 'exist': True}, {'tokens': ['sandwich', 'on', 'the', 'right'], 'raw': 'sandwich on the right', 'sent_id': 74024, 'sent': 'sandwich on the right', 'spcy_WORD': ['sandwich', 'on', 'the', 'right'], 'spcy_DEP': ['ROOT', 'prep', 'det', 'pobj'], 'spcy_POS': ['PROPN', 'ADP', 'DET', 'NOUN'], 'spcy_LEM': ['sandwich', 'on', 'the', 'right'], 'spcy_TAG': ['NNP', 'IN', 'DT', 'NN'], 'spcy_IS_STOP': [False, True, True, False], 'spcy_ENTS': ['sandwich'], 'spcy_NOUN_CHUNKS': ['sandwich', 'the right'], 'is_false_premise': False, 'exist': True}, {'tokens': ['the', 'pizza', 'to', 'the', 'right', 'of', 'the', 'fries', 'with', 'the', 'olives'], 'raw': 'the pizza to the right of the fries with the olives', 'sent_id': -1, 'sent': 'the pizza to the right of the fries with the olives', 'spcy_WORD': ['the', 'pizza', 'to', 'the', 'right', 'of', 'the', 'fries', 'with', 'the', 'olives'], 'spcy_DEP': ['det', 'ROOT', 'prep', 'det', 'pobj', 'prep', 'det', 'pobj', 'prep', 'det', 'pobj'], 'spcy_POS': ['DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN'], 'spcy_LEM': ['the', 'pizza', 'to', 'the', 'right', 'of', 'the', 'fry', 'with', 'the', 'olive'], 'spcy_TAG': ['DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NNS', 'IN', 'DT', 'NNS'], 'spcy_IS_STOP': [True, False, True, True, False, True, True, False, True, True, False], 'spcy_ENTS': [], 'spcy_NOUN_CHUNKS': ['the pizza', 'the right', 'the fries', 'the olives'], 'main_subject': ['pizza'], 'ref_id': 26038, 'ann_id': 309640, 'gt_sent_id': 74022, 'gt_sent': 'the sandwich to the right of the fries with the olives', 'is_false_premise': True, 'change_type': 'main_subject', 'exist': False}, {'tokens': ['right', 'side', 'of', 'burger'], 'raw': 'right side of burger', 'sent_id': -1, 'sent': 'right side of burger', 'spcy_WORD': ['right', 'side', 'of', 'burger'], 'spcy_DEP': ['amod', 'ROOT', 'prep', 'pobj'], 'spcy_POS': ['ADJ', 'NOUN', 'ADP', 'NOUN'], 'spcy_LEM': ['right', 'side', 'of', 'burger'], 'spcy_TAG': ['JJ', 'NN', 'IN', 'NN'], 'spcy_IS_STOP': [False, True, True, False], 'spcy_ENTS': [], 'spcy_NOUN_CHUNKS': ['right side', 'burger'], 'main_subject': [], 'ref_id': 26038, 'ann_id': 309640, 'gt_sent_id': 74023, 'gt_sent': 'right side of sandwich', 'is_false_premise': True, 'change_type': 'other_subject', 'exist': False}], 'category_id': 54}\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "print(refcoco_new_scrubbed.refs[1000])\n",
    "print(\"\")\n",
    "print(refcoco_new_scrubbed.refs_data[1000])\n",
    "print(len(refcoco_new_scrubbed.refs_data), len(refcoco_new_scrubbed.refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving instances.json:  /home/gbiamby/proj/geo-llm-ret/output/refcoco_google-gb006_remove_guidelines-gpt-3.5-turbo/refer_seg/fprefcoco_v002/instances.json\n",
      "Saving refs:  /home/gbiamby/proj/geo-llm-ret/output/refcoco_google-gb006_remove_guidelines-gpt-3.5-turbo/refer_seg/fprefcoco_v002/refs(berkeley).p\n",
      "Saved new refer_seg dataset to:  /home/gbiamby/proj/geo-llm-ret/output/refcoco_google-gb006_remove_guidelines-gpt-3.5-turbo/refer_seg/fprefcoco_v002\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "def save_refs(refcoco: COCO, save_dir: Path, split_by: str):\n",
    "    assert save_dir.exists(), str(save_dir)\n",
    "    refs_path = save_dir / f\"refs({split_by}).p\"\n",
    "    print(\"Saving refs: \", refs_path)\n",
    "    pickle.dump(refcoco.refs_data, open(refs_path, \"wb\"))\n",
    "\n",
    "\n",
    "def make_new_dataset(refcoco: COCO, save_dir: Path, dataset_name: str, split_by: str):\n",
    "    new_dataset_path = save_dir / \"refer_seg\" / f\"{dataset_name}\"\n",
    "    new_dataset_path.mkdir(exist_ok=True, parents=True)\n",
    "    # Copy coco instances.json:\n",
    "    source_path = refcoco.DATA_DIR / \"instances.json\"\n",
    "    assert source_path.exists(), str(source_path)\n",
    "    print(\"saving instances.json: \", new_dataset_path / \"instances.json\")\n",
    "    shutil.copy(source_path, new_dataset_path / \"instances.json\")\n",
    "    save_refs(refcoco, new_dataset_path, split_by)\n",
    "    print(\"Saved new refer_seg dataset to: \", new_dataset_path)\n",
    "    return new_dataset_path\n",
    "\n",
    "\n",
    "new_ds_path = make_new_dataset(\n",
    "    refcoco_new_scrubbed, api_results_dir, \"fprefcoco_v002\", \"berkeley\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Inspect the Newly Saved RefCOCO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "============================================================================================================================================================================================================================\n",
      "Dataset: fprefcoco_v002(berkeley)\n",
      "Loading refs from '/home/gbiamby/proj/geo-llm-ret/output/refcoco_google-gb006_remove_guidelines-gpt-3.5-turbo/refer_seg/fprefcoco_v002/refs(berkeley).p'\n",
      "Loaded 50000 refs\n",
      "loading annotations into memory...\n",
      "Done (t=9.04s)\n",
      "creating index...\n",
      "index created!\n",
      "num images: 19994\n",
      "num annotations: 196771\n",
      "pos/neg sentence_counts:  142210 134720\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_sent_count</th>\n",
       "      <th>neg_sent_count</th>\n",
       "      <th>dataset</th>\n",
       "      <th>num_refs</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>total_pos_sents</th>\n",
       "      <th>total_neg_sents</th>\n",
       "      <th>ann_count</th>\n",
       "      <th>img_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>119</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>589</td>\n",
       "      <td>1767</td>\n",
       "      <td>1178</td>\n",
       "      <td>589</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>7451</td>\n",
       "      <td>29804</td>\n",
       "      <td>14902</td>\n",
       "      <td>14902</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>515</td>\n",
       "      <td>1545</td>\n",
       "      <td>1545</td>\n",
       "      <td>0</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>781</td>\n",
       "      <td>3124</td>\n",
       "      <td>2343</td>\n",
       "      <td>781</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>3442</td>\n",
       "      <td>17210</td>\n",
       "      <td>10326</td>\n",
       "      <td>6884</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>36735</td>\n",
       "      <td>220410</td>\n",
       "      <td>110205</td>\n",
       "      <td>110205</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>11</td>\n",
       "      <td>66</td>\n",
       "      <td>44</td>\n",
       "      <td>22</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>39</td>\n",
       "      <td>273</td>\n",
       "      <td>156</td>\n",
       "      <td>117</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>297</td>\n",
       "      <td>2376</td>\n",
       "      <td>1188</td>\n",
       "      <td>1188</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pos_sent_count  neg_sent_count                   dataset  num_refs  \\\n",
       "0                1               1  fprefcoco_v002(berkeley)         2   \n",
       "1                2               0  fprefcoco_v002(berkeley)       119   \n",
       "2                2               1  fprefcoco_v002(berkeley)       589   \n",
       "3                2               2  fprefcoco_v002(berkeley)      7451   \n",
       "4                3               0  fprefcoco_v002(berkeley)       515   \n",
       "5                3               1  fprefcoco_v002(berkeley)       781   \n",
       "6                3               2  fprefcoco_v002(berkeley)      3442   \n",
       "7                3               3  fprefcoco_v002(berkeley)     36735   \n",
       "8                4               0  fprefcoco_v002(berkeley)         8   \n",
       "9                4               1  fprefcoco_v002(berkeley)         5   \n",
       "10               4               2  fprefcoco_v002(berkeley)        11   \n",
       "11               4               3  fprefcoco_v002(berkeley)        39   \n",
       "12               4               4  fprefcoco_v002(berkeley)       297   \n",
       "13               5               4  fprefcoco_v002(berkeley)         2   \n",
       "14               5               5  fprefcoco_v002(berkeley)         3   \n",
       "15               6               2  fprefcoco_v002(berkeley)         1   \n",
       "\n",
       "    sent_count  total_pos_sents  total_neg_sents  ann_count  img_count  \n",
       "0            4                2                2     196771      19994  \n",
       "1          238              238                0     196771      19994  \n",
       "2         1767             1178              589     196771      19994  \n",
       "3        29804            14902            14902     196771      19994  \n",
       "4         1545             1545                0     196771      19994  \n",
       "5         3124             2343              781     196771      19994  \n",
       "6        17210            10326             6884     196771      19994  \n",
       "7       220410           110205           110205     196771      19994  \n",
       "8           32               32                0     196771      19994  \n",
       "9           25               20                5     196771      19994  \n",
       "10          66               44               22     196771      19994  \n",
       "11         273              156              117     196771      19994  \n",
       "12        2376             1188             1188     196771      19994  \n",
       "13          18               10                8     196771      19994  \n",
       "14          30               15               15     196771      19994  \n",
       "15           8                6                2     196771      19994  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VALID_SPLITS = {\n",
    "    \"R-refcoco\": [\"unc\"],\n",
    "    \"R-refcoco+\": [\"unc\"],\n",
    "    \"R-refcocog\": [\"umd\"],\n",
    "    \"refclef\": [\"berkeley\", \"unc\"],\n",
    "    \"refcoco\": [\"google\"],\n",
    "    \"refcoco+\": [\"unc\"],\n",
    "    \"refcocog\": [\"google\", \"umd\"],\n",
    "    \"fprefcoco_v002\": [\"berkeley\"],\n",
    "    \"fprefcocog_v002\": [\"berkeley\"],\n",
    "}\n",
    "\n",
    "\n",
    "def build_refcoco(refseg_path: Path, dataset_name: str, split_by: str = None) -> COCO:\n",
    "    assert dataset_name in VALID_SPLITS, dataset_name\n",
    "    if split_by is None:\n",
    "        split_by = VALID_SPLITS[dataset_name][0]\n",
    "    else:\n",
    "        assert split_by in VALID_SPLITS[dataset_name]\n",
    "    coco = COCO(\n",
    "        refseg_path / dataset_name / \"instances.json\",\n",
    "        is_ref_dataset=True,\n",
    "        dataset_name=dataset_name,\n",
    "        split_by=split_by,\n",
    "    )\n",
    "    return coco\n",
    "\n",
    "\n",
    "df_aggs = []\n",
    "for ds_name in [\"fprefcoco_v002\"]:\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"=\" * 220)\n",
    "    print(f\"Dataset: {ds_name}(berkeley)\")\n",
    "    coconegref_stats = CocoClassDistHelper(\n",
    "        new_ds_path.parent,\n",
    "        is_ref_dataset=True,\n",
    "        dataset_name=ds_name,\n",
    "        split_by=\"berkeley\",\n",
    "    )\n",
    "    df_refcoco, df_refcoco_agg = coconegref_stats.get_ref_stats()\n",
    "    df_aggs.append(df_refcoco_agg)\n",
    "\n",
    "\n",
    "df_aggs = pd.concat(df_aggs)\n",
    "\n",
    "\n",
    "display(df_aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_refs</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>total_pos_sents</th>\n",
       "      <th>total_neg_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>50000</td>\n",
       "      <td>276930</td>\n",
       "      <td>142210</td>\n",
       "      <td>134720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      total_refs  sent_count  total_pos_sents  total_neg_sents\n",
       "True       50000      276930           142210           134720"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    df_aggs.groupby(lambda x: True).agg(\n",
    "        total_refs=(\"num_refs\", \"sum\"),\n",
    "        sent_count=(\"sent_count\", \"sum\"),\n",
    "        total_pos_sents=(\"total_pos_sents\", \"sum\"),\n",
    "        total_neg_sents=(\"total_neg_sents\", \"sum\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref has keys:  dict_keys(['sent_ids', 'file_name', 'ann_id', 'ref_id', 'image_id', 'split', 'sentences', 'category_id'])\n",
      "ref has 6 sentences\n",
      "sent_id:2836, is_FP:False, sent: 'chef in back'\n",
      "sent_id:2837, is_FP:False, sent: 'right guy'\n",
      "sent_id:2838, is_FP:False, sent: 'cook in the back'\n",
      "sent_id:-1, is_FP:True, sent: 'plumber in back'\n",
      "\tchange_type:  main_subject\n",
      "\tparent_sent_id: 2836, parent_sent: 'chef in back'\n",
      "sent_id:-1, is_FP:True, sent: 'right alien'\n",
      "\tchange_type:  main_subject\n",
      "\tparent_sent_id: 2837, parent_sent: 'right guy'\n",
      "sent_id:-1, is_FP:True, sent: 'engineer in the back'\n",
      "\tchange_type:  main_subject\n",
      "\tparent_sent_id: 2838, parent_sent: 'cook in the back'\n"
     ]
    }
   ],
   "source": [
    "def show_a_refexp(refcoco: COCO):\n",
    "    ref = refcoco.refs[1000]\n",
    "    print(\"ref has keys: \", ref.keys())\n",
    "    print(f\"ref has {len(ref['sentences'])} sentences\")\n",
    "    for s in ref[\"sentences\"]:\n",
    "        # print(s.keys())\n",
    "        print(\n",
    "            f\"sent_id:{s['sent_id']}, is_FP:{s['is_false_premise']}, sent: '{s['sent']}'\"\n",
    "        )\n",
    "        if s[\"is_false_premise\"]:\n",
    "            print(\"\\tchange_type: \", s[\"change_type\"])\n",
    "            print(f\"\\tparent_sent_id: {s['gt_sent_id']}, parent_sent: '{s['gt_sent']}'\")\n",
    "\n",
    "\n",
    "show_a_refexp(coconegref_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cocobetter",
   "language": "python",
   "name": "cocobetter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
