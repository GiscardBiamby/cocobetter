{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fprefcocov002.ipynb Create a false premise referring expressions dataset\n",
    "\n",
    "Create a COCO formatted dataset that uses `gpt-3.5-turbo` to create false premise referring expressions that refer to objects that do not exist in the image. \n",
    "\n",
    "## Types of Modifications\n",
    "\n",
    "We ask GPT to modify the ground truth referring expressions for each image.\n",
    "We categorize each FP according to the type of modification. We have three categories:\n",
    "\n",
    "- Modify the main subject of the sentence. This means changing from one noun or noun phrase to another one. \"A woman...\" -> \"A cat...\"\n",
    "- Modify an attribute of the main subject. \"A tall man...\" -> \"A short man\"\n",
    "- Modify some other portion of the description. This usually means either modifying a spatial relation, or a participatory object that the expression relates somehow to the main subject, or sometimes an attribute of the participatory object.\n",
    "\n",
    "## File Format\n",
    "The referring expressions follow same format as refcoco/refcocog/refcoco+/R-refcoco/etc, i.e., a COCO formatted json file, accompanied by a file with a `.p` extension, which contains the true and false referring expressions. The `.p` file is a python pickle file. These datasets can be loaded using the common `refer.py`, or the `COCO` class in `github.com/GiscardBiamby/cocobetter.git`. Examples can be found later in this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastjsonschema                    2.18.0\n",
      "json5                             0.9.14\n",
      "jsonpointer                       2.4\n",
      "jsons                             1.6.3\n",
      "jsonschema                        4.19.1\n",
      "jsonschema-specifications         2023.7.1\n",
      "pysimdjson                        5.0.2\n",
      "python-json-logger                2.0.7\n",
      "python-lsp-jsonrpc                1.1.1\n",
      "ujson                             5.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list | grep json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbiamby/mambaforge/envs/cocobetter/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import copy\n",
    "import csv\n",
    "import decimal\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import typing\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Set, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as pil_img\n",
    "import regex as re\n",
    "import seaborn as sns\n",
    "import simdjson as json\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO, Ann, Cat, Image, Ref\n",
    "from pycocotools.helpers import CocoClassDistHelper, CocoJsonBuilder\n",
    "from pycocotools.helpers.coco_builder import COCOShrinker\n",
    "from simplediff import diff, string_diff\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# from geo_llm_ret.ref_datasets import build_ref_coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_DIR = Path(\"/shared/gbiamby/data/coco\")\n",
    "IMG_DIR = COCO_DIR / \"val2017\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Generate Enhanced Version of the refcoco file\n",
    "\n",
    "Enhanced version augments the `ref[\"sentences\"]` dictionaries with spacy tagging info (parts of speech, dependency parsing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading refs from '/home/gbiamby/proj/geo-llm-ret/lib/cocobetter/PythonAPI/notebooks/ref_correct/output/ref_seg/refcoco+/refs(unc).p'\n",
      "Loaded 49856 refs\n",
      "loading annotations into memory...\n",
      "Done (t=1.08s)\n",
      "creating index...\n",
      "index created!\n",
      "Computing spacy docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ComputeSpacy: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 141564/141564 [02:03<00:00, 1144.95it/s]\n",
      "UpdateSents: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 141564/141564 [00:06<00:00, 20265.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to  /home/gbiamby/proj/geo-llm-ret/lib/cocobetter/PythonAPI/notebooks/ref_correct/output/ref_seg/refcoco+/refs(unc_enhanced).p\n"
     ]
    }
   ],
   "source": [
    "def get_gt_sentences_flat(refcoco: COCO) -> list[dict]:\n",
    "    # Make a flat list of FP sentences so we can batch process with spacy:\n",
    "    sents_all = []\n",
    "    for ref_id, ref in refcoco.refs.items():\n",
    "        sents: list[str] = ref[\"sentences\"]\n",
    "        sents_all.extend(\n",
    "            [\n",
    "                {\n",
    "                    \"sent_dict\": s,\n",
    "                    \"sent_id\": s[\"sent_id\"],\n",
    "                    \"image_id\": ref[\"image_id\"],\n",
    "                    \"ref_id\": ref_id,\n",
    "                }\n",
    "                for s in sents\n",
    "            ]\n",
    "        )\n",
    "    return sents_all\n",
    "\n",
    "\n",
    "def add_spacy_tags(refcoco: COCO):\n",
    "    \"\"\"\n",
    "    Creates a deep copy of refcoco's `.refs_data` property, and enhances the\n",
    "    copy by adding SpaCy NLP parsing tags for POS, TAG, DEP, etc. See here\n",
    "    for more info: https://spacy.io/usage/linguistic-features\n",
    "    \"\"\"\n",
    "    sentences: list[dict] = get_gt_sentences_flat(refcoco)\n",
    "    sent_list = [s[\"sent_dict\"][\"sent\"] for s in sentences]\n",
    "\n",
    "    print(\"Computing spacy docs\")\n",
    "    B = 1000\n",
    "    docs: list[spacy.tokens.Doc] = [\n",
    "        d\n",
    "        for d in tqdm(\n",
    "            nlp.pipe(sent_list, batch_size=B), total=len(sent_list), desc=\"ComputeSpacy\"\n",
    "        )\n",
    "    ]\n",
    "    assert len(sent_list) == len(docs)\n",
    "\n",
    "    # Update sentences:\n",
    "    for sent, doc in tqdm(zip(sentences, docs), total=len(docs), desc=\"UpdateSents\"):\n",
    "        sent[\"sent_dict\"].update(\n",
    "            {\n",
    "                \"spcy_WORD\": [str(word) for word in doc],\n",
    "                \"spcy_DEP\": [word.dep_ for word in doc],\n",
    "                \"spcy_POS\": [word.pos_ for word in doc],\n",
    "                \"spcy_LEM\": [word.lemma_ for word in doc],\n",
    "                \"spcy_TAG\": [word.tag_ for word in doc],\n",
    "                \"spcy_IS_STOP\": [word.is_stop for word in doc],\n",
    "                \"spcy_ENTS\": [str(ent).strip() for ent in doc.ents],\n",
    "                \"spcy_NOUN_CHUNKS\": [str(nc).strip() for nc in doc.noun_chunks],\n",
    "                # \"spcy_DOC\": doc, # this takes up way too much space 2.7G vs 115MB\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return refcoco\n",
    "\n",
    "\n",
    "def make_enhanced_refcoco(dataset_name: str, split_by: str, refseg_dir: Path):\n",
    "    refcoco = build_refcoco(refseg_dir, dataset_name, split_by)\n",
    "    refcoco = add_spacy_tags(refcoco)\n",
    "\n",
    "    output_path = Path(\n",
    "        f\"output/ref_seg/{refcoco.dataset_name}/refs({refcoco.split_by.replace('_enhanced', '')}_enhanced).p\"\n",
    "    ).resolve()\n",
    "    print(\"saving to \", output_path)\n",
    "    output_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    pickle.dump(refcoco.refs_data, open(output_path, \"wb\"))\n",
    "    # print(\"Done\")\n",
    "    return refcoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading refs from '/home/gbiamby/proj/geo-llm-ret/lib/cocobetter/PythonAPI/notebooks/ref_correct/output/ref_seg/refcoco+/refs(unc_enhanced).p'\n",
      "Loaded 49856 refs\n",
      "loading annotations into memory...\n",
      "Done (t=3.07s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "VALID_SPLITS = {\n",
    "    \"R-refcoco\": [\"unc\"],\n",
    "    \"R-refcoco+\": [\"unc\"],\n",
    "    \"R-refcocog\": [\"umd\"],\n",
    "    \"refclef\": [\"berkeley\", \"unc\"],\n",
    "    \"refcoco\": [\"google\"],\n",
    "    \"refcoco+\": [\"unc\"],\n",
    "    \"refcocog\": [\"google\", \"umd\"],\n",
    "}\n",
    "\n",
    "\n",
    "def build_refcoco(refseg_path: Path, dataset_name: str, split_by: str = None) -> COCO:\n",
    "    assert dataset_name in VALID_SPLITS, dataset_name\n",
    "    if split_by is None:\n",
    "        split_by = VALID_SPLITS[dataset_name][0]\n",
    "    else:\n",
    "        assert split_by.replace(\"_enhanced\", \"\") in VALID_SPLITS[dataset_name]\n",
    "    coco = COCO(\n",
    "        refseg_path / dataset_name / \"instances.json\",\n",
    "        is_ref_dataset=True,\n",
    "        dataset_name=dataset_name,\n",
    "        split_by=split_by,\n",
    "    )\n",
    "    return coco\n",
    "\n",
    "\n",
    "IMG_DIR = Path(\"/shared/gbiamby/data/coco/train2014\")\n",
    "PROJ_ROOT = Path(\"../../../../../\").resolve()\n",
    "assert PROJ_ROOT.exists()\n",
    "# REFSEG_DIR = Path(\"/shared/gbiamby/data/refer_seg\")\n",
    "REFSEG_DIR = Path(\"output/ref_seg\")\n",
    "\n",
    "# Make \"_enhanced\" versions:\n",
    "# refcoco = make_enhanced_refcoco(\"refcocog\", \"google\", REFSEG_DIR)\n",
    "# refcoco = make_enhanced_refcoco(\"refcoco\", \"google\", REFSEG_DIR)\n",
    "# refcoco = make_enhanced_refcoco(\"refcoco+\", \"unc\", REFSEG_DIR)\n",
    "\n",
    "# refcoco = build_refcoco(REFSEG_DIR, \"refcocog\", \"google_enhanced\")\n",
    "# refcoco = build_refcoco(REFSEG_DIR, \"refcoco\", \"google_enhanced\")\n",
    "refcoco = build_refcoco(REFSEG_DIR, \"refcoco+\", \"unc_enhanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_results_dir = (\n",
    "    PROJ_ROOT / \"output/refcoco+_unc-gb006_remove_guidelines-gpt-3.5-turbo\"\n",
    ")\n",
    "assert api_results_dir.exists(), str(api_results_dir)\n",
    "assert api_results_dir.is_dir(), str(api_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19992/19992 [00:00<00:00, 25905.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19992 responses from /home/gbiamby/proj/geo-llm-ret/output/refcoco+_unc-gb006_remove_guidelines-gpt-3.5-turbo\n",
      "Example response: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'api_response': {'choices': [{'finish_reason': 'stop',\n",
       "    'index': 0,\n",
       "    'message': {'content': 'Altered Descriptions: (6 sentences): [\"the alien behind the guy catching\", \"man behind\", \"guy takinga header behind purple shirt dude\", \"purple shirt\", \"man in purple\", \"alien with hat near us\"]',\n",
       "     'role': 'assistant'}}],\n",
       "  'created': 1699387946,\n",
       "  'id': 'chatcmpl-8IMiwma2vLAMcVkVL134yMY5YV5K6',\n",
       "  'model': 'gpt-3.5-turbo-0613',\n",
       "  'object': 'chat.completion',\n",
       "  'usage': {'completion_tokens': 48,\n",
       "   'prompt_tokens': 509,\n",
       "   'total_tokens': 557}},\n",
       " 'image_id': 100012,\n",
       " 'request_info': {'ann_ids': [521105, 501646],\n",
       "  'image_id': 100012,\n",
       "  'ref_ids': [41151, 41152],\n",
       "  'sent_ids': [[116933, 116934, 116935], [116936, 116937, 116938]],\n",
       "  'sentences': ['the guy behind the guy catching',\n",
       "   'man behind',\n",
       "   'guy takinga header behind white shirt dude',\n",
       "   'white shirt',\n",
       "   'man in white',\n",
       "   'guy with hat near us']}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_api_responses(api_results_dir: Path, max_results: int = None) -> list[dict]:\n",
    "    response_files = sorted(api_results_dir.glob(\"responses/img_id_*.json\"))\n",
    "    if max_results is not None and max_results > 0:\n",
    "        response_files = response_files[:max_results]\n",
    "    results = []\n",
    "    for f in tqdm(response_files):\n",
    "        with open(f, \"r\", encoding=\"utf-8\") as json_file:\n",
    "            result = json.load(json_file)\n",
    "            results.append(result)\n",
    "\n",
    "    print(f\"Loaded {len(results)} responses from {api_results_dir}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "api_responses = load_api_responses(api_results_dir)\n",
    "print(\"Example response: \")\n",
    "display(api_responses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check Quality of the API Results - Filter Bad Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19992/19992 [00:00<00:00, 29701.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19992 responses from /home/gbiamby/proj/geo-llm-ret/output/refcoco+_unc-gb006_remove_guidelines-gpt-3.5-turbo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19992/19992 [00:00<00:00, 32716.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_results:  19992\n",
      "has_fpsents_count:  19877\n",
      "Found 5980 warnings\n",
      "Found 25 errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_result(image_id: int, reply: str, warnings, errors) -> list[str]:\n",
    "    def parse_result_main(reply: str):\n",
    "        # print(f\"ChatGPT Reply: \\n\\t{reply}\")\n",
    "        matches = re.match(\n",
    "            # '.*Descriptions:*\\\\s*(\\\\(.{1,} sentence[s]{0,1}\\\\):){0,1}\\\\s*(?P<descriptions>\\\\[\\\\\".*\\\\\"\\\\])',\n",
    "            '.*Description[s]{0,1}:*\\\\s*(\\\\(.{1,} sentence[s]{0,1}\\\\):){0,1}\\\\s*\\\\[{0,1}(?P<descriptions>\\\\\".*\\\\\")\\\\]{0,1}',\n",
    "            reply,\n",
    "            re.MULTILINE | re.DOTALL,\n",
    "        )\n",
    "        if matches is None:\n",
    "            return None\n",
    "        list_str = matches.group(\"descriptions\")\n",
    "        if not list_str.startswith(\"[\"):\n",
    "            list_str = \"[\" + list_str\n",
    "        if not list_str.endswith(\"]\"):\n",
    "            list_str = list_str + \"]\"\n",
    "        new_sents = ast.literal_eval(list_str)\n",
    "        # print(\"New Sents: \", new_sent)\n",
    "        if new_sents is None:\n",
    "            errors.append(\n",
    "                {\n",
    "                    \"image_id\": image_id,\n",
    "                    \"msg\": f\"No FP sents found (fp_sents is None)\",\n",
    "                    \"raw_reply\": reply,\n",
    "                }\n",
    "            )\n",
    "            return None\n",
    "        return new_sents\n",
    "\n",
    "    def parse_result_multiline_list(reply: str):\n",
    "        matches = re.match(\n",
    "            '(?:.*Description[s]{0,1}:[ ]*(\\\\(.+ sentence[s]{0,1}\\\\):){0,1})\\\\n*?(?P<descriptions>\\\\n\\\\d\\\\.[ ]*\\\\\"[^\\\\n\\\\\"]+\\\\\")+',\n",
    "            reply,\n",
    "            re.MULTILINE | re.DOTALL,\n",
    "        )\n",
    "        if matches is None:\n",
    "            return None\n",
    "        captures = matches.capturesdict()\n",
    "        if (captures is None or len(captures) == 0) or (\n",
    "            captures is not None and \"descriptions\" not in captures\n",
    "        ):\n",
    "            return None\n",
    "        new_sents = []\n",
    "        for cap in captures[\"descriptions\"]:\n",
    "            matches = re.match('(?:[\\\\d]\\\\.)\\\\s*?\\\\\"(?P<sent>[^\\\\\"]+)\\\\\"', cap.strip())\n",
    "            # print(\"match: \", matches)\n",
    "            # print(\"sent: \", matches.groupdict()[\"sent\"])\n",
    "            new_sents.append(matches.groupdict()[\"sent\"])\n",
    "        return new_sents\n",
    "\n",
    "    try:\n",
    "        reply = reply.replace('\\\\\"', '\"')\n",
    "        new_sents = parse_result_main(reply)\n",
    "        if new_sents is None:\n",
    "            new_sents = parse_result_multiline_list(reply)\n",
    "            # if new_sent is None:\n",
    "            #     print(reply)\n",
    "        return new_sents\n",
    "    except Exception as ex:\n",
    "        errors.append(\n",
    "            {\n",
    "                \"image_id\": image_id,\n",
    "                \"msg\": str(ex) + \" ex type: \" + str(type(ex)),\n",
    "                \"raw_reply\": reply,\n",
    "            }\n",
    "        )\n",
    "        return None\n",
    "\n",
    "\n",
    "def verify_results(result: list[dict], refcoco: COCO):\n",
    "    image_id = result[\"image_id\"]\n",
    "    warnings, errors = [], []\n",
    "    raw_reply: list[str] = result[\"api_response\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "    fp_sents = parse_result(image_id, raw_reply, warnings, errors)\n",
    "    request_info = result[\"request_info\"]\n",
    "    gt_sents = request_info[\"sentences\"]\n",
    "\n",
    "    if fp_sents is None:\n",
    "        return warnings, errors\n",
    "    assert isinstance(fp_sents, list)\n",
    "\n",
    "    # Ensure correct number of FP sentences were generated:\n",
    "    if len(fp_sents) != len(gt_sents):\n",
    "        errors.append(\n",
    "            {\n",
    "                \"image_id\": image_id,\n",
    "                \"msg\": \"Wrong number of FP sentences\",\n",
    "                \"msg_detail\": f\"len(fp_sents):{len(fp_sents)}!=len(gt_sents):{len(gt_sents)}\",\n",
    "                \"fp_sents\": fp_sents,\n",
    "                \"gt_sents\": gt_sents,\n",
    "                \"reply\": result,\n",
    "                \"raw_reply\": raw_reply,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    for sent, fp_sent in zip(gt_sents, fp_sents):\n",
    "        # Warn if FP sentence is same as original sentence:\n",
    "        if sent.lower() == fp_sent.lower():\n",
    "            warnings.append(\n",
    "                {\n",
    "                    \"image_id\": image_id,\n",
    "                    \"msg\": \"FP is exact match for GT sentence\",\n",
    "                    \"msg_detail\": f\"{sent}=={fp_sent}\",\n",
    "                    \"fp_sents\": fp_sent,\n",
    "                    \"gt_sent\": sent,\n",
    "                    \"reply\": result,\n",
    "                    \"raw_reply\": raw_reply,\n",
    "                }\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # diff_result = string_diff(sent.lower(), fp_sent.lower())\n",
    "        # num_matching_spans = len([res[0] for res in diff_result if res[0] == \"=\"])\n",
    "\n",
    "        # if num_matching_spans not in {1, 2}:\n",
    "        #     warnings.append(\n",
    "        #         {\n",
    "        #             \"image_id\": image_id,\n",
    "        #             \"msg\": \"Wrong num_matching_spans\",\n",
    "        #             \"msg_detail\": f\":{num_matching_spans}, diff:{diff_result}\",\n",
    "        #             \"fp_sents\": fp_sent,\n",
    "        #             \"gt_sent\": sent,\n",
    "        #             \"reply\": result,\n",
    "        #             \"raw_reply\": raw_reply,\n",
    "        #         }\n",
    "        #     )\n",
    "        # print(\"\")\n",
    "    result[\"fp_sents\"] = fp_sents\n",
    "    # print(\"warnings: \", len(warnings), \"errors: \", len(errors))\n",
    "    return warnings, errors\n",
    "\n",
    "\n",
    "def check_fpsent_counts(results: list[dict]):\n",
    "    has_fpsents_count = 0\n",
    "    num_results = 0\n",
    "    for result in results:\n",
    "        num_results += 1\n",
    "        if \"fp_sents\" in result and len(result[\"fp_sents\"]) > 0:\n",
    "            has_fpsents_count += 1\n",
    "\n",
    "    print(\"num_results: \", num_results)\n",
    "    print(\"has_fpsents_count: \", has_fpsents_count)\n",
    "\n",
    "\n",
    "# Check all results:\n",
    "api_responses = load_api_responses(api_results_dir)[:200000]\n",
    "warnings = []\n",
    "errors = []\n",
    "for result in tqdm(api_responses):\n",
    "    _warnings, _errors = verify_results(result, refcoco)\n",
    "    warnings.extend(_warnings)\n",
    "    errors.extend(_errors)\n",
    "\n",
    "check_fpsent_counts(api_responses)\n",
    "\n",
    "# Summarize Results:\n",
    "print(f\"Found {len(warnings)} warnings\")\n",
    "print(f\"Found {len(errors)} errors\")\n",
    "# print(\"\")\n",
    "# print(\"=\" * 220)\n",
    "# print(\"Warnings:\")\n",
    "# for warn in warnings[:3]:\n",
    "#     print(\"\")\n",
    "#     print(\"=\" * 100)\n",
    "#     print(json.dumps(warn, indent=4))\n",
    "# print(\"\")\n",
    "# print(\"=\" * 220)\n",
    "# print(\"Errors:\")\n",
    "# for err in errors[:3]:\n",
    "#     print(\"\")\n",
    "#     print(\"=\" * 100)\n",
    "#     print(json.dumps(err, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Intermediate Results: API Responses With Parsed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parsed_results_path = api_results_dir / \"parsed_results_001.pkl\"\n",
    "pickle.dump(api_responses, open(parsed_results_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Error Counts Grouped By Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>uniqe_imgs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>msg</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Wrong number of FP sentences</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invalid syntax (&lt;unknown&gt;, line 3) ex type: &lt;class 'SyntaxError'&gt;</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    total  uniqe_imgs\n",
       "msg                                                                  \n",
       "Wrong number of FP sentences                           23          23\n",
       "invalid syntax (<unknown>, line 3) ex type: <cl...      2           2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_errors = pd.DataFrame(errors)\n",
    "# display(df_errors)\n",
    "df_err_counts = (\n",
    "    df_errors.groupby([\"msg\"])\n",
    "    .agg(\n",
    "        total=(\"image_id\", \"count\"),\n",
    "        uniqe_imgs=(\"image_id\", \"nunique\"),\n",
    "    )\n",
    "    .sort_values(\"total\", ascending=False)\n",
    ")\n",
    "display(df_err_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This amount of errors seems acceptable. The top two types of error are:\n",
    "\n",
    "_refcocog_\n",
    "\n",
    "- (59 errors) Invalid pythong list syntaks for the sentences, e.g., unmatched string quotes, missing commas\n",
    "- (55 errors) Wrong number of false premise sentences returned by chat-gpt. If we wanted to, we could use whatever sentences gpt was able to provide.\n",
    "\n",
    "\n",
    "_refcoco_\n",
    "\n",
    "- (29 errors) Wrong number of false premise sentences returned by chat-gpt. If we wanted to, we could use whatever sentences gpt was able to provide.Wrong number of false premise sentences returned by chat-gpt. If we wanted to, we could use whatever sentences gpt was able to provide.\n",
    "- (1 errors) `EOL while scanning string literal (<unknown>, line 1) ex type: <class 'SyntaxError'>\tEOL while scanning string literal (<unknown>, line 1) ex type: <class 'SyntaxError'>\t`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>msg</th>\n",
       "      <th>msg_detail</th>\n",
       "      <th>fp_sents</th>\n",
       "      <th>gt_sents</th>\n",
       "      <th>reply</th>\n",
       "      <th>raw_reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128029</td>\n",
       "      <td>Wrong number of FP sentences</td>\n",
       "      <td>len(fp_sents):1!=len(gt_sents):6</td>\n",
       "      <td>[orange plant]</td>\n",
       "      <td>[single rose, flowers behind cat, the rose, flowers before cat, vase with greenery, black plant]</td>\n",
       "      <td>{'api_response': {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': '## Altered Descriptions: \\n\\n    1. Altered Description: \"single tulip\"\\n    2. Altered Description: \"plants behind cat\"\\n    3. Altered Description: \"the daisy\"\\n    4. Altered Description: \"plants before cat\"\\n    5. Altered Description: \"vase with succulents\"\\n    6. Altered Description: \"orange plant\"', 'role': 'assistant'}}], 'created': 1699387487, 'id': 'chatcmpl-8IMbXxf3Xyb86GSF15XFVwVAatInx', 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 87, 'prompt_tokens': 498, 'total_tokens': 585}}, 'image_id': 128029, 'request_info': {'ann_ids': [291891, 19399], 'image_id': 128029, 'ref_ids': [38836, 38837], 'sent_ids': [[110341, 110342, 110343], [110344, 110345, 110346]], 'sentences': ['single rose', 'flowers behind cat', 'the rose', 'flowers before cat', 'vase with greenery', 'black plant']}, 'fp_sents': ['orange plant']}</td>\n",
       "      <td>## Altered Descriptions: \\n\\n    1. Altered Description: \"single tulip\"\\n    2. Altered Description: \"plants behind cat\"\\n    3. Altered Description: \"the daisy\"\\n    4. Altered Description: \"plants before cat\"\\n    5. Altered Description: \"vase with succulents\"\\n    6. Altered Description: \"orange plant\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>167765</td>\n",
       "      <td>Wrong number of FP sentences</td>\n",
       "      <td>len(fp_sents):9!=len(gt_sents):12</td>\n",
       "      <td>[purple pup, eyes open, purple, shorts cuff, purple short leg that the kitten is sniffing, shorts below kitty nose, foot, foot, foot]</td>\n",
       "      <td>[tan pup, eyes shut, tan, jeans cuff, blue jean leg that the puppy is sniffing, jeans below doggie nose, hand, hand, hand, white dog head, white puppy face, part hidden dog]</td>\n",
       "      <td>{'api_response': {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': 'Altered Descriptions: \\n1. \"purple pup\"\\n2. \"eyes open\"\\n3. \"purple\"\\n4. \"shorts cuff\"\\n5. \"purple short leg that the kitten is sniffing\"\\n6. \"shorts below kitty nose\"\\n7. \"foot\"\\n8. \"foot\"\\n9. \"foot\"\\n10. \"black cat head\"\\n11. \"black kitten face\"\\n12. \"part hidden cat\"', 'role': 'assistant'}}], 'created': 1699386161, 'id': 'chatcmpl-8IMG9iH9sdWLdDRdrgSgH4VnHbrwd', 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 88, 'prompt_tokens': 528, 'total_tokens': 616}}, 'image_id': 167765, 'request_info': {'ann_ids': [1817357, 254764, 183813, 15580], 'image_id': 167765, 'ref_ids': [35549, 35550, 35551, 35552], 'sent_ids': [[101055, 101056, 101057], [101058, 101059, 101060], [101061, 101062, 101063], [101064, 101065, 101066]], 'sentences': ['tan pup', 'eyes shut', 'tan', 'jeans cuff', 'blue jean leg that the puppy is sniffing', 'jeans below doggie nose', 'hand', 'hand', 'hand', 'white dog head', 'white puppy face', 'part hidden dog']}, 'fp_sents': ['purple pup', 'eyes open', 'purple', 'shorts cuff', 'purple short leg that the kitten is sniffing', 'shorts below kitty nose', 'foot', 'foot', 'foot']}</td>\n",
       "      <td>Altered Descriptions: \\n1. \"purple pup\"\\n2. \"eyes open\"\\n3. \"purple\"\\n4. \"shorts cuff\"\\n5. \"purple short leg that the kitten is sniffing\"\\n6. \"shorts below kitty nose\"\\n7. \"foot\"\\n8. \"foot\"\\n9. \"foot\"\\n10. \"black cat head\"\\n11. \"black kitten face\"\\n12. \"part hidden cat\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>157371</td>\n",
       "      <td>Wrong number of FP sentences</td>\n",
       "      <td>len(fp_sents):11!=len(gt_sents):12</td>\n",
       "      <td>[stool that man is sitting on, bench guy is sitting on, throne where woman is on, couch the woman is sitting in, rocking chair lady sitting on, bean bag chair in suit, puppet man, penguin man, unicorn woman in skirt, mermaid woman, queen lady]</td>\n",
       "      <td>[chair that man is sitting on, chair guy is sitting on, guys chair, chair where woman is on, chair the woman is sitting in, chair lady sitting on, man in suit, man, man, woman in skirt, woman, lady]</td>\n",
       "      <td>{'api_response': {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': 'Altered Descriptions: [\"stool that man is sitting on\", \"bench guy is sitting on\", \"throne where woman is on\", \"couch the woman is sitting in\", \"rocking chair lady sitting on\", \"bean bag chair in suit\", \"puppet man\", \"penguin man\", \"unicorn woman in skirt\", \"mermaid woman\", \"queen lady\"]', 'role': 'assistant'}}], 'created': 1699386395, 'id': 'chatcmpl-8IMJvhgbTHt8oD5YICn5VEZD3CVHa', 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 78, 'prompt_tokens': 533, 'total_tokens': 611}}, 'image_id': 157371, 'request_info': {'ann_ids': [374494, 289709, 213140, 204643], 'image_id': 157371, 'ref_ids': [36353, 36354, 36355, 36356], 'sent_ids': [[103326, 103327, 103328], [103329, 103330, 103331], [103332, 103333, 103334], [103335, 103336, 103337]], 'sentences': ['chair that man is sitting on', 'chair guy is sitting on', 'guys chair', 'chair where woman is on', 'chair the woman is sitting in', 'chair lady sitting on', 'man in suit', 'man', 'man', 'woman in skirt', 'woman', 'lady']}, 'fp_sents': ['stool that man is sitting on', 'bench guy is sitting on', 'throne where woman is on', 'couch the woman is sitting in', 'rocking chair lady sitting on', 'bean bag chair in suit', 'puppet man', 'penguin man', 'unicorn woman in skirt', 'mermaid woman', 'queen lady']}</td>\n",
       "      <td>Altered Descriptions: [\"stool that man is sitting on\", \"bench guy is sitting on\", \"throne where woman is on\", \"couch the woman is sitting in\", \"rocking chair lady sitting on\", \"bean bag chair in suit\", \"puppet man\", \"penguin man\", \"unicorn woman in skirt\", \"mermaid woman\", \"queen lady\"]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                           msg                          msg_detail  \\\n",
       "0    128029  Wrong number of FP sentences    len(fp_sents):1!=len(gt_sents):6   \n",
       "2    167765  Wrong number of FP sentences   len(fp_sents):9!=len(gt_sents):12   \n",
       "1    157371  Wrong number of FP sentences  len(fp_sents):11!=len(gt_sents):12   \n",
       "\n",
       "                                                                                                                                                                                                                                              fp_sents  \\\n",
       "0                                                                                                                                                                                                                                       [orange plant]   \n",
       "2                                                                                                                [purple pup, eyes open, purple, shorts cuff, purple short leg that the kitten is sniffing, shorts below kitty nose, foot, foot, foot]   \n",
       "1  [stool that man is sitting on, bench guy is sitting on, throne where woman is on, couch the woman is sitting in, rocking chair lady sitting on, bean bag chair in suit, puppet man, penguin man, unicorn woman in skirt, mermaid woman, queen lady]   \n",
       "\n",
       "                                                                                                                                                                                                 gt_sents  \\\n",
       "0                                                                                                        [single rose, flowers behind cat, the rose, flowers before cat, vase with greenery, black plant]   \n",
       "2                           [tan pup, eyes shut, tan, jeans cuff, blue jean leg that the puppy is sniffing, jeans below doggie nose, hand, hand, hand, white dog head, white puppy face, part hidden dog]   \n",
       "1  [chair that man is sitting on, chair guy is sitting on, guys chair, chair where woman is on, chair the woman is sitting in, chair lady sitting on, man in suit, man, man, woman in skirt, woman, lady]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                reply  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                             {'api_response': {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': '## Altered Descriptions: \\n\\n    1. Altered Description: \"single tulip\"\\n    2. Altered Description: \"plants behind cat\"\\n    3. Altered Description: \"the daisy\"\\n    4. Altered Description: \"plants before cat\"\\n    5. Altered Description: \"vase with succulents\"\\n    6. Altered Description: \"orange plant\"', 'role': 'assistant'}}], 'created': 1699387487, 'id': 'chatcmpl-8IMbXxf3Xyb86GSF15XFVwVAatInx', 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 87, 'prompt_tokens': 498, 'total_tokens': 585}}, 'image_id': 128029, 'request_info': {'ann_ids': [291891, 19399], 'image_id': 128029, 'ref_ids': [38836, 38837], 'sent_ids': [[110341, 110342, 110343], [110344, 110345, 110346]], 'sentences': ['single rose', 'flowers behind cat', 'the rose', 'flowers before cat', 'vase with greenery', 'black plant']}, 'fp_sents': ['orange plant']}   \n",
       "2                                                                                                                                                              {'api_response': {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': 'Altered Descriptions: \\n1. \"purple pup\"\\n2. \"eyes open\"\\n3. \"purple\"\\n4. \"shorts cuff\"\\n5. \"purple short leg that the kitten is sniffing\"\\n6. \"shorts below kitty nose\"\\n7. \"foot\"\\n8. \"foot\"\\n9. \"foot\"\\n10. \"black cat head\"\\n11. \"black kitten face\"\\n12. \"part hidden cat\"', 'role': 'assistant'}}], 'created': 1699386161, 'id': 'chatcmpl-8IMG9iH9sdWLdDRdrgSgH4VnHbrwd', 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 88, 'prompt_tokens': 528, 'total_tokens': 616}}, 'image_id': 167765, 'request_info': {'ann_ids': [1817357, 254764, 183813, 15580], 'image_id': 167765, 'ref_ids': [35549, 35550, 35551, 35552], 'sent_ids': [[101055, 101056, 101057], [101058, 101059, 101060], [101061, 101062, 101063], [101064, 101065, 101066]], 'sentences': ['tan pup', 'eyes shut', 'tan', 'jeans cuff', 'blue jean leg that the puppy is sniffing', 'jeans below doggie nose', 'hand', 'hand', 'hand', 'white dog head', 'white puppy face', 'part hidden dog']}, 'fp_sents': ['purple pup', 'eyes open', 'purple', 'shorts cuff', 'purple short leg that the kitten is sniffing', 'shorts below kitty nose', 'foot', 'foot', 'foot']}   \n",
       "1  {'api_response': {'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': 'Altered Descriptions: [\"stool that man is sitting on\", \"bench guy is sitting on\", \"throne where woman is on\", \"couch the woman is sitting in\", \"rocking chair lady sitting on\", \"bean bag chair in suit\", \"puppet man\", \"penguin man\", \"unicorn woman in skirt\", \"mermaid woman\", \"queen lady\"]', 'role': 'assistant'}}], 'created': 1699386395, 'id': 'chatcmpl-8IMJvhgbTHt8oD5YICn5VEZD3CVHa', 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'usage': {'completion_tokens': 78, 'prompt_tokens': 533, 'total_tokens': 611}}, 'image_id': 157371, 'request_info': {'ann_ids': [374494, 289709, 213140, 204643], 'image_id': 157371, 'ref_ids': [36353, 36354, 36355, 36356], 'sent_ids': [[103326, 103327, 103328], [103329, 103330, 103331], [103332, 103333, 103334], [103335, 103336, 103337]], 'sentences': ['chair that man is sitting on', 'chair guy is sitting on', 'guys chair', 'chair where woman is on', 'chair the woman is sitting in', 'chair lady sitting on', 'man in suit', 'man', 'man', 'woman in skirt', 'woman', 'lady']}, 'fp_sents': ['stool that man is sitting on', 'bench guy is sitting on', 'throne where woman is on', 'couch the woman is sitting in', 'rocking chair lady sitting on', 'bean bag chair in suit', 'puppet man', 'penguin man', 'unicorn woman in skirt', 'mermaid woman', 'queen lady']}   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                            raw_reply  \n",
       "0  ## Altered Descriptions: \\n\\n    1. Altered Description: \"single tulip\"\\n    2. Altered Description: \"plants behind cat\"\\n    3. Altered Description: \"the daisy\"\\n    4. Altered Description: \"plants before cat\"\\n    5. Altered Description: \"vase with succulents\"\\n    6. Altered Description: \"orange plant\"  \n",
       "2                                      Altered Descriptions: \\n1. \"purple pup\"\\n2. \"eyes open\"\\n3. \"purple\"\\n4. \"shorts cuff\"\\n5. \"purple short leg that the kitten is sniffing\"\\n6. \"shorts below kitty nose\"\\n7. \"foot\"\\n8. \"foot\"\\n9. \"foot\"\\n10. \"black cat head\"\\n11. \"black kitten face\"\\n12. \"part hidden cat\"  \n",
       "1                     Altered Descriptions: [\"stool that man is sitting on\", \"bench guy is sitting on\", \"throne where woman is on\", \"couch the woman is sitting in\", \"rocking chair lady sitting on\", \"bean bag chair in suit\", \"puppet man\", \"penguin man\", \"unicorn woman in skirt\", \"mermaid woman\", \"queen lady\"]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context(\"display.max_colwidth\", None, \"display.max_columns\", None):\n",
    "    display(\n",
    "        df_errors[df_errors.msg == \"Wrong number of FP sentences\"]\n",
    "        .head(3)\n",
    "        .sort_values(\"raw_reply\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with pd.option_context(\"display.max_colwidth\", None, \"display.max_columns\", None):\n",
    "#     display(df_errors.iloc[38][\"raw_reply\"])\n",
    "#     print(df_errors.iloc[38][\"raw_reply\"])\n",
    "#     print(df_errors.iloc[38][\"raw_reply\"].replace('\\\\\"', '\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Scratch Code to Debug Regex parsing of the chatGPT replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def parse_result(image_id: int, reply: str, warnings, errors) -> list[str]:\n",
    "#     def parse_result_main(reply: str, debug=False):\n",
    "#         # print(f\"ChatGPT Reply: \\n\\t{reply}\")\n",
    "#         matches = re.match(\n",
    "#             # '.*Descriptions:*\\\\s*(\\\\(.{1,} sentence[s]{0,1}\\\\):){0,1}\\\\s*(?P<descriptions>\\\\[\\\\\".*\\\\\"\\\\])',\n",
    "#             '.*Description[s]{0,1}:*\\\\s*(\\\\(.{1,} sentence[s]{0,1}\\\\):){0,1}\\\\s*\\\\[{0,1}(?P<descriptions>\\\\\".*\\\\\")\\\\]{0,1}',\n",
    "#             reply,\n",
    "#             re.MULTILINE | re.DOTALL,\n",
    "#         )\n",
    "#         if matches is None:\n",
    "#             if DEBUG:\n",
    "#                 print(\"parse_result_main() No matches\")\n",
    "#             return None\n",
    "#         list_str = matches.group(\"descriptions\")\n",
    "#         if not list_str.startswith(\"[\"):\n",
    "#             list_str = \"[\" + list_str\n",
    "#         if not list_str.endswith(\"]\"):\n",
    "#             list_str = list_str + \"]\"\n",
    "\n",
    "#         new_sent = ast.literal_eval(list_str)\n",
    "#         # print(\"New Sents: \", new_sent)\n",
    "#         if new_sent is None:\n",
    "#             errors.append(\n",
    "#                 {\n",
    "#                     \"image_id\": image_id,\n",
    "#                     \"msg\": f\"No FP sents found (fp_sents is None)\",\n",
    "#                     \"raw_reply\": reply,\n",
    "#                 }\n",
    "#             )\n",
    "#             return None\n",
    "#         return new_sent\n",
    "\n",
    "#     def parse_result_multiline_list(reply: str, debug=False):\n",
    "#         matches = re.match(\n",
    "#             '(?:.*Description[s]{0,1}:[ ]*(\\\\(.+ sentence[s]{0,1}\\\\):){0,1})\\\\n*?(?P<descriptions>\\\\n\\\\d\\\\.[ ]*\\\\\"[^\\\\n\\\\\"]+\\\\\")+',\n",
    "#             reply,\n",
    "#             re.MULTILINE | re.DOTALL,\n",
    "#         )\n",
    "#         if matches is None:\n",
    "#             if DEBUG:\n",
    "#                 print(\"parse_result_multiline_list() No matches\")\n",
    "#             return None\n",
    "#         captures = matches.capturesdict()\n",
    "#         if (captures is None or len(captures) == 0) or (\n",
    "#             captures is not None and \"descriptions\" not in captures\n",
    "#         ):\n",
    "#             return None\n",
    "#         sents = []\n",
    "#         for cap in captures[\"descriptions\"]:\n",
    "#             matches = re.match('(?:[\\\\d]\\\\.)\\\\s*?\\\\\"(?P<sent>[^\\\\\"]+)\\\\\"', cap.strip())\n",
    "#             # print(\"match: \", matches)\n",
    "#             # print(\"sent: \", matches.groupdict()[\"sent\"])\n",
    "#             sents.append(matches.groupdict()[\"sent\"])\n",
    "#         return sents\n",
    "\n",
    "#     # try:\n",
    "#     # reply = reply.replace('\\\\\"', '\"')\n",
    "#     new_sent = parse_result_main(reply)\n",
    "#     print(\"new_sent1: \", new_sent)\n",
    "#     if new_sent is None:\n",
    "#         new_sent = parse_result_multiline_list(reply)\n",
    "#         print(\"new_sent2: \", new_sent)\n",
    "#         # if new_sent is None:\n",
    "#         #     print(reply)\n",
    "#     return new_sent\n",
    "#     # except Exception as ex:\n",
    "#     #     print(\n",
    "#     #         {\n",
    "#     #             \"image_id\": image_id,\n",
    "#     #             \"msg\": str(ex) + \" ex type: \" + str(type(ex)),\n",
    "#     #             \"raw_reply\": reply,\n",
    "#     #         }\n",
    "#     #     )\n",
    "#     #     return None\n",
    "#     return None\n",
    "\n",
    "\n",
    "# reply = \"\"\"## Answer\\n\\nAltered Descriptions: (6 sentences): [\"man in a purple and black jacket bending over\", \"an older gentleman reaching up to pick to sign a page \\\", \"man wearing a yellow shirt\", \"man wearing glasses , purple shirt and khakis\", \"the chair behind the man in a yellow shirt\", \"the sofa in lavender color\"]\n",
    "# \"\"\"\n",
    "# reply = df_errors.iloc[38][\"raw_reply\"].replace('\\\\\"', '\"')\n",
    "# print(reply)\n",
    "# print(parse_result(-1, reply, [], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # reply = \"\"\"## Answer\n",
    "# # Altered Descriptions: (4 sentences): [\"the giraffe eating the grass on the ground\", \"the giraffe is grazing\", \"a giraffe looking straight at the camera\", \"giraffe on the left that is looking at cameraman\"]\n",
    "# # \"\"\"\n",
    "# # reply = df_errors.iloc[164][\"raw_reply\"]\n",
    "# reply = '''Altered Descriptions:\n",
    "# 1. \"a bowl of some smelly food possibly applesauce\"\n",
    "# 2. \"bowl of food with blue spoon in the bowl\"'''\n",
    "\n",
    "# print(reply)\n",
    "\n",
    "# patterns = [\n",
    "#     # re.compile(\n",
    "#     #     '.*Descriptions:*\\\\s*(\\\\(.{1,} sentence[s]{0,1}\\\\):){0,1}\\\\s*(?P<descriptions>\\\\[\\\\\".*\\\\\"\\\\])',\n",
    "#     #     re.MULTILINE | re.DOTALL,\n",
    "#     # ),\n",
    "#     re.compile(\n",
    "#         # '(?:[^\\\\n\\\\\"]*Descriptions:\\\\s*(\\\\(.+ sentences\\\\):)\\\\s*\\n+)(\\\\n\\\\d\\\\.\\\\s*\\\\\"[^\\\\n\\\\\"]+\\\\\")+',\n",
    "#         '(?:.*Descriptions:[ ]*(\\\\(.+ sentences\\\\):){0,1})\\\\n*?(?P<descriptions>\\\\n\\\\d\\\\.[ ]*\\\\\"[^\\\\n\\\\\"]+\\\\\")+',\n",
    "#         re.MULTILINE | re.DOTALL,\n",
    "#     ),\n",
    "# ]\n",
    "# for pat in patterns:\n",
    "#     matches = pat.match(reply)\n",
    "#     if matches is None:\n",
    "#         print(\"NO MATCHES\")\n",
    "#         continue\n",
    "#     print(\"\\nmatches: \", matches)\n",
    "#     print(\"\\ngroupdict: \", matches.groupdict())\n",
    "#     print(\"\\ncapturesdict: \", matches.capturesdict())\n",
    "#     print(\"len(ncapturesdict) \", len(matches.capturesdict()[\"descriptions\"]))\n",
    "\n",
    "#     if matches is not None and \"descriptions\" in matches.groupdict():\n",
    "#         list_str = matches.group(\"descriptions\")\n",
    "#         print(\"list: \", list_str)\n",
    "#         # Convert the string to a Python object\n",
    "#         new_sent = ast.literal_eval(list_str)\n",
    "#         print(\"new_sent: \", new_sent)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import ast\n",
    "\n",
    "# import regex as re\n",
    "\n",
    "# replies = [\n",
    "#     'Altered Description: \"purple bush to the left of the sign\"',\n",
    "#     '''Altered Descriptions:\n",
    "# 1. \"a bowl of some smelly food possibly applesauce\"\n",
    "# 2. \"bowl of food with blue spoon in the bowl\"''',\n",
    "#     \"\"\"Altered Descriptions: (1 sentence): [\"a purple chair facing the garden\"]\"\"\",\n",
    "#     \"\"\"Altered Descriptions: (6 sentences): [\"the hat of the standing man\", \"a purple hat with repetitive circular patterns\", \"the girl in the green dress standing next to the man\", \"a girl with sunglasses on\", \"a man with straight blonde hair in a blue suit with a yellow hat stands with a woman in sunglasses and a red see through outfit\", \"a man with light blonde, straight hair wearing a suit and hat standing next to a woman\"]\"\"\",\n",
    "#     \"\"\"Altered Descriptions: [\"the hat of the standing man\", \"a purple hat with repetitive circular patterns\", \"the girl in the green dress standing next to the man\", \"a girl with sunglasses on\", \"a man with straight blonde hair in a blue suit with a yellow hat stands with a woman in sunglasses and a red see through outfit\", \"a man with light blonde, straight hair wearing a suit and hat standing next to a woman\"]\"\"\",\n",
    "#     \"\"\"Modified Descriptions: (6 sentences): [\"the hat of the standing man\", \"a purple hat with repetitive circular patterns\", \"the girl in the green dress standing next to the man\", \"a girl with sunglasses on\", \"a man with straight blonde hair in a blue suit with a yellow hat stands with a woman in sunglasses and a red see through outfit\", \"a man with light blonde, straight hair wearing a suit and hat standing next to a woman\"]\"\"\",\n",
    "#     \"\"\"Modified Descriptions: [\"the hat of the standing man\", \"a purple hat with repetitive circular patterns\", \"the girl in the green dress standing next to the man\", \"a girl with sunglasses on\", \"a man with straight blonde hair in a blue suit with a yellow hat stands with a woman in sunglasses and a red see through outfit\", \"a man with light blonde, straight hair wearing a suit and hat standing next to a woman\"]\"\"\",\n",
    "#     \"\"\"Altered Descriptions (6 sentences): [\"the bowtie of the standing man\", \"a neon green tie with repetitive lightning bolt patterns\", \"the girl in the pink dress standing next to the man\", \"a girl with sunglasses on\", \"a man with blue hair in a purple suit with a neon green bowtie stands with a woman in sunglasses and a neon pink outfit\", \"a man with blonde, straight hair wearing a tuxedo and bowtie standing next to a woman\"]\"\"\",\n",
    "#     \"\"\"## Answer Altered Descriptions: (4 sentences): [\"the giraffe eating the grass on the ground\", \"the giraffe is grazing\", \"a giraffe looking straight at the camera\", \"giraffe on the left that is looking at cameraman\"]\"\"\",\n",
    "#     \"\"\"## Answer\n",
    "# Altered Descriptions: (4 sentences): [\"the giraffe eating the grass on the ground\", \"the giraffe is grazing\", \"a giraffe looking straight at the camera\", \"giraffe on the left that is looking at cameraman\"]\"\"\",\n",
    "#     #     \"\"\"Altered Descriptions: (6 sentences):\n",
    "#     # 1. \"the crown of the standing man\"\n",
    "#     # 2. \"a purple crown with repetitive circular patterns\"\n",
    "#     # 3. \"the girl in the pink dress standing next to the man\"\n",
    "#     # 4. \"a girl with sunglasses on\"\n",
    "#     # 5. \"a man with curly black hair in a black suit with a purple crown stands with a woman in sunglasses and a black see through outfit\"\n",
    "#     # 6. \"a man with dark, curly hair wearing a suit and crown standing next to a woman\"\n",
    "#     # \"\"\"\n",
    "# ]\n",
    "\n",
    "# for reply in replies:\n",
    "#     print(\"\")\n",
    "#     # matches = re.match(\"Altered Descriptions[:]+ \\\\(.{1,} sentences\\\\):\\\\s+(?P<descriptions>\\\\[\\\\\\\".*\\\\\\\"\\\\])\", reply)\n",
    "#     matches = re.match(\n",
    "#         '.*Description[s]{0,1}:*\\\\s*(\\\\(.{1,} sentence[s]{0,1}\\\\):){0,1}\\\\s*\\\\[{0,1}(?P<descriptions>\\\\\".*\\\\\")\\\\]{0,1}',\n",
    "#         reply,\n",
    "#         re.MULTILINE,\n",
    "#     )\n",
    "#     print(\"matches: \", matches)\n",
    "#     if matches is not None and \"descriptions\" in matches.groupdict():\n",
    "#         print(\"match.group: \", matches.group(\"descriptions\"))\n",
    "\n",
    "#     if matches is not None and \"descriptions\" in matches.groupdict():\n",
    "#         list_str = matches.group(\"descriptions\")\n",
    "#         # Convert the string to a Python object\n",
    "#         new_sent = ast.literal_eval(list_str)\n",
    "#         print(\"new_sent: \", new_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize False Premise Types\n",
    "\n",
    "First compute spacy tags for each FP sentence and cache the results to disk. \n",
    "\n",
    "Then, use the spacy tags to categorize the FP sentence modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.require_gpu()\n",
    "import spacy_transformers\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing spacy docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 140590/140590 [02:09<00:00, 1085.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num spacy docs:  140590\n",
      "140590 140590 140590\n"
     ]
    }
   ],
   "source": [
    "def get_fp_sentences_flat(responses: list[dict]) -> tuple[list[str], list[int]]:\n",
    "    # Make a flat list of FP sentences so we can batch process with spacy:\n",
    "    fp_sents_all = []\n",
    "    img_ids_all = []\n",
    "    for response in responses:\n",
    "        if \"fp_sents\" not in response:\n",
    "            continue\n",
    "        fp_sents: list[str] = [s for s in response[\"fp_sents\"] if len(s) > 0]\n",
    "        fp_sents_all.extend(fp_sents)\n",
    "        img_ids = [response[\"image_id\"]] * len(fp_sents)\n",
    "        img_ids_all.extend(img_ids)\n",
    "    return fp_sents_all, img_ids_all\n",
    "\n",
    "\n",
    "def get_gt_sentences_flat(refcoco: COCO) -> list[dict]:\n",
    "    # Make a flat list of FP sentences so we can batch process with spacy:\n",
    "    sents_all = []\n",
    "    for ref_id, ref in refcoco.refs.items():\n",
    "        sents: list[str] = ref[\"sentences\"]\n",
    "        sents_all.extend(\n",
    "            [\n",
    "                {\n",
    "                    \"sent\": s,\n",
    "                    \"sent_id\": s[\"sent_id\"],\n",
    "                    \"image_id\": ref[\"image_id\"],\n",
    "                    \"ref_id\": ref_id,\n",
    "                }\n",
    "                for s in sents\n",
    "            ]\n",
    "        )\n",
    "    return sents_all\n",
    "\n",
    "\n",
    "def get_spacy_docs(\n",
    "    responses, refcoco: COCO, api_results_dir: Path, force_recompute: bool = False\n",
    ") -> tuple[list[spacy.tokens.Doc], list[int]]:\n",
    "    \"\"\"\n",
    "    Get spacy doc for FP sentences in all the responses. Caches the output to\n",
    "    disk, and if a cached result already exists, it loads and returns that\n",
    "    instead of re-computing the spacy Docs.\n",
    "\n",
    "    Returns:\n",
    "    :docs: flat list of spacy docs\n",
    "    :img_ids: flat list (same length as docs) that maps indexes of docs to image_id\n",
    "    \"\"\"\n",
    "    docs_path = api_results_dir / \"fp_sentences_spacy_docs.pkl\"\n",
    "    fp_sents_all, doc_to_image = get_fp_sentences_flat(responses)\n",
    "\n",
    "    if not force_recompute and docs_path.exists():\n",
    "        print(\"loading cached spacy docs from disk\")\n",
    "        docs = pickle.load(open(docs_path, \"rb\"))\n",
    "        assert len(docs) == len(fp_sents_all), f\"{len(docs)} != {len(fp_sents_all)}\"\n",
    "    else:\n",
    "        print(\"Computing spacy docs for FP sentences\")\n",
    "        B = 1000\n",
    "        docs: list[spacy.tokens.Doc] = [\n",
    "            d\n",
    "            for d in tqdm(nlp.pipe(fp_sents_all, batch_size=B), total=len(fp_sents_all))\n",
    "        ]\n",
    "        assert len(docs) == len(fp_sents_all), f\"{len(docs)} != {len(fp_sents_all)}\"\n",
    "        pickle.dump(docs, open(docs_path, \"wb\"))\n",
    "    return docs, doc_to_image, fp_sents_all\n",
    "\n",
    "\n",
    "# Only compute/load the docs if they aren't already in memory:\n",
    "if (\n",
    "    \"docs\" not in locals()\n",
    "    or \"doc_to_image\" not in locals()\n",
    "    or \"fp_sents_all\" not in locals()\n",
    "    or True\n",
    "):\n",
    "    docs, doc_to_image, fp_sents_all = get_spacy_docs(\n",
    "        api_responses, refcoco, api_results_dir, force_recompute=False\n",
    "    )\n",
    "print(\"Num spacy docs: \", len(docs))\n",
    "print(len(docs), len(doc_to_image), len(fp_sents_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 140590/140590 [00:06<00:00, 20282.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140590 140590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def get_main_subject(sent: dict, use_root: bool = True):\n",
    "    \"\"\"Should always pass use_root=True, when there is no nsubj the ROOT is the main subject\"\"\"\n",
    "    subjects = [\n",
    "        word\n",
    "        for word, dep in zip(sent[\"spcy_WORD\"], sent[\"spcy_DEP\"])\n",
    "        if dep == \"nsubj\" and word not in nlp.Defaults.stop_words\n",
    "    ]\n",
    "    if use_root and (subjects is None or len(subjects) == 0):\n",
    "        subjects = [\n",
    "            word\n",
    "            for word, dep in zip(sent[\"spcy_WORD\"], sent[\"spcy_DEP\"])\n",
    "            if (dep == \"ROOT\") and word not in nlp.Defaults.stop_words\n",
    "        ]\n",
    "    return subjects\n",
    "\n",
    "\n",
    "def tag_fp_sentences(\n",
    "    responses: list[dict],\n",
    "    refcoco: COCO,\n",
    "    docs: list[spacy.tokens.Doc],\n",
    "    doc_to_img: list[int],\n",
    "    fp_sents_all: list[str],\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a deep copy of refcoco's `.refs_data` property, and enhances the\n",
    "    copy by adding SpaCy NLP parsing tags for POS, TAG, DEP, etc. See here\n",
    "    for more info: https://spacy.io/usage/linguistic-features\n",
    "    \"\"\"\n",
    "    # Map docs back to the fp_sents\n",
    "    fp_sent_dicts = []\n",
    "    for i, (fp_sent, doc, image_id) in tqdm(\n",
    "        enumerate(zip(fp_sents_all, docs, doc_to_image)), total=len(docs)\n",
    "    ):\n",
    "        # response = img_to_response[image_id]\n",
    "        fp_sent_dict = {\n",
    "            # \"ref_id\": -1, we'll have to match it with the right ref_id at some point\n",
    "            \"tokens\": [\n",
    "                word.strip()\n",
    "                for word in fp_sent.split(\" \")\n",
    "                if word.strip() not in string.punctuation\n",
    "            ],\n",
    "            \"raw\": fp_sent,\n",
    "            \"sent_id\": -1,\n",
    "            \"sent\": fp_sent,\n",
    "            \"spcy_WORD\": [str(word) for word in doc],\n",
    "            \"spcy_DEP\": [word.dep_ for word in doc],\n",
    "            \"spcy_POS\": [word.pos_ for word in doc],\n",
    "            \"spcy_LEM\": [word.lemma_ for word in doc],\n",
    "            \"spcy_TAG\": [word.tag_ for word in doc],\n",
    "            \"spcy_IS_STOP\": [word.is_stop for word in doc],\n",
    "            \"spcy_ENTS\": [str(ent).strip() for ent in doc.ents],\n",
    "            \"spcy_NOUN_CHUNKS\": [str(nc).strip() for nc in doc.noun_chunks],\n",
    "            # \"spcy_DOC\": doc, # this takes up way too much space 2.7G vs 115MB\n",
    "        }\n",
    "        fp_sent_dict[\"main_subject\"] = get_main_subject(fp_sent_dict)\n",
    "        # print(fp_tags)\n",
    "        fp_sent_dicts.append(fp_sent_dict)\n",
    "        # if i == 0:\n",
    "        #     print(fp_sent_dict)\n",
    "\n",
    "    return fp_sent_dicts\n",
    "\n",
    "\n",
    "fp_sent_dicts = tag_fp_sentences(\n",
    "    api_responses, refcoco, docs, doc_to_image, fp_sents_all\n",
    ")\n",
    "print(len(fp_sent_dicts), len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent': {'tokens': ['navy', 'blue', 'shirt'],\n",
       "  'raw': 'navy blue shirt',\n",
       "  'sent_id': 0,\n",
       "  'sent': 'navy blue shirt',\n",
       "  'spcy_WORD': ['navy', 'blue', 'shirt'],\n",
       "  'spcy_DEP': ['amod', 'amod', 'ROOT'],\n",
       "  'spcy_POS': ['ADJ', 'ADJ', 'NOUN'],\n",
       "  'spcy_LEM': ['navy', 'blue', 'shirt'],\n",
       "  'spcy_TAG': ['JJ', 'JJ', 'NN'],\n",
       "  'spcy_IS_STOP': [False, False, False],\n",
       "  'spcy_ENTS': [],\n",
       "  'spcy_NOUN_CHUNKS': ['navy blue shirt']},\n",
       " 'sent_id': 0,\n",
       " 'image_id': 581857,\n",
       " 'ref_id': 0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_flat = get_gt_sentences_flat(refcoco)\n",
    "gt_flat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct Ambiguous FP vs. GT Sentence Matches\n",
    "\n",
    "Sometimes GPT does not return an FP for every GT we input, i.e., we give it five ground truth sentences and it only outputs four false premise sentences. These cases are ambiguous (we don't know which sentence(s) GPT skipped) but we can attempt to match the outputs using string similarity metrics.\n",
    "\n",
    "For the initial refcocog run, around 10% of the images consist of these ambiguous cases, so it is worth correcting. We do the corrections in the below cell.\n",
    "\n",
    "#### fp vs gt sentence count \n",
    "- num_match: 23,128 images\n",
    "- not_match: 2,496 images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp:  140590\n",
      "gt:  141564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                     | 0/19877 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================================================================================================\n",
      "Num gt:  6\n",
      "img_fps:  1 dict_keys(['tokens', 'raw', 'sent_id', 'sent', 'spcy_WORD', 'spcy_DEP', 'spcy_POS', 'spcy_LEM', 'spcy_TAG', 'spcy_IS_STOP', 'spcy_ENTS', 'spcy_NOUN_CHUNKS', 'main_subject', 'ref_id', 'ann_id', 'gt_sent_id', 'gt_sent', 'is_false_premise'])\n",
      "\n",
      "ref_id:  38836\n",
      "# sentences:  3\n",
      "sentence:  110341 single rose \n",
      "sentence:  110342 flowers behind cat \n",
      "sentence:  110343 the rose \n",
      "\n",
      "ref_id:  38837\n",
      "# sentences:  4\n",
      "sentence:  110344 flowers before cat \n",
      "sentence:  110345 vase with greenery \n",
      "sentence:  110346 black plant \n",
      "sentence:  -1 orange plant gt: (110346) True, black plant\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'img_fps: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[\"'sent_id': -1\",\n",
       " \"'sent': orange plant\",\n",
       " \"'main_subject': ['plant']\",\n",
       " \"'ref_id': 38837\",\n",
       " \"'ann_id': 19399\",\n",
       " \"'gt_sent_id': 110346\",\n",
       " \"'gt_sent': black plant\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19877/19877 [00:00<00:00, 107343.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_match: 19854, not_match: 23, num_corrected: 18\n"
     ]
    }
   ],
   "source": [
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "\n",
    "def sentence_similarities(list_a: list[str], list_b: list[str]) -> dict[int, list[int]]:\n",
    "    \"\"\"Calculates the edit distance between elements of list_a and list_b.\"\"\"\n",
    "    similarity_scores = {}\n",
    "\n",
    "    for i, a_item in enumerate(list_a):\n",
    "        scores = []\n",
    "        for b_item in list_b:\n",
    "            score = levenshtein_distance(a_item, b_item)\n",
    "            scores.append(score)\n",
    "        similarity_scores[i] = scores\n",
    "\n",
    "    return similarity_scores\n",
    "\n",
    "\n",
    "def match_sentences(\n",
    "    img_fps: list[dict], gt_refs_and_sents: list[tuple[dict, dict]], debug=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Two list s of sentences for a single image, and attempt unambiguous match\n",
    "    between them. img_fps is shorter in length than gt_refs_and_sents.\n",
    "    \"\"\"\n",
    "    sim_scores = sentence_similarities(\n",
    "        [sent[\"sent\"] for sent in img_fps],\n",
    "        [gt[\"sent\"] for ref, gt in gt_refs_and_sents],\n",
    "    )\n",
    "\n",
    "    match_indices = []\n",
    "    for (fp_idx, scores), fp in zip(sim_scores.items(), img_fps):\n",
    "        idx = np.argmin(scores)\n",
    "        match_indices.append(idx)\n",
    "\n",
    "    if debug:\n",
    "        print(\"match_indices: \", match_indices)\n",
    "    # Consider matches unambiguous if each fp sentence is matched to a unique gt sentence\n",
    "    is_unambiguous = len(set(match_indices)) == len(match_indices)\n",
    "    result = []\n",
    "    if is_unambiguous:\n",
    "        for fp, match_idx in zip(img_fps, match_indices):\n",
    "            gt_match = gt_refs_and_sents[match_idx]\n",
    "            if debug:\n",
    "                print(f\"\\t match: '{fp['sent']}', '{gt_match[1]['sent']}'\")\n",
    "            result.append((fp, gt_match))\n",
    "    # else:\n",
    "    #     print(\"No match! \", match_indices)\n",
    "    return result\n",
    "\n",
    "\n",
    "def match_fp_with_gt(fp_sent_dicts: list[dict], doc_to_image: list[int], refcoco: COCO):\n",
    "    \"\"\"\n",
    "    Return a copy of the refcoco object where the false premise sentences in\n",
    "    fp_sent_dicts are added to the refcoco.imgs.refs[\"sentences\"] lists. The\n",
    "    fp sentence are mapped to their ground truth sentences\n",
    "    \"\"\"\n",
    "    DEBUG = True\n",
    "    refcoco = deepcopy(refcoco)\n",
    "    gt_flat = get_gt_sentences_flat(refcoco)\n",
    "    print(\"fp: \", len(fp_sent_dicts))\n",
    "    print(\"gt: \", len(gt_flat))\n",
    "    img_fps_all = defaultdict(list)\n",
    "    {img_fps_all[img_id].append(fp) for fp, img_id in zip(fp_sent_dicts, doc_to_image)}\n",
    "    num_match, num_not_match = 0, 0\n",
    "    num_corrected = 0\n",
    "\n",
    "    for idx, (image_id, img_fps) in tqdm(\n",
    "        enumerate(img_fps_all.items()), total=len(img_fps_all)\n",
    "    ):\n",
    "        # img_fps is a list of dicts. Each dict has keys: ['tokens',\n",
    "        #    'raw', 'sent_id', 'sent', 'spcy_WORD', 'spcy_DEP', 'spcy_POS',\n",
    "        #    'spcy_LEM', 'spcy_TAG', 'spcy_IS_STOP', 'spcy_ENTS',\n",
    "        #    'spcy_NOUN_CHUNKS', 'main_subject']\n",
    "        # This function adds the following keys to these dicts:\n",
    "        #    ['ref_id', 'ann_id', 'gt_sent_id', 'gt_sent']\n",
    "        gt_refs_and_sents: list[tuple[dict, dict]] = []\n",
    "\n",
    "        for img_ref in refcoco.img_to_refs[image_id]:\n",
    "            # img_ref keys: ['image_id', 'split', 'sentences', 'file_name',\n",
    "            #    'category_id', 'ann_id', 'sent_ids', 'ref_id']\n",
    "            for s in img_ref[\"sentences\"]:\n",
    "                gt_refs_and_sents.append((img_ref, s))\n",
    "        if len(gt_refs_and_sents) == len(img_fps):\n",
    "            num_match += 1\n",
    "            for fp, (img_ref, gt_sent) in zip(img_fps, gt_refs_and_sents):\n",
    "                if len(fp[\"sent\"].strip()) == 0:\n",
    "                    print(\"EMPTY1 (img_id: {image_id}): \", fp)\n",
    "                fp[\"ref_id\"] = img_ref[\"ref_id\"]\n",
    "                fp[\"ann_id\"] = img_ref[\"ann_id\"]\n",
    "                fp[\"gt_sent_id\"] = gt_sent[\"sent_id\"]\n",
    "                fp[\"gt_sent\"] = gt_sent[\"sent\"]\n",
    "                fp[\"is_false_premise\"] = True\n",
    "                img_ref[\"sentences\"].append(fp)\n",
    "        else:\n",
    "            num_not_match += 1\n",
    "            matches = match_sentences(img_fps, gt_refs_and_sents, False)\n",
    "            if matches:\n",
    "                num_corrected += 1\n",
    "            for fp, (img_ref, gt_sent) in matches:\n",
    "                if len(fp[\"sent\"].strip()) == 0:\n",
    "                    print(f\"EMPTY2 (img_id: {image_id}): \", fp)\n",
    "                fp[\"ref_id\"] = img_ref[\"ref_id\"]\n",
    "                fp[\"ann_id\"] = img_ref[\"ann_id\"]\n",
    "                fp[\"gt_sent_id\"] = gt_sent[\"sent_id\"]\n",
    "                fp[\"gt_sent\"] = gt_sent[\"sent\"]\n",
    "                fp[\"is_false_premise\"] = True\n",
    "                img_ref[\"sentences\"].append(fp)\n",
    "        # DEBUG:\n",
    "        if DEBUG and num_corrected == 1:\n",
    "            print(\"\")\n",
    "            print(\"=\" * 200)\n",
    "            print(\"Num gt: \", len(gt_refs_and_sents))\n",
    "            print(\"img_fps: \", len(img_fps), img_fps[0].keys())\n",
    "            img_refs = refcoco.img_to_refs[image_id]\n",
    "            for img_ref in img_refs:\n",
    "                print(\"\")\n",
    "                print(\"ref_id: \", img_ref[\"ref_id\"])\n",
    "                print(\"# sentences: \", len(img_ref[\"sentences\"]))\n",
    "                # print(\"# sentences: \", len([ref img_refs[\"sentences\"]]))\n",
    "                for s in img_ref[\"sentences\"]:\n",
    "                    print(\n",
    "                        \"sentence: \",\n",
    "                        s[\"sent_id\"],\n",
    "                        s[\"sent\"],\n",
    "                        (\n",
    "                            f\"gt: ({s['gt_sent_id']}) {s['is_false_premise']}, {s['gt_sent']}\"\n",
    "                        )\n",
    "                        if \"is_false_premise\" in s\n",
    "                        else \"\",\n",
    "                    )\n",
    "\n",
    "            for i in range(len(img_fps)):\n",
    "                display(\n",
    "                    \"img_fps: \",\n",
    "                    [\n",
    "                        f\"'{k}': {v}\"\n",
    "                        for k, v in img_fps[i].items()\n",
    "                        if k\n",
    "                        in {\n",
    "                            \"sent_id\",\n",
    "                            \"main_subject\",\n",
    "                            \"ref_id\",\n",
    "                            \"ann_id\",\n",
    "                            \"gt_sent_id\",\n",
    "                            \"gt_sent\",\n",
    "                            \"sent\",\n",
    "                        }\n",
    "                    ],\n",
    "                )\n",
    "            DEBUG = False\n",
    "\n",
    "    print(\n",
    "        f\"num_match: {num_match}, not_match: {num_not_match}, num_corrected: {num_corrected}\"\n",
    "    )\n",
    "    return refcoco\n",
    "\n",
    "\n",
    "# REFSEG_DIR = Path(\"/shared/gbiamby/data/refer_seg\")\n",
    "# REFSEG_DIR = Path(\"output/ref_seg\")\n",
    "# refcoco = build_refcoco(REFSEG_DIR, \"refcocog\", \"google_enhanced\")\n",
    "refcoco_new = match_fp_with_gt(fp_sent_dicts, doc_to_image, refcoco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Detect Which Part of Each FP Sentence is Changed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████████████████████▎                                                                                                                                                                                                                                                | 4230/49856 [00:00<00:02, 21524.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['shirt']\n",
      "navy blue shirt\n",
      "yellow shirt\n",
      "[('-', ['navy', 'blue']), ('+', ['yellow']), ('=', ['shirt'])]\n",
      "num changes:  1\n",
      "\n",
      "['woman']\n",
      "woman back in blue\n",
      "woman back in red\n",
      "[('=', ['woman', 'back', 'in']), ('-', ['blue']), ('+', ['red'])]\n",
      "num changes:  1\n",
      "\n",
      "['shirt']\n",
      "blue shirt\n",
      "purple shirt\n",
      "[('-', ['blue']), ('+', ['purple']), ('=', ['shirt'])]\n",
      "num changes:  1\n",
      "\n",
      "['shirt']\n",
      "gray shirt wearing glasses\n",
      "gray shirt wearing sunglasses\n",
      "[('=', ['gray', 'shirt', 'wearing']), ('-', ['glasses']), ('+', ['sunglasses'])]\n",
      "num changes:  1\n",
      "\n",
      "['lady']\n",
      "lady with glasses\n",
      "lady with sunglasses\n",
      "[('=', ['lady', 'with']), ('-', ['glasses']), ('+', ['sunglasses'])]\n",
      "num changes:  1\n",
      "\n",
      "['woman']\n",
      "woman with gray shirt standing next to man\n",
      "woman with purple shirt standing next to man\n",
      "[('=', ['woman', 'with']), ('-', ['gray']), ('+', ['purple']), ('=', ['shirt', 'standing', 'next', 'to', 'man'])]\n",
      "num changes:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49856/49856 [00:02<00:00, 23584.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>gt_subj</th>\n",
       "      <th>fp_subj</th>\n",
       "      <th>gt_sent</th>\n",
       "      <th>fp_sent</th>\n",
       "      <th>num_changes</th>\n",
       "      <th>num_subs</th>\n",
       "      <th>num_del</th>\n",
       "      <th>num_add</th>\n",
       "      <th>diff_ops</th>\n",
       "      <th>diff</th>\n",
       "      <th>gt_NOUN_CHUNKS</th>\n",
       "      <th>fp_NOUN_CHUNKS</th>\n",
       "      <th>change_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>581857</td>\n",
       "      <td>1</td>\n",
       "      <td>[shirt]</td>\n",
       "      <td>[shirt]</td>\n",
       "      <td>navy blue shirt</td>\n",
       "      <td>yellow shirt</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-, +, =)</td>\n",
       "      <td>[(-, [navy, blue]), (+, [yellow]), (=, [shirt])]</td>\n",
       "      <td>[navy blue shirt]</td>\n",
       "      <td>[yellow shirt]</td>\n",
       "      <td>other_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>581857</td>\n",
       "      <td>1</td>\n",
       "      <td>[woman]</td>\n",
       "      <td>[woman]</td>\n",
       "      <td>woman back in blue</td>\n",
       "      <td>woman back in red</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [woman, back, in]), (-, [blue]), (+, [red])]</td>\n",
       "      <td>[woman, blue]</td>\n",
       "      <td>[woman, red]</td>\n",
       "      <td>other_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>581857</td>\n",
       "      <td>1</td>\n",
       "      <td>[shirt]</td>\n",
       "      <td>[shirt]</td>\n",
       "      <td>blue shirt</td>\n",
       "      <td>purple shirt</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-, +, =)</td>\n",
       "      <td>[(-, [blue]), (+, [purple]), (=, [shirt])]</td>\n",
       "      <td>[blue shirt]</td>\n",
       "      <td>[purple shirt]</td>\n",
       "      <td>other_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>581857</td>\n",
       "      <td>1</td>\n",
       "      <td>[glasses]</td>\n",
       "      <td>[shirt]</td>\n",
       "      <td>gray shirt wearing glasses</td>\n",
       "      <td>gray shirt wearing sunglasses</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [gray, shirt, wearing]), (-, [glasses]), ...</td>\n",
       "      <td>[gray shirt wearing glasses]</td>\n",
       "      <td>[gray shirt, sunglasses]</td>\n",
       "      <td>main_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>581857</td>\n",
       "      <td>1</td>\n",
       "      <td>[lady]</td>\n",
       "      <td>[lady]</td>\n",
       "      <td>lady with glasses</td>\n",
       "      <td>lady with sunglasses</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [lady, with]), (-, [glasses]), (+, [sungl...</td>\n",
       "      <td>[lady, glasses]</td>\n",
       "      <td>[lady, sunglasses]</td>\n",
       "      <td>other_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140538</th>\n",
       "      <td>49854</td>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>[giraffe]</td>\n",
       "      <td>[giraffe]</td>\n",
       "      <td>giraffe whose neck is partially covered by other</td>\n",
       "      <td>giraffe whose tail is partially covered by other</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +, =)</td>\n",
       "      <td>[(=, [giraffe, whose]), (-, [neck]), (+, [tail...</td>\n",
       "      <td>[giraffe, whose neck]</td>\n",
       "      <td>[giraffe, whose tail]</td>\n",
       "      <td>other_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140539</th>\n",
       "      <td>49855</td>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>[giraffe]</td>\n",
       "      <td>[zebra]</td>\n",
       "      <td>shorter giraffe</td>\n",
       "      <td>taller zebra</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-, +)</td>\n",
       "      <td>[(-, [shorter, giraffe]), (+, [taller, zebra])]</td>\n",
       "      <td>[shorter giraffe]</td>\n",
       "      <td>[taller zebra]</td>\n",
       "      <td>main_subject(+2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140540</th>\n",
       "      <td>49855</td>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>[giraffe]</td>\n",
       "      <td>[giraffe]</td>\n",
       "      <td>giraffe closest to camera</td>\n",
       "      <td>giraffe furthest from camera</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +, =)</td>\n",
       "      <td>[(=, [giraffe]), (-, [closest, to]), (+, [furt...</td>\n",
       "      <td>[giraffe, camera]</td>\n",
       "      <td>[giraffe, camera]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140541</th>\n",
       "      <td>49855</td>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>[neck]</td>\n",
       "      <td>[neck]</td>\n",
       "      <td>bent neck</td>\n",
       "      <td>straight neck</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-, +, =)</td>\n",
       "      <td>[(-, [bent]), (+, [straight]), (=, [neck])]</td>\n",
       "      <td>[bent neck]</td>\n",
       "      <td>[straight neck]</td>\n",
       "      <td>other_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140542</th>\n",
       "      <td>49855</td>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>[animal]</td>\n",
       "      <td>[animal]</td>\n",
       "      <td>shorter animal</td>\n",
       "      <td>smaller animal</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-, +, =)</td>\n",
       "      <td>[(-, [shorter]), (+, [smaller]), (=, [animal])]</td>\n",
       "      <td>[shorter animal]</td>\n",
       "      <td>[smaller animal]</td>\n",
       "      <td>other_subject</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140543 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ref_id  image_id  cat_id    gt_subj    fp_subj  \\\n",
       "0            0    581857       1    [shirt]    [shirt]   \n",
       "1            0    581857       1    [woman]    [woman]   \n",
       "2            0    581857       1    [shirt]    [shirt]   \n",
       "3            1    581857       1  [glasses]    [shirt]   \n",
       "4            1    581857       1     [lady]     [lady]   \n",
       "...        ...       ...     ...        ...        ...   \n",
       "140538   49854        72      25  [giraffe]  [giraffe]   \n",
       "140539   49855        72      25  [giraffe]    [zebra]   \n",
       "140540   49855        72      25  [giraffe]  [giraffe]   \n",
       "140541   49855        72      25     [neck]     [neck]   \n",
       "140542   49855        72      25   [animal]   [animal]   \n",
       "\n",
       "                                                 gt_sent  \\\n",
       "0                                        navy blue shirt   \n",
       "1                                     woman back in blue   \n",
       "2                                             blue shirt   \n",
       "3                             gray shirt wearing glasses   \n",
       "4                                      lady with glasses   \n",
       "...                                                  ...   \n",
       "140538  giraffe whose neck is partially covered by other   \n",
       "140539                                   shorter giraffe   \n",
       "140540                         giraffe closest to camera   \n",
       "140541                                         bent neck   \n",
       "140542                                    shorter animal   \n",
       "\n",
       "                                                 fp_sent  num_changes  \\\n",
       "0                                           yellow shirt            1   \n",
       "1                                      woman back in red            1   \n",
       "2                                           purple shirt            1   \n",
       "3                          gray shirt wearing sunglasses            1   \n",
       "4                                   lady with sunglasses            1   \n",
       "...                                                  ...          ...   \n",
       "140538  giraffe whose tail is partially covered by other            1   \n",
       "140539                                      taller zebra            1   \n",
       "140540                      giraffe furthest from camera            1   \n",
       "140541                                     straight neck            1   \n",
       "140542                                    smaller animal            1   \n",
       "\n",
       "        num_subs  num_del  num_add      diff_ops  \\\n",
       "0              1        0        0     (-, +, =)   \n",
       "1              1        0        0     (=, -, +)   \n",
       "2              1        0        0     (-, +, =)   \n",
       "3              1        0        0     (=, -, +)   \n",
       "4              1        0        0     (=, -, +)   \n",
       "...          ...      ...      ...           ...   \n",
       "140538         1        0        0  (=, -, +, =)   \n",
       "140539         1        0        0        (-, +)   \n",
       "140540         1        0        0  (=, -, +, =)   \n",
       "140541         1        0        0     (-, +, =)   \n",
       "140542         1        0        0     (-, +, =)   \n",
       "\n",
       "                                                     diff  \\\n",
       "0        [(-, [navy, blue]), (+, [yellow]), (=, [shirt])]   \n",
       "1       [(=, [woman, back, in]), (-, [blue]), (+, [red])]   \n",
       "2              [(-, [blue]), (+, [purple]), (=, [shirt])]   \n",
       "3       [(=, [gray, shirt, wearing]), (-, [glasses]), ...   \n",
       "4       [(=, [lady, with]), (-, [glasses]), (+, [sungl...   \n",
       "...                                                   ...   \n",
       "140538  [(=, [giraffe, whose]), (-, [neck]), (+, [tail...   \n",
       "140539    [(-, [shorter, giraffe]), (+, [taller, zebra])]   \n",
       "140540  [(=, [giraffe]), (-, [closest, to]), (+, [furt...   \n",
       "140541        [(-, [bent]), (+, [straight]), (=, [neck])]   \n",
       "140542    [(-, [shorter]), (+, [smaller]), (=, [animal])]   \n",
       "\n",
       "                      gt_NOUN_CHUNKS            fp_NOUN_CHUNKS  \\\n",
       "0                  [navy blue shirt]            [yellow shirt]   \n",
       "1                      [woman, blue]              [woman, red]   \n",
       "2                       [blue shirt]            [purple shirt]   \n",
       "3       [gray shirt wearing glasses]  [gray shirt, sunglasses]   \n",
       "4                    [lady, glasses]        [lady, sunglasses]   \n",
       "...                              ...                       ...   \n",
       "140538         [giraffe, whose neck]     [giraffe, whose tail]   \n",
       "140539             [shorter giraffe]            [taller zebra]   \n",
       "140540             [giraffe, camera]         [giraffe, camera]   \n",
       "140541                   [bent neck]           [straight neck]   \n",
       "140542              [shorter animal]          [smaller animal]   \n",
       "\n",
       "             change_type  \n",
       "0          other_subject  \n",
       "1          other_subject  \n",
       "2          other_subject  \n",
       "3           main_subject  \n",
       "4          other_subject  \n",
       "...                  ...  \n",
       "140538     other_subject  \n",
       "140539  main_subject(+2)  \n",
       "140540     NOT_MAIN_SUBJ  \n",
       "140541     other_subject  \n",
       "140542     other_subject  \n",
       "\n",
       "[140543 rows x 16 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_num_changes(diff):\n",
    "    diff_ops = [d[0] for d in diff]\n",
    "    num_subs = \"\".join(diff_ops).count(\"-+\")\n",
    "    num_deletions = \"\".join(diff_ops).replace(\"-+\", \"\").count(\"-\")\n",
    "    num_additions = \"\".join(diff_ops).replace(\"-+\", \"\").count(\"+\")\n",
    "    total_changes = num_subs + num_deletions + num_additions\n",
    "    return total_changes, num_subs, num_deletions, num_additions\n",
    "\n",
    "\n",
    "def get_sentence_lookup(refcoco: COCO) -> dict[int, dict[str, Any]]:\n",
    "    \"\"\"Returns dict of all the sentences, with sent_id as the key\"\"\"\n",
    "    sent_lookup = {}\n",
    "    for ref_id, ref in refcoco.refs.items():\n",
    "        for sent in ref[\"sentences\"]:\n",
    "            if \"is_false_premise\" in sent and sent[\"is_false_premise\"]:\n",
    "                continue\n",
    "            sent_lookup[sent[\"sent_id\"]] = sent\n",
    "    return sent_lookup\n",
    "\n",
    "\n",
    "def get_change_type(fp_sent: dict, change_info: dict):\n",
    "    change_type = \"\"\n",
    "\n",
    "    if change_info[\"num_changes\"] == 1 and change_info[\"num_subs\"] == 1:\n",
    "        # Sentence has a single change. Get the subtracted and added words:\n",
    "        _sub_words, _add_words = None, None\n",
    "        for op, words in change_info[\"diff\"]:\n",
    "            # print(op)\n",
    "            if op == \"-\":\n",
    "                _sub_words = words\n",
    "                # print(\"_sub_words \", _sub_words)\n",
    "            if op == \"+\":\n",
    "                _add_words = words\n",
    "                # print(\"_add_words \",_add_words)\n",
    "                break\n",
    "        assert _sub_words, str(change_info[\"diff\"])\n",
    "        assert _add_words, str(change_info[\"diff\"])\n",
    "        # Categorize:\n",
    "        gt_subject = change_info[\"gt_subj\"]\n",
    "        if isinstance(gt_subject, list) and len(gt_subject) > 0:\n",
    "            gt_subject = gt_subject[0]\n",
    "        if gt_subject in _sub_words:\n",
    "            change_type = \"main_subject\"\n",
    "            if len(_sub_words) > 1:\n",
    "                change_type += f\"(+{len(_sub_words)})\"\n",
    "        else:\n",
    "            change_type = \"NOT_MAIN_SUBJ\"\n",
    "            new_phrase = \" \".join(_add_words)\n",
    "            for nc in change_info[\"fp_NOUN_CHUNKS\"]:\n",
    "                if new_phrase in nc:\n",
    "                    change_type = \"other_subject\"\n",
    "\n",
    "    return change_type\n",
    "\n",
    "\n",
    "def detect_changes(refcoco: COCO):\n",
    "    sent_lookup = get_sentence_lookup(refcoco)\n",
    "    items = []\n",
    "    for idx, (ref_id, ref) in tqdm(\n",
    "        enumerate(refcoco.refs.items()), total=len(refcoco.refs)\n",
    "    ):\n",
    "        sentences: list[str] = [\n",
    "            s\n",
    "            for s in ref[\"sentences\"]\n",
    "            if (\"is_false_premise\" in s and s[\"is_false_premise\"])\n",
    "        ]\n",
    "        for sent in sentences:\n",
    "            gt_sent = sent_lookup[sent[\"gt_sent_id\"]]\n",
    "            diffs = string_diff(sent[\"gt_sent\"], sent[\"sent\"])\n",
    "            num_changes, subs, deletions, additions = get_num_changes(diffs)\n",
    "            items.append(\n",
    "                {\n",
    "                    \"ref_id\": ref_id,\n",
    "                    \"image_id\": ref[\"image_id\"],\n",
    "                    \"cat_id\": ref[\"category_id\"],\n",
    "                    \"gt_subj\": get_main_subject(gt_sent),\n",
    "                    \"fp_subj\": sent[\"main_subject\"],\n",
    "                    \"gt_sent\": sent[\"gt_sent\"],\n",
    "                    \"fp_sent\": sent[\"sent\"],\n",
    "                    \"num_changes\": num_changes,\n",
    "                    \"num_subs\": subs,\n",
    "                    \"num_del\": deletions,\n",
    "                    \"num_add\": additions,\n",
    "                    \"diff_ops\": tuple([d[0] for d in diffs]),\n",
    "                    \"diff\": diffs,\n",
    "                    \"gt_NOUN_CHUNKS\": gt_sent[\"spcy_NOUN_CHUNKS\"],\n",
    "                    \"fp_NOUN_CHUNKS\": sent[\"spcy_NOUN_CHUNKS\"],\n",
    "                }\n",
    "            )\n",
    "            items[-1][\"change_type\"] = get_change_type(sent, items[-1])\n",
    "            sent[\"change_type\"] = items[-1][\"change_type\"]\n",
    "            if idx < 2:\n",
    "                print(\"\")\n",
    "                print(sent[\"main_subject\"])\n",
    "                print(sent[\"gt_sent\"])\n",
    "                print(sent[\"sent\"])\n",
    "                print(diffs)\n",
    "                print(\"num changes: \", num_changes)\n",
    "\n",
    "    return pd.DataFrame(items)\n",
    "\n",
    "\n",
    "df_changes = detect_changes(refcoco_new)\n",
    "display(df_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_changes</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              total\n",
       "num_changes        \n",
       "0              5981\n",
       "1            123433\n",
       "2             10576\n",
       "3               523\n",
       "4                26\n",
       "5                 3\n",
       "6                 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_changes</th>\n",
       "      <th>diff_ops</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>(=,)</th>\n",
       "      <td>5981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th>(-, +, =)</th>\n",
       "      <td>46646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(=, -, +)</th>\n",
       "      <td>30807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(=, -, +, =)</th>\n",
       "      <td>24838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-, +)</th>\n",
       "      <td>20849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>(=, -, +, =, -, =, +, =, -, +)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5</th>\n",
       "      <th>(-, +, =, -, +, =, -, +, =, -, +, =, -, +)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(=, -, +, =, -, +, =, -, +, =, -, +, =, -, +)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(=, -, +, =, -, +, =, -, +, =, -, +, =, -, +, =)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>(-, +, =, -, +, =, -, +, =, -, +, =, -, +, =, -, +)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                total\n",
       "num_changes diff_ops                                                 \n",
       "0           (=,)                                                 5981\n",
       "1           (-, +, =)                                           46646\n",
       "            (=, -, +)                                           30807\n",
       "            (=, -, +, =)                                        24838\n",
       "            (-, +)                                              20849\n",
       "...                                                               ...\n",
       "4           (=, -, +, =, -, =, +, =, -, +)                          1\n",
       "5           (-, +, =, -, +, =, -, +, =, -, +, =, -, +)              1\n",
       "            (=, -, +, =, -, +, =, -, +, =, -, +, =, -, +)           1\n",
       "            (=, -, +, =, -, +, =, -, +, =, -, +, =, -, +, =)        1\n",
       "6           (-, +, =, -, +, =, -, +, =, -, +, =, -, +, =, -...      1\n",
       "\n",
       "[61 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_changes</th>\n",
       "      <th>num_subs</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>5981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>123140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>2</th>\n",
       "      <td>10395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>3</th>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4</th>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       total\n",
       "num_changes num_subs        \n",
       "0           0           5981\n",
       "1           1         123140\n",
       "            0            293\n",
       "2           2          10395\n",
       "            1             95\n",
       "            0             86\n",
       "3           3            513\n",
       "            1              5\n",
       "            2              5\n",
       "4           4             24\n",
       "            2              1\n",
       "            3              1\n",
       "5           5              3\n",
       "6           6              1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>17403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOT_MAIN_SUBJ</th>\n",
       "      <td>6145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_subject</th>\n",
       "      <td>52146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_subject(+2)</th>\n",
       "      <td>4898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_subject(+3)</th>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_subject(+4)</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_subject(+5)</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_subject(+6)</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_subject(+7)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_subject(+8)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_subject</th>\n",
       "      <td>59653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  total\n",
       "change_type            \n",
       "                  17403\n",
       "NOT_MAIN_SUBJ      6145\n",
       "main_subject      52146\n",
       "main_subject(+2)   4898\n",
       "main_subject(+3)    268\n",
       "main_subject(+4)     21\n",
       "main_subject(+5)      4\n",
       "main_subject(+6)      3\n",
       "main_subject(+7)      1\n",
       "main_subject(+8)      1\n",
       "other_subject     59653"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    df_changes.groupby([\"num_changes\"], dropna=False).agg(total=(\"ref_id\", \"count\"))\n",
    ")\n",
    "display(\n",
    "    df_changes.groupby([\"num_changes\", \"diff_ops\"], dropna=False)\n",
    "    .agg(total=(\"ref_id\", \"count\"))\n",
    "    .sort_values([\"num_changes\", \"total\"], ascending=[True, False])\n",
    ")\n",
    "display(\n",
    "    df_changes.groupby([\"num_changes\", \"num_subs\"], dropna=False)\n",
    "    .agg(total=(\"ref_id\", \"count\"))\n",
    "    .sort_values([\"num_changes\", \"total\"], ascending=[True, False])\n",
    ")\n",
    "display(\n",
    "    df_changes.groupby([\"change_type\"], dropna=False).agg(total=(\"ref_id\", \"count\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>gt_subj</th>\n",
       "      <th>fp_subj</th>\n",
       "      <th>gt_sent</th>\n",
       "      <th>fp_sent</th>\n",
       "      <th>num_changes</th>\n",
       "      <th>num_subs</th>\n",
       "      <th>num_del</th>\n",
       "      <th>num_add</th>\n",
       "      <th>diff_ops</th>\n",
       "      <th>diff</th>\n",
       "      <th>gt_NOUN_CHUNKS</th>\n",
       "      <th>fp_NOUN_CHUNKS</th>\n",
       "      <th>change_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>581739</td>\n",
       "      <td>86</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>no 3</td>\n",
       "      <td>no 7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [no]), (-, [3]), (+, [7])]</td>\n",
       "      <td>[no]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9</td>\n",
       "      <td>581739</td>\n",
       "      <td>86</td>\n",
       "      <td>[flower]</td>\n",
       "      <td>[flower]</td>\n",
       "      <td>vase of the flower tilting over</td>\n",
       "      <td>vase of the flower standing upright</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [vase, of, the, flower]), (-, [tilting, over]), (+, [standing, upright])]</td>\n",
       "      <td>[vase, the flower]</td>\n",
       "      <td>[vase, the flower]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11</td>\n",
       "      <td>581738</td>\n",
       "      <td>6</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>red bus you dont see the front of</td>\n",
       "      <td>green bus you dont see the front of</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-, +, =)</td>\n",
       "      <td>[(-, [red]), (+, [green]), (=, [bus, you, dont, see, the, front, of])]</td>\n",
       "      <td>[you, the front]</td>\n",
       "      <td>[you, the front]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>15</td>\n",
       "      <td>581719</td>\n",
       "      <td>1</td>\n",
       "      <td>[boy]</td>\n",
       "      <td>[boy]</td>\n",
       "      <td>boy with a cap close</td>\n",
       "      <td>boy with a cap open</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [boy, with, a, cap]), (-, [close]), (+, [open])]</td>\n",
       "      <td>[boy, a cap]</td>\n",
       "      <td>[boy, a cap]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>18</td>\n",
       "      <td>581657</td>\n",
       "      <td>73</td>\n",
       "      <td>[screen]</td>\n",
       "      <td>[laptop]</td>\n",
       "      <td>laptop wit dim screen</td>\n",
       "      <td>laptop with vibrant screen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +, =)</td>\n",
       "      <td>[(=, [laptop]), (-, [wit, dim]), (+, [with, vibrant]), (=, [screen])]</td>\n",
       "      <td>[laptop wit dim screen]</td>\n",
       "      <td>[laptop, vibrant screen]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140514</th>\n",
       "      <td>49846</td>\n",
       "      <td>144</td>\n",
       "      <td>25</td>\n",
       "      <td>[giraffe]</td>\n",
       "      <td>[eating]</td>\n",
       "      <td>drinking water giraffe</td>\n",
       "      <td>eating leaves giraffe</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-, +, =)</td>\n",
       "      <td>[(-, [drinking, water]), (+, [eating, leaves]), (=, [giraffe])]</td>\n",
       "      <td>[drinking water giraffe]</td>\n",
       "      <td>[giraffe]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140515</th>\n",
       "      <td>49846</td>\n",
       "      <td>144</td>\n",
       "      <td>25</td>\n",
       "      <td>[giraffe]</td>\n",
       "      <td>[giraffe]</td>\n",
       "      <td>giraffe with head down</td>\n",
       "      <td>giraffe with head up</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [giraffe, with, head]), (-, [down]), (+, [up])]</td>\n",
       "      <td>[giraffe, head]</td>\n",
       "      <td>[giraffe, head]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140516</th>\n",
       "      <td>49846</td>\n",
       "      <td>144</td>\n",
       "      <td>25</td>\n",
       "      <td>[head]</td>\n",
       "      <td>[head]</td>\n",
       "      <td>head down</td>\n",
       "      <td>head up</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +)</td>\n",
       "      <td>[(=, [head]), (-, [down]), (+, [up])]</td>\n",
       "      <td>[head]</td>\n",
       "      <td>[head]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140523</th>\n",
       "      <td>49849</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>man full</td>\n",
       "      <td>cat full</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(-, +, =)</td>\n",
       "      <td>[(-, [man]), (+, [cat]), (=, [full])]</td>\n",
       "      <td>[man full]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140540</th>\n",
       "      <td>49855</td>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>[giraffe]</td>\n",
       "      <td>[giraffe]</td>\n",
       "      <td>giraffe closest to camera</td>\n",
       "      <td>giraffe furthest from camera</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(=, -, +, =)</td>\n",
       "      <td>[(=, [giraffe]), (-, [closest, to]), (+, [furthest, from]), (=, [camera])]</td>\n",
       "      <td>[giraffe, camera]</td>\n",
       "      <td>[giraffe, camera]</td>\n",
       "      <td>NOT_MAIN_SUBJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6145 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ref_id  image_id  cat_id    gt_subj    fp_subj  \\\n",
       "21           8    581739      86         []         []   \n",
       "24           9    581739      86   [flower]   [flower]   \n",
       "27          11    581738       6         []         []   \n",
       "40          15    581719       1      [boy]      [boy]   \n",
       "48          18    581657      73   [screen]   [laptop]   \n",
       "...        ...       ...     ...        ...        ...   \n",
       "140514   49846       144      25  [giraffe]   [eating]   \n",
       "140515   49846       144      25  [giraffe]  [giraffe]   \n",
       "140516   49846       144      25     [head]     [head]   \n",
       "140523   49849       113       1         []         []   \n",
       "140540   49855        72      25  [giraffe]  [giraffe]   \n",
       "\n",
       "                                  gt_sent  \\\n",
       "21                                   no 3   \n",
       "24        vase of the flower tilting over   \n",
       "27      red bus you dont see the front of   \n",
       "40                   boy with a cap close   \n",
       "48                  laptop wit dim screen   \n",
       "...                                   ...   \n",
       "140514             drinking water giraffe   \n",
       "140515             giraffe with head down   \n",
       "140516                          head down   \n",
       "140523                           man full   \n",
       "140540          giraffe closest to camera   \n",
       "\n",
       "                                    fp_sent  num_changes  num_subs  num_del  \\\n",
       "21                                     no 7            1         1        0   \n",
       "24      vase of the flower standing upright            1         1        0   \n",
       "27      green bus you dont see the front of            1         1        0   \n",
       "40                      boy with a cap open            1         1        0   \n",
       "48               laptop with vibrant screen            1         1        0   \n",
       "...                                     ...          ...       ...      ...   \n",
       "140514                eating leaves giraffe            1         1        0   \n",
       "140515                 giraffe with head up            1         1        0   \n",
       "140516                              head up            1         1        0   \n",
       "140523                             cat full            1         1        0   \n",
       "140540         giraffe furthest from camera            1         1        0   \n",
       "\n",
       "        num_add      diff_ops  \\\n",
       "21            0     (=, -, +)   \n",
       "24            0     (=, -, +)   \n",
       "27            0     (-, +, =)   \n",
       "40            0     (=, -, +)   \n",
       "48            0  (=, -, +, =)   \n",
       "...         ...           ...   \n",
       "140514        0     (-, +, =)   \n",
       "140515        0     (=, -, +)   \n",
       "140516        0     (=, -, +)   \n",
       "140523        0     (-, +, =)   \n",
       "140540        0  (=, -, +, =)   \n",
       "\n",
       "                                                                                  diff  \\\n",
       "21                                                     [(=, [no]), (-, [3]), (+, [7])]   \n",
       "24      [(=, [vase, of, the, flower]), (-, [tilting, over]), (+, [standing, upright])]   \n",
       "27              [(-, [red]), (+, [green]), (=, [bus, you, dont, see, the, front, of])]   \n",
       "40                               [(=, [boy, with, a, cap]), (-, [close]), (+, [open])]   \n",
       "48               [(=, [laptop]), (-, [wit, dim]), (+, [with, vibrant]), (=, [screen])]   \n",
       "...                                                                                ...   \n",
       "140514                 [(-, [drinking, water]), (+, [eating, leaves]), (=, [giraffe])]   \n",
       "140515                            [(=, [giraffe, with, head]), (-, [down]), (+, [up])]   \n",
       "140516                                           [(=, [head]), (-, [down]), (+, [up])]   \n",
       "140523                                           [(-, [man]), (+, [cat]), (=, [full])]   \n",
       "140540      [(=, [giraffe]), (-, [closest, to]), (+, [furthest, from]), (=, [camera])]   \n",
       "\n",
       "                  gt_NOUN_CHUNKS            fp_NOUN_CHUNKS    change_type  \n",
       "21                          [no]                        []  NOT_MAIN_SUBJ  \n",
       "24            [vase, the flower]        [vase, the flower]  NOT_MAIN_SUBJ  \n",
       "27              [you, the front]          [you, the front]  NOT_MAIN_SUBJ  \n",
       "40                  [boy, a cap]              [boy, a cap]  NOT_MAIN_SUBJ  \n",
       "48       [laptop wit dim screen]  [laptop, vibrant screen]  NOT_MAIN_SUBJ  \n",
       "...                          ...                       ...            ...  \n",
       "140514  [drinking water giraffe]                 [giraffe]  NOT_MAIN_SUBJ  \n",
       "140515           [giraffe, head]           [giraffe, head]  NOT_MAIN_SUBJ  \n",
       "140516                    [head]                    [head]  NOT_MAIN_SUBJ  \n",
       "140523                [man full]                        []  NOT_MAIN_SUBJ  \n",
       "140540         [giraffe, camera]         [giraffe, camera]  NOT_MAIN_SUBJ  \n",
       "\n",
       "[6145 rows x 16 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with pd.option_context(\n",
    "#     \"display.max_colwidth\", None, \"display.max_columns\", None, \"display.max_rows\", 200\n",
    "# ):\n",
    "#     display(df_changes[df_changes.diff_ops == (\"-\", \"+\")])\n",
    "\n",
    "# with pd.option_context(\n",
    "#     \"display.max_colwidth\", None, \"display.max_columns\", None, \"display.max_rows\", 200\n",
    "# ):\n",
    "#     display(df_changes[df_changes.num_changes == 1])\n",
    "\n",
    "\n",
    "with pd.option_context(\n",
    "    \"display.max_colwidth\", None, \"display.max_columns\", None, \"display.max_rows\", 200\n",
    "):\n",
    "    display(df_changes[df_changes.change_type == \"NOT_MAIN_SUBJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49856/49856 [00:00<00:00, 306550.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_sents: 282107, total_kept: 276126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def scrub(refcoco: COCO):\n",
    "    refcoco = deepcopy(refcoco)\n",
    "    total_sents = 0\n",
    "    total_sents_kept = 0\n",
    "    for ref_id, ref in tqdm(list(refcoco.refs.items()), total=len(refcoco.refs)):\n",
    "        sentences_new = []\n",
    "        total_sents += len(ref[\"sentences\"])\n",
    "        for s in ref[\"sentences\"]:\n",
    "            if \"is_false_premise\" not in s:\n",
    "                s[\"is_false_premise\"] = False\n",
    "            s[\"exist\"] = not s[\"is_false_premise\"]\n",
    "            if s[\"is_false_premise\"] and (s[\"sent\"] == s[\"gt_sent\"]):\n",
    "                continue\n",
    "            # if not s[\"change_type\"]:\n",
    "            #     s[\"change_type\"]\n",
    "            sentences_new.append(s)\n",
    "        total_sents_kept += len(sentences_new)\n",
    "        ref[\"sentences\"] = sentences_new\n",
    "    print(f\"total_sents: {total_sents}, total_kept: {total_sents_kept}\")\n",
    "    return refcoco\n",
    "\n",
    "\n",
    "refcoco_new_scrubbed = scrub(refcoco_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save new RefCOCO Dataset\n",
    "\n",
    "Enhanced version augments the `ref[\"sentences\"]` dictionaries with spacy tagging info (parts of speech, dependency parsing, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sent_ids': [2851, 2852], 'file_name': 'COCO_train2014_000000570878_0.jpg', 'ann_id': 65034, 'ref_id': 1000, 'image_id': 570878, 'split': 'train', 'sentences': [{'tokens': ['animal', 'looking', 'at', 'you', 'with', 'both', 'horns', 'visible'], 'raw': 'animal looking at you with both horns visible', 'sent_id': 2851, 'sent': 'animal looking at you with both horns visible', 'spcy_WORD': ['animal', 'looking', 'at', 'you', 'with', 'both', 'horns', 'visible'], 'spcy_DEP': ['nsubj', 'ROOT', 'prep', 'pobj', 'mark', 'det', 'nsubj', 'advcl'], 'spcy_POS': ['NOUN', 'VERB', 'ADP', 'PRON', 'SCONJ', 'DET', 'NOUN', 'ADJ'], 'spcy_LEM': ['animal', 'look', 'at', 'you', 'with', 'both', 'horn', 'visible'], 'spcy_TAG': ['NN', 'VBG', 'IN', 'PRP', 'IN', 'DT', 'NNS', 'JJ'], 'spcy_IS_STOP': [False, False, True, True, True, True, False, False], 'spcy_ENTS': [], 'spcy_NOUN_CHUNKS': ['animal', 'you', 'both horns'], 'is_false_premise': False, 'exist': True}, {'tokens': ['third', 'animal', 'looking', 'at', 'camera'], 'raw': '3rd animal looking at camera', 'sent_id': 2852, 'sent': 'third animal looking at camera', 'spcy_WORD': ['third', 'animal', 'looking', 'at', 'camera'], 'spcy_DEP': ['amod', 'nsubj', 'ROOT', 'prep', 'pobj'], 'spcy_POS': ['ADJ', 'NOUN', 'VERB', 'ADP', 'NOUN'], 'spcy_LEM': ['third', 'animal', 'look', 'at', 'camera'], 'spcy_TAG': ['JJ', 'NN', 'VBG', 'IN', 'NN'], 'spcy_IS_STOP': [True, False, False, True, False], 'spcy_ENTS': ['third'], 'spcy_NOUN_CHUNKS': ['third animal', 'camera'], 'is_false_premise': False, 'exist': True}, {'tokens': ['creature', 'looking', 'at', 'you', 'with', 'both', 'horns', 'visible'], 'raw': 'creature looking at you with both horns visible', 'sent_id': -1, 'sent': 'creature looking at you with both horns visible', 'spcy_WORD': ['creature', 'looking', 'at', 'you', 'with', 'both', 'horns', 'visible'], 'spcy_DEP': ['ROOT', 'acl', 'prep', 'pobj', 'mark', 'det', 'nsubj', 'advcl'], 'spcy_POS': ['NOUN', 'VERB', 'ADP', 'PRON', 'SCONJ', 'DET', 'NOUN', 'ADJ'], 'spcy_LEM': ['creature', 'look', 'at', 'you', 'with', 'both', 'horn', 'visible'], 'spcy_TAG': ['NN', 'VBG', 'IN', 'PRP', 'IN', 'DT', 'NNS', 'JJ'], 'spcy_IS_STOP': [False, False, True, True, True, True, False, False], 'spcy_ENTS': [], 'spcy_NOUN_CHUNKS': ['creature', 'you', 'both horns'], 'main_subject': ['horns'], 'ref_id': 1000, 'ann_id': 65034, 'gt_sent_id': 2851, 'gt_sent': 'animal looking at you with both horns visible', 'is_false_premise': True, 'change_type': 'main_subject', 'exist': False}, {'tokens': ['third', 'creature', 'looking', 'at', 'camera'], 'raw': 'third creature looking at camera', 'sent_id': -1, 'sent': 'third creature looking at camera', 'spcy_WORD': ['third', 'creature', 'looking', 'at', 'camera'], 'spcy_DEP': ['amod', 'ROOT', 'acl', 'prep', 'pobj'], 'spcy_POS': ['ADJ', 'NOUN', 'VERB', 'ADP', 'NOUN'], 'spcy_LEM': ['third', 'creature', 'look', 'at', 'camera'], 'spcy_TAG': ['JJ', 'NN', 'VBG', 'IN', 'NN'], 'spcy_IS_STOP': [True, False, False, True, False], 'spcy_ENTS': ['third'], 'spcy_NOUN_CHUNKS': ['third creature', 'camera'], 'main_subject': ['creature'], 'ref_id': 1000, 'ann_id': 65034, 'gt_sent_id': 2852, 'gt_sent': 'third animal looking at camera', 'is_false_premise': True, 'change_type': 'main_subject', 'exist': False}], 'category_id': 20}\n",
      "\n",
      "{'sent_ids': [2851, 2852], 'file_name': 'COCO_train2014_000000570878_0.jpg', 'ann_id': 65034, 'ref_id': 1000, 'image_id': 570878, 'split': 'train', 'sentences': [{'tokens': ['animal', 'looking', 'at', 'you', 'with', 'both', 'horns', 'visible'], 'raw': 'animal looking at you with both horns visible', 'sent_id': 2851, 'sent': 'animal looking at you with both horns visible', 'spcy_WORD': ['animal', 'looking', 'at', 'you', 'with', 'both', 'horns', 'visible'], 'spcy_DEP': ['nsubj', 'ROOT', 'prep', 'pobj', 'mark', 'det', 'nsubj', 'advcl'], 'spcy_POS': ['NOUN', 'VERB', 'ADP', 'PRON', 'SCONJ', 'DET', 'NOUN', 'ADJ'], 'spcy_LEM': ['animal', 'look', 'at', 'you', 'with', 'both', 'horn', 'visible'], 'spcy_TAG': ['NN', 'VBG', 'IN', 'PRP', 'IN', 'DT', 'NNS', 'JJ'], 'spcy_IS_STOP': [False, False, True, True, True, True, False, False], 'spcy_ENTS': [], 'spcy_NOUN_CHUNKS': ['animal', 'you', 'both horns'], 'is_false_premise': False, 'exist': True}, {'tokens': ['third', 'animal', 'looking', 'at', 'camera'], 'raw': '3rd animal looking at camera', 'sent_id': 2852, 'sent': 'third animal looking at camera', 'spcy_WORD': ['third', 'animal', 'looking', 'at', 'camera'], 'spcy_DEP': ['amod', 'nsubj', 'ROOT', 'prep', 'pobj'], 'spcy_POS': ['ADJ', 'NOUN', 'VERB', 'ADP', 'NOUN'], 'spcy_LEM': ['third', 'animal', 'look', 'at', 'camera'], 'spcy_TAG': ['JJ', 'NN', 'VBG', 'IN', 'NN'], 'spcy_IS_STOP': [True, False, False, True, False], 'spcy_ENTS': ['third'], 'spcy_NOUN_CHUNKS': ['third animal', 'camera'], 'is_false_premise': False, 'exist': True}, {'tokens': ['creature', 'looking', 'at', 'you', 'with', 'both', 'horns', 'visible'], 'raw': 'creature looking at you with both horns visible', 'sent_id': -1, 'sent': 'creature looking at you with both horns visible', 'spcy_WORD': ['creature', 'looking', 'at', 'you', 'with', 'both', 'horns', 'visible'], 'spcy_DEP': ['ROOT', 'acl', 'prep', 'pobj', 'mark', 'det', 'nsubj', 'advcl'], 'spcy_POS': ['NOUN', 'VERB', 'ADP', 'PRON', 'SCONJ', 'DET', 'NOUN', 'ADJ'], 'spcy_LEM': ['creature', 'look', 'at', 'you', 'with', 'both', 'horn', 'visible'], 'spcy_TAG': ['NN', 'VBG', 'IN', 'PRP', 'IN', 'DT', 'NNS', 'JJ'], 'spcy_IS_STOP': [False, False, True, True, True, True, False, False], 'spcy_ENTS': [], 'spcy_NOUN_CHUNKS': ['creature', 'you', 'both horns'], 'main_subject': ['horns'], 'ref_id': 1000, 'ann_id': 65034, 'gt_sent_id': 2851, 'gt_sent': 'animal looking at you with both horns visible', 'is_false_premise': True, 'change_type': 'main_subject', 'exist': False}, {'tokens': ['third', 'creature', 'looking', 'at', 'camera'], 'raw': 'third creature looking at camera', 'sent_id': -1, 'sent': 'third creature looking at camera', 'spcy_WORD': ['third', 'creature', 'looking', 'at', 'camera'], 'spcy_DEP': ['amod', 'ROOT', 'acl', 'prep', 'pobj'], 'spcy_POS': ['ADJ', 'NOUN', 'VERB', 'ADP', 'NOUN'], 'spcy_LEM': ['third', 'creature', 'look', 'at', 'camera'], 'spcy_TAG': ['JJ', 'NN', 'VBG', 'IN', 'NN'], 'spcy_IS_STOP': [True, False, False, True, False], 'spcy_ENTS': ['third'], 'spcy_NOUN_CHUNKS': ['third creature', 'camera'], 'main_subject': ['creature'], 'ref_id': 1000, 'ann_id': 65034, 'gt_sent_id': 2852, 'gt_sent': 'third animal looking at camera', 'is_false_premise': True, 'change_type': 'main_subject', 'exist': False}], 'category_id': 20}\n",
      "49856 49856\n"
     ]
    }
   ],
   "source": [
    "print(refcoco_new_scrubbed.refs[1000])\n",
    "print(\"\")\n",
    "print(refcoco_new_scrubbed.refs_data[1000])\n",
    "print(len(refcoco_new_scrubbed.refs_data), len(refcoco_new_scrubbed.refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving instances.json:  /home/gbiamby/proj/geo-llm-ret/output/refcoco+_unc-gb006_remove_guidelines-gpt-3.5-turbo/refer_seg/fprefcoco+_v002/instances.json\n",
      "Saving refs:  /home/gbiamby/proj/geo-llm-ret/output/refcoco+_unc-gb006_remove_guidelines-gpt-3.5-turbo/refer_seg/fprefcoco+_v002/refs(berkeley).p\n",
      "Saved new refer_seg dataset to:  /home/gbiamby/proj/geo-llm-ret/output/refcoco+_unc-gb006_remove_guidelines-gpt-3.5-turbo/refer_seg/fprefcoco+_v002\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "def save_refs(refcoco: COCO, save_dir: Path, split_by: str):\n",
    "    assert save_dir.exists(), str(save_dir)\n",
    "    refs_path = save_dir / f\"refs({split_by}).p\"\n",
    "    print(\"Saving refs: \", refs_path)\n",
    "    pickle.dump(refcoco.refs_data, open(refs_path, \"wb\"))\n",
    "\n",
    "\n",
    "def make_new_dataset(refcoco: COCO, save_dir: Path, dataset_name: str, split_by: str):\n",
    "    new_dataset_path = save_dir / \"refer_seg\" / f\"{dataset_name}\"\n",
    "    new_dataset_path.mkdir(exist_ok=True, parents=True)\n",
    "    # Copy coco instances.json:\n",
    "    source_path = refcoco.DATA_DIR / \"instances.json\"\n",
    "    assert source_path.exists(), str(source_path)\n",
    "    print(\"saving instances.json: \", new_dataset_path / \"instances.json\")\n",
    "    shutil.copy(source_path, new_dataset_path / \"instances.json\")\n",
    "    save_refs(refcoco, new_dataset_path, split_by)\n",
    "    print(\"Saved new refer_seg dataset to: \", new_dataset_path)\n",
    "    return new_dataset_path\n",
    "\n",
    "\n",
    "new_ds_path = make_new_dataset(\n",
    "    refcoco_new_scrubbed, api_results_dir, \"fprefcoco+_v002\", \"berkeley\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Inspect the Newly Saved RefCOCO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "============================================================================================================================================================================================================================\n",
      "Dataset: fprefcoco_v002(berkeley)\n",
      "Loading refs from '/home/gbiamby/proj/geo-llm-ret/output/refcoco_google-gb006_remove_guidelines-gpt-3.5-turbo/refer_seg/fprefcoco_v002/refs(berkeley).p'\n",
      "Loaded 50000 refs\n",
      "loading annotations into memory...\n",
      "Done (t=9.04s)\n",
      "creating index...\n",
      "index created!\n",
      "num images: 19994\n",
      "num annotations: 196771\n",
      "pos/neg sentence_counts:  142210 134720\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_sent_count</th>\n",
       "      <th>neg_sent_count</th>\n",
       "      <th>dataset</th>\n",
       "      <th>num_refs</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>total_pos_sents</th>\n",
       "      <th>total_neg_sents</th>\n",
       "      <th>ann_count</th>\n",
       "      <th>img_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>119</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>589</td>\n",
       "      <td>1767</td>\n",
       "      <td>1178</td>\n",
       "      <td>589</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>7451</td>\n",
       "      <td>29804</td>\n",
       "      <td>14902</td>\n",
       "      <td>14902</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>515</td>\n",
       "      <td>1545</td>\n",
       "      <td>1545</td>\n",
       "      <td>0</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>781</td>\n",
       "      <td>3124</td>\n",
       "      <td>2343</td>\n",
       "      <td>781</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>3442</td>\n",
       "      <td>17210</td>\n",
       "      <td>10326</td>\n",
       "      <td>6884</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>36735</td>\n",
       "      <td>220410</td>\n",
       "      <td>110205</td>\n",
       "      <td>110205</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>11</td>\n",
       "      <td>66</td>\n",
       "      <td>44</td>\n",
       "      <td>22</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>39</td>\n",
       "      <td>273</td>\n",
       "      <td>156</td>\n",
       "      <td>117</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>297</td>\n",
       "      <td>2376</td>\n",
       "      <td>1188</td>\n",
       "      <td>1188</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>fprefcoco_v002(berkeley)</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>196771</td>\n",
       "      <td>19994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pos_sent_count  neg_sent_count                   dataset  num_refs  \\\n",
       "0                1               1  fprefcoco_v002(berkeley)         2   \n",
       "1                2               0  fprefcoco_v002(berkeley)       119   \n",
       "2                2               1  fprefcoco_v002(berkeley)       589   \n",
       "3                2               2  fprefcoco_v002(berkeley)      7451   \n",
       "4                3               0  fprefcoco_v002(berkeley)       515   \n",
       "5                3               1  fprefcoco_v002(berkeley)       781   \n",
       "6                3               2  fprefcoco_v002(berkeley)      3442   \n",
       "7                3               3  fprefcoco_v002(berkeley)     36735   \n",
       "8                4               0  fprefcoco_v002(berkeley)         8   \n",
       "9                4               1  fprefcoco_v002(berkeley)         5   \n",
       "10               4               2  fprefcoco_v002(berkeley)        11   \n",
       "11               4               3  fprefcoco_v002(berkeley)        39   \n",
       "12               4               4  fprefcoco_v002(berkeley)       297   \n",
       "13               5               4  fprefcoco_v002(berkeley)         2   \n",
       "14               5               5  fprefcoco_v002(berkeley)         3   \n",
       "15               6               2  fprefcoco_v002(berkeley)         1   \n",
       "\n",
       "    sent_count  total_pos_sents  total_neg_sents  ann_count  img_count  \n",
       "0            4                2                2     196771      19994  \n",
       "1          238              238                0     196771      19994  \n",
       "2         1767             1178              589     196771      19994  \n",
       "3        29804            14902            14902     196771      19994  \n",
       "4         1545             1545                0     196771      19994  \n",
       "5         3124             2343              781     196771      19994  \n",
       "6        17210            10326             6884     196771      19994  \n",
       "7       220410           110205           110205     196771      19994  \n",
       "8           32               32                0     196771      19994  \n",
       "9           25               20                5     196771      19994  \n",
       "10          66               44               22     196771      19994  \n",
       "11         273              156              117     196771      19994  \n",
       "12        2376             1188             1188     196771      19994  \n",
       "13          18               10                8     196771      19994  \n",
       "14          30               15               15     196771      19994  \n",
       "15           8                6                2     196771      19994  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VALID_SPLITS = {\n",
    "    \"R-refcoco\": [\"unc\"],\n",
    "    \"R-refcoco+\": [\"unc\"],\n",
    "    \"R-refcocog\": [\"umd\"],\n",
    "    \"refclef\": [\"berkeley\", \"unc\"],\n",
    "    \"refcoco\": [\"google\"],\n",
    "    \"refcoco+\": [\"unc\"],\n",
    "    \"refcocog\": [\"google\", \"umd\"],\n",
    "    \"fprefcoco_v002\": [\"berkeley\"],\n",
    "    \"fprefcocog_v002\": [\"berkeley\"],\n",
    "}\n",
    "\n",
    "\n",
    "def build_refcoco(refseg_path: Path, dataset_name: str, split_by: str = None) -> COCO:\n",
    "    assert dataset_name in VALID_SPLITS, dataset_name\n",
    "    if split_by is None:\n",
    "        split_by = VALID_SPLITS[dataset_name][0]\n",
    "    else:\n",
    "        assert split_by in VALID_SPLITS[dataset_name]\n",
    "    coco = COCO(\n",
    "        refseg_path / dataset_name / \"instances.json\",\n",
    "        is_ref_dataset=True,\n",
    "        dataset_name=dataset_name,\n",
    "        split_by=split_by,\n",
    "    )\n",
    "    return coco\n",
    "\n",
    "\n",
    "df_aggs = []\n",
    "for ds_name in [\"fprefcoco_v002\"]:\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"=\" * 220)\n",
    "    print(f\"Dataset: {ds_name}(berkeley)\")\n",
    "    coconegref_stats = CocoClassDistHelper(\n",
    "        new_ds_path.parent,\n",
    "        is_ref_dataset=True,\n",
    "        dataset_name=ds_name,\n",
    "        split_by=\"berkeley\",\n",
    "    )\n",
    "    df_refcoco, df_refcoco_agg = coconegref_stats.get_ref_stats()\n",
    "    df_aggs.append(df_refcoco_agg)\n",
    "\n",
    "\n",
    "df_aggs = pd.concat(df_aggs)\n",
    "\n",
    "\n",
    "display(df_aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_refs</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>total_pos_sents</th>\n",
       "      <th>total_neg_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>50000</td>\n",
       "      <td>276930</td>\n",
       "      <td>142210</td>\n",
       "      <td>134720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      total_refs  sent_count  total_pos_sents  total_neg_sents\n",
       "True       50000      276930           142210           134720"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    df_aggs.groupby(lambda x: True).agg(\n",
    "        total_refs=(\"num_refs\", \"sum\"),\n",
    "        sent_count=(\"sent_count\", \"sum\"),\n",
    "        total_pos_sents=(\"total_pos_sents\", \"sum\"),\n",
    "        total_neg_sents=(\"total_neg_sents\", \"sum\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref has keys:  dict_keys(['sent_ids', 'file_name', 'ann_id', 'ref_id', 'image_id', 'split', 'sentences', 'category_id'])\n",
      "ref has 6 sentences\n",
      "sent_id:2836, is_FP:False, sent: 'chef in back'\n",
      "sent_id:2837, is_FP:False, sent: 'right guy'\n",
      "sent_id:2838, is_FP:False, sent: 'cook in the back'\n",
      "sent_id:-1, is_FP:True, sent: 'plumber in back'\n",
      "\tchange_type:  main_subject\n",
      "\tparent_sent_id: 2836, parent_sent: 'chef in back'\n",
      "sent_id:-1, is_FP:True, sent: 'right alien'\n",
      "\tchange_type:  main_subject\n",
      "\tparent_sent_id: 2837, parent_sent: 'right guy'\n",
      "sent_id:-1, is_FP:True, sent: 'engineer in the back'\n",
      "\tchange_type:  main_subject\n",
      "\tparent_sent_id: 2838, parent_sent: 'cook in the back'\n"
     ]
    }
   ],
   "source": [
    "def show_a_refexp(refcoco: COCO):\n",
    "    ref = refcoco.refs[1000]\n",
    "    print(\"ref has keys: \", ref.keys())\n",
    "    print(f\"ref has {len(ref['sentences'])} sentences\")\n",
    "    for s in ref[\"sentences\"]:\n",
    "        # print(s.keys())\n",
    "        print(\n",
    "            f\"sent_id:{s['sent_id']}, is_FP:{s['is_false_premise']}, sent: '{s['sent']}'\"\n",
    "        )\n",
    "        if s[\"is_false_premise\"]:\n",
    "            print(\"\\tchange_type: \", s[\"change_type\"])\n",
    "            print(f\"\\tparent_sent_id: {s['gt_sent_id']}, parent_sent: '{s['gt_sent']}'\")\n",
    "\n",
    "\n",
    "show_a_refexp(coconegref_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cocobetter",
   "language": "python",
   "name": "cocobetter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
