{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastjsonschema                    2.18.0\n",
      "json5                             0.9.14\n",
      "jsonpointer                       2.4\n",
      "jsons                             1.6.3\n",
      "jsonschema                        4.19.1\n",
      "jsonschema-specifications         2023.7.1\n",
      "pysimdjson                        5.0.2\n",
      "python-json-logger                2.0.7\n",
      "python-lsp-jsonrpc                1.1.1\n",
      "ujson                             5.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list | grep json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import csv\n",
    "import decimal\n",
    "import typing\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import simdjson as json\n",
    "from IPython.display import display\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.helpers import CocoClassDistHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 96M\n",
      "drwxr-xr-x 2 gbiamby users    5 Nov 20  2016  \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxr-xr-x 7 gbiamby users   16 Oct 23 21:25  \u001b[01;34m..\u001b[0m/\n",
      "-rw-r--r-- 1 gbiamby users 119M Feb 11  2016  instances.json\n",
      "-rw-r--r-- 1 gbiamby users  33M Feb 16  2016 'refs(google).p'\n",
      "-rw-r--r-- 1 gbiamby users  33M Nov 20  2016 'refs(umd).p'\n"
     ]
    }
   ],
   "source": [
    "%ls -lah \"/home/gbiamby/proj/geo-llm-ret/lib/neg_refer_llm/dataset/refer_seg/refcocog/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFSEG_DIR = Path(\"/home/gbiamby/proj/geo-llm-ret/lib/neg_refer_llm/dataset/refer_seg/\")\n",
    "# dataset_json = Path(REFSEG_DIR / \"R-refcocog/instances.json\")\n",
    "# coco_dist = CocoClassDistHelper(dataset_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### refcocog AKA G-Ref has two versions, sort of\n",
    "While we were collecting our dataset, we learned that Tamara Berg had independently applied her ReferIt game [27] to the MSCOCO dataset to generate expressions for 50,000 objects from 19,994 images. She kindly shared her data (named as UNC-Ref-COCO dataset) with us. For brevity, we call our Google Refexp dataset as G-Ref and the UNC-Ref-COCO as UNC-ref. We report results on both datasets in this paper. However, due to differences in our collection methodologies, we have found that the de- scriptions in the two overlapped datasets exhibit significant qualitative differences, with descriptions in the UNC-Ref dataset tending to be more concise and to contain less flow- ery language than our descriptions. 1 More specifically, the average lengths of expressions from our dataset and UNC- Ref are 8.43 and 3.61 respectively. And the size of the word dictionaries (keeping only words appearing more than 3 times) from our dataset and UNC-Ref are 4849 and 2890 respectively. See Figure 3 for some visual comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:,}\".format\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "\n",
    "def get_property_details(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        first_value = list(obj.values())[0]\n",
    "        if isinstance(first_value, list):\n",
    "            first_item = first_value[0]\n",
    "            if isinstance(first_item, dict):\n",
    "                return \"dict->list->dict:\" + str(first_item.keys())\n",
    "            else:\n",
    "                return (\n",
    "                    \"a: dict->list->\"\n",
    "                    + str(type(first_item))\n",
    "                    + \", lengths: \"\n",
    "                    + str([len(i) for i in first_item[:5]])\n",
    "                )\n",
    "        elif isinstance(first_value, dict):\n",
    "            second_value = list(first_value.values())[0]\n",
    "            if isinstance(second_value, list):\n",
    "                _list = second_value\n",
    "                if isinstance(_list[0], list):\n",
    "                    return (\n",
    "                        \"b: dict->dict->list[\"\n",
    "                        + str(type(_list[0]))\n",
    "                        + \"]\"\n",
    "                        + \", lengths: \"\n",
    "                        + str([len(i) for i in _list[:5]])\n",
    "                    )\n",
    "                else:\n",
    "                    return \"b: dict->dict->list[\" + str(type(_list[0])) + \"]\"\n",
    "\n",
    "            else:\n",
    "                return (\n",
    "                    \"b: dict->dict->[\"\n",
    "                    + str(type(second_value))\n",
    "                    + \"]\"\n",
    "                    + str(first_value.keys())\n",
    "                )\n",
    "        else:\n",
    "            return \"c: dict->UNKNOWN \" + str(type(first_value)) + \"]\"\n",
    "    elif isinstance(obj, list):\n",
    "        first_item = obj[0]\n",
    "        if isinstance(first_item, dict):\n",
    "            second_value = list(first_item.values())[0]\n",
    "            return (\n",
    "                \"d: list->dict\" + str(type(second_value)) + \"]\" + str(first_item.keys())\n",
    "            )\n",
    "        else:\n",
    "            return \"UNKNOWN\"\n",
    "\n",
    "\n",
    "def get_coco_df(coco: COCO) -> pd.DataFrame:\n",
    "    df_meta = pd.DataFrame(\n",
    "        {\n",
    "            \"property\": [\n",
    "                \"cats\",\n",
    "                \"images\",\n",
    "                \"anns\",\n",
    "                \"refs_data\",\n",
    "                \"refs\",\n",
    "                \"img_to_refs\",\n",
    "                \"cat_to_refs\",\n",
    "                \"ann_to_ref\",\n",
    "                \"ref_to_ann\",\n",
    "                \"sents\",\n",
    "                \"sent_to_ref\",\n",
    "                \"sent_to_tokens\",\n",
    "            ],\n",
    "            \"count\": [\n",
    "                len(coco.cats),\n",
    "                len(coco.imgs),\n",
    "                len(coco.anns),\n",
    "                len(coco.refs_data),\n",
    "                len(coco.refs),\n",
    "                len(coco.img_to_refs),\n",
    "                len(coco.cat_to_refs),\n",
    "                len(coco.ann_to_ref),\n",
    "                len(coco.ref_to_ann),\n",
    "                len(coco.sents),\n",
    "                len(coco.sent_to_ref),\n",
    "                len(coco.sent_to_tokens),\n",
    "            ],\n",
    "            \"python_type\": [\n",
    "                str(type(coco.cats)),\n",
    "                str(type(coco.imgs)),\n",
    "                str(type(coco.anns)),\n",
    "                str(type(coco.refs_data)),\n",
    "                str(type(coco.refs)),\n",
    "                str(type(coco.img_to_refs)),\n",
    "                str(type(coco.cat_to_refs)),\n",
    "                str(type(coco.ann_to_ref)),\n",
    "                str(type(coco.ref_to_ann)),\n",
    "                str(type(coco.sents)),\n",
    "                str(type(coco.sent_to_ref)),\n",
    "                str(type(coco.sent_to_tokens)),\n",
    "            ],\n",
    "            \"dict_keys\": [\n",
    "                get_property_details(coco.cats),\n",
    "                get_property_details(coco.imgs),\n",
    "                get_property_details(coco.anns),\n",
    "                get_property_details(coco.refs_data),\n",
    "                get_property_details(coco.refs),\n",
    "                get_property_details(coco.img_to_refs),\n",
    "                get_property_details(coco.cat_to_refs),\n",
    "                get_property_details(coco.ann_to_ref),\n",
    "                get_property_details(coco.ref_to_ann),\n",
    "                get_property_details(coco.sents),\n",
    "                get_property_details(coco.sent_to_ref),\n",
    "                get_property_details(coco.sent_to_tokens),\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    display(df_meta)\n",
    "    # display(df_meta.style.format(thousands=\",\"))\n",
    "\n",
    "\n",
    "# df_meta_ref = get_coco_df(ref_coco)\n",
    "# df_meta_rref = get_coco_df(rref_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sentence_counts(coco: COCO, L=3):\n",
    "    # print(\"Sentence counts for \", coco.)\n",
    "    sentence_counts = Counter()\n",
    "    counts = []\n",
    "    for idx, ref in enumerate(coco.refs_data):\n",
    "        sentences: list[dict] = ref[\"sentences\"]\n",
    "        count = len(sentences)\n",
    "        sentence_counts.update({count: 1})\n",
    "        counts.append(\n",
    "            {\n",
    "                \"ref_id\": ref[\"ref_id\"],\n",
    "                \"ann_id\": ref[\"ann_id\"],\n",
    "                \"category_id\": ref[\"category_id\"],\n",
    "                \"category\": coco.cats[ref[\"category_id\"]][\"name\"],\n",
    "                \"supercategory\": coco.cats[ref[\"category_id\"]][\"supercategory\"]\n",
    "                if \"supercategory\" in coco.cats[ref[\"category_id\"]]\n",
    "                else str(coco.cats[ref[\"category_id\"]]),\n",
    "                \"sent_count\": len(sentences),\n",
    "                \"pos_sent_count\": len(\n",
    "                    [\n",
    "                        s\n",
    "                        for s in sentences\n",
    "                        if (\"exist\" in s and s[\"exist\"]) or \"exist\" not in s\n",
    "                    ]\n",
    "                ),\n",
    "                \"neg_sent_count\": len(\n",
    "                    [s for s in sentences if (\"exist\" in s and not s[\"exist\"])]\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # print(sentence_counts, len(sentence_counts))\n",
    "    df = pd.DataFrame(counts)\n",
    "    # display(df)\n",
    "    print(\"pos/neg sentence_counts: \", df.pos_sent_count.sum(), df.neg_sent_count.sum())\n",
    "    df_agg = pd.DataFrame(\n",
    "        df.groupby(lambda x: True).agg(\n",
    "            num_refs=(\"ref_id\", \"count\"),\n",
    "            sent_count=(\"sent_count\", \"sum\"),\n",
    "            total_pos_sents=(\"pos_sent_count\", \"sum\"),\n",
    "            total_neg_sents=(\"neg_sent_count\", \"sum\"),\n",
    "        )\n",
    "    )\n",
    "    display(df_agg)\n",
    "    if L >= 1:\n",
    "        df_agg = pd.DataFrame(\n",
    "            df.groupby([\"pos_sent_count\"]).agg(\n",
    "                num_refs=(\"ref_id\", \"count\"),\n",
    "                sent_count=(\"sent_count\", \"sum\"),\n",
    "                total_pos_sents=(\"pos_sent_count\", \"sum\"),\n",
    "                total_neg_sents=(\"neg_sent_count\", \"sum\"),\n",
    "            )\n",
    "        )\n",
    "        display(df_agg)\n",
    "    if L >= 2:\n",
    "        df_agg = pd.DataFrame(\n",
    "            df.groupby([\"pos_sent_count\", \"neg_sent_count\"]).agg(\n",
    "                num_refs=(\"ref_id\", \"count\"),\n",
    "                sent_count=(\"sent_count\", \"sum\"),\n",
    "                total_pos_sents=(\"pos_sent_count\", \"sum\"),\n",
    "                total_neg_sents=(\"neg_sent_count\", \"sum\"),\n",
    "            )\n",
    "        )\n",
    "        display(df_agg)\n",
    "    # display(df_agg.droplevel(axis=0, level=0).reset_index(drop=True))\n",
    "    return df, df_agg.reset_index()\n",
    "\n",
    "\n",
    "# print(\"\\nrefcoco:\")\n",
    "# df_refcoco, ref_recoco_agg = show_sentence_counts(ref_coco)\n",
    "# print(\"\\nR-refcoco:\")\n",
    "# df_rrefcoco, df_rrefcoco_agg = show_sentence_counts(rref_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SPLITS = {\n",
    "    \"R-refcoco\": [\"unc\"],\n",
    "    \"R-refcoco+\": [\"unc\"],\n",
    "    \"R-refcocog\": [\"umd\"],\n",
    "    \"refclef\": [\"berkeley\", \"unc\"],\n",
    "    \"refcoco\": [\"google\"],\n",
    "    \"refcoco+\": [\"unc\"],\n",
    "    \"refcocog\": [\"google\", \"umd\"],\n",
    "    \"coconegref\": [\"berkeley\"],\n",
    "}\n",
    "\n",
    "\n",
    "def build_refcoco(refseg_path: Path, dataset_name: str, splitBy: str = None) -> COCO:\n",
    "    assert dataset_name in VALID_SPLITS, dataset_name\n",
    "    if splitBy is None:\n",
    "        splitBy = VALID_SPLITS[dataset_name][0]\n",
    "    else:\n",
    "        assert splitBy in VALID_SPLITS[dataset_name]\n",
    "    coco = COCO(\n",
    "        refseg_path / dataset_name / \"instances.json\",\n",
    "        is_ref_dataset=True,\n",
    "        dataset_name=dataset_name,\n",
    "        splitBy=splitBy,\n",
    "    )\n",
    "    return coco\n",
    "\n",
    "\n",
    "df_aggs = []\n",
    "for dataset_name, splits in VALID_SPLITS.items():\n",
    "    for split in splits:\n",
    "        print(\"\\n\\n\")\n",
    "        print(\"=\" * 220)\n",
    "        print(f\"Dataset: {dataset_name}({split})\")\n",
    "        ref_coco = build_refcoco(dataset_name, split)\n",
    "        df_meta = get_coco_df(ref_coco)\n",
    "        df_refcoco, df_refcoco_agg = show_sentence_counts(ref_coco, L=1)\n",
    "        df_refcoco_agg[\"dataset\"] = f\"{dataset_name}({split})\"\n",
    "        df_refcoco_agg[\"ann_count\"] = len(ref_coco.anns)\n",
    "        df_refcoco_agg[\"img_count\"] = len(ref_coco.imgs)\n",
    "        df_aggs.append(df_refcoco_agg)\n",
    "        \n",
    "df_aggs = pd.concat(df_aggs)\n",
    "# Make 'dataset' the first column:\n",
    "df_aggs.insert(0, \"dataset\", df_aggs.pop(\"dataset\"))\n",
    "\n",
    "display(df_aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_aggs.groupby(\"dataset\").agg(\n",
    "    num_refs=(\"num_refs\", \"sum\"),\n",
    "    sent_count=(\"sent_count\", \"sum\"),\n",
    "    total_pos_sents=(\"total_pos_sents\", \"sum\"),\n",
    "    total_neg_sents=(\"total_neg_sents\", \"sum\"),\n",
    "    total_ann_count=(\"ann_count\", \"min\"),\n",
    "    total_img_count=(\"img_count\", \"min\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from Levenshtein import distance, hamming\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "import spacy_transformers\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "\n",
    "def inspect_robust_ref(\n",
    "    num_pos: int, num_neg: int, coco: COCO, df: pd.DataFrame, ref_index=None\n",
    "):\n",
    "    print(\"=\" * 220)\n",
    "    df_ref_example = df[(df.pos_sent_count == num_pos) & (df.neg_sent_count == num_neg)]\n",
    "    if ref_index is None or ref_index < 0:\n",
    "        ref_index = int(np.random.uniform(0, len(df_ref_example)) // 1)\n",
    "        print(\"ref_index: \", ref_index)\n",
    "    display(pd.DataFrame(df_ref_example.iloc[ref_index]))\n",
    "    # print(type(df_ref_example))\n",
    "\n",
    "    example_ref = coco.refs[df_ref_example.iloc[ref_index][\"ref_id\"]]\n",
    "\n",
    "    # Show the category and ann:\n",
    "    ann = coco.ref_to_ann[example_ref[\"ref_id\"]]\n",
    "    print(\"ann: \", ann)\n",
    "    print(\"category: \", coco.cats[ann[\"category_id\"]])\n",
    "    # Show the ref:\n",
    "    ref_display = copy.deepcopy(example_ref)\n",
    "    del ref_display[\"sentences\"]\n",
    "    print(\"ref: \", ref_display)\n",
    "\n",
    "    # Show sentences:\n",
    "    pos_sents = [s for s in example_ref[\"sentences\"] if s[\"exist\"]]\n",
    "    neg_sents = [s for s in example_ref[\"sentences\"] if not s[\"exist\"]]\n",
    "    print(\"\\npositive sentences:\")\n",
    "    for s in pos_sents:\n",
    "        s = copy.deepcopy(s)\n",
    "        del s[\"tokens\"]\n",
    "        del s[\"raw\"]\n",
    "        print(s)\n",
    "        doc = nlp(s[\"sent\"])\n",
    "        print(\"noun chunks: \", list(doc.noun_chunks))\n",
    "        # print(doc.\n",
    "        spacy.displacy.render(doc, style=\"dep\")\n",
    "        spacy.displacy.render(doc, style=\"span\")\n",
    "\n",
    "    print(\"\\nnegative sentences:\")\n",
    "    for s in neg_sents:\n",
    "        s_display = copy.deepcopy(s)\n",
    "        num_tokens = len(s[\"tokens\"])\n",
    "        del s_display[\"tokens\"]\n",
    "        del s_display[\"raw\"]\n",
    "        print(\"neg_sent: \", s_display)\n",
    "        print(\"noun chunks: \", list(nlp(s[\"sent\"]).noun_chunks))\n",
    "        for ps in pos_sents:\n",
    "            distances = [distance(ps[\"tokens\"], s[\"tokens\"])]\n",
    "            print(\n",
    "                f\"\\tLevenstein distances from pos_sent '{ps['sent_id']}': \",\n",
    "                distances,\n",
    "                [d / num_tokens for d in distances],\n",
    "            )\n",
    "            distances = [hamming(ps[\"tokens\"], s[\"tokens\"])]\n",
    "            print(\n",
    "                f\"\\tHamming    distances from pos_sent '{ps['sent_id']}': \",\n",
    "                distances,\n",
    "                [d / num_tokens for d in distances],\n",
    "            )\n",
    "\n",
    "\n",
    "## Look at an example with 2 positive and 33 negative sentences, to make sure the above counts make sense:\n",
    "inspect_robust_ref(2, 10, rref_coco, df_rrefcoco)\n",
    "inspect_robust_ref(2, 33, rref_coco, df_rrefcoco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inspect_robust_ref(1, 11, rref_coco, df_rrefcoco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# % pip install spacy[cuda11x,transformers]\n",
    "# %python -m spacy download en_core_web_trf\n",
    "# Shouldn't need this since we added transformers in the first line:\n",
    "### %pip install spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cocobetter",
   "language": "python",
   "name": "cocobetter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
