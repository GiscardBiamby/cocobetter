{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neg_refcocov001.ipynb\n",
    "\n",
    "Create a COCO formatted dataset that uses a simplistic method to create false-premise referring expressions, along with correcting expressions. The method is to swap nouns with categories from a sibling class in the same COCO supercategory.\n",
    "\n",
    "The referring expressions follow same format as refcoco/refcocog/refcoco+/R-refcoco/etc, i.e., a coco formatted json file, accompanied by a file with a `.p` extension, which contains the referring expression. The `.p` file is a python pickle file. These datasets can be loaded using the common `refer.py`, or the `COCO` class in `github.com/GiscardBiamby/cocobetter.git`.\n",
    "\n",
    "### Assumptions:\n",
    "\n",
    "- Negative samples are what we call the false premise referring expressions. I.e., expressions that specify objects that don't exist in the image.\n",
    "- Each negative sample has a \"parent\", which is the possitive sample (a referring expression).\n",
    "- In this simplified dataset, the positive samples are just class names, and the negative samples are some other COCO class name that is swapped in for the positive one.\n",
    "- Positive/negative pairs are only created for unambiguous cases (ignoring noisy dataset issues, like unlabelled objects). This means we only create positive and negative referring expressions for \"human\" if the image contains exactly one human ground truth annotation. If there are two or more humans, we cannot easily create a referring expression that refers to one specific human, so we avoid trying to do that.\n",
    "\n",
    "### Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip list | grep json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import csv\n",
    "import decimal\n",
    "import json\n",
    "import os\n",
    "import typing\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Set, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as pil_img\n",
    "import seaborn as sns\n",
    "import simdjson as json\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO, Ann, Cat, Image, Ref\n",
    "from pycocotools.helpers import CocoClassDistHelper, CocoJsonBuilder\n",
    "from pycocotools.helpers.coco_builder import COCOShrinker\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_DIR = Path(\"/shared/gbiamby/data/coco\")\n",
    "IMG_DIR = COCO_DIR / \"val2017\"\n",
    "\n",
    "dataset_json = Path(COCO_DIR / \"annotations\" / \"instances_val2017.json\")\n",
    "coco_dist = CocoClassDistHelper(dataset_json)\n",
    "coco = COCO(dataset_json)\n",
    "for key in list(coco.anns.keys()):\n",
    "    ann = coco.anns[key]\n",
    "    if \"is_neg\" not in ann:\n",
    "        ann[\"is_neg\"] = False\n",
    "    cat = coco.cats[ann[\"category_id\"]]\n",
    "    ann[\"supercategory\"] = cat[\"supercategory\"]\n",
    "    ann[\"cat_name\"] = cat[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:,}\".format\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(coco.imgs.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(coco.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# spacy.prefer_gpu()\n",
    "# import spacy_transformers\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# doc = nlp(\"This is a sentence.\")\n",
    "# print(\"noun chunks: \", list(doc.noun_chunks))\n",
    "# print([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_img_info(img_dir: Path, img: dict):\n",
    "    img_path = img_dir / img[\"file_name\"]\n",
    "    img = deepcopy(img)\n",
    "    result = {\n",
    "        \"filename\": img_path.name,\n",
    "        \"suffix\": img_path.suffix,\n",
    "        \"img_dim\": np.asarray(pil_img.open(img_path).convert(\"L\")).shape,\n",
    "        \"image_height\": np.asarray(pil_img.open(img_path).convert(\"L\")).shape[0],\n",
    "        \"image_width\": np.asarray(pil_img.open(img_path).convert(\"L\")).shape[1],\n",
    "    }\n",
    "    result[\"area\"] = result[\"img_dim\"][0] * result[\"img_dim\"][1]\n",
    "    img.update(result)\n",
    "    return img\n",
    "\n",
    "\n",
    "print(f\"Found {len(coco.imgs)} images to process.\")\n",
    "tqdm._instances.clear()\n",
    "df_imgs = pd.DataFrame(\n",
    "    get_img_info(IMG_DIR, img) for img in tqdm(list(coco.imgs.values()))\n",
    ")\n",
    "display(df_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anns = pd.DataFrame(coco.anns.values()).drop(columns=[\"segmentation\", \"bbox\"])\n",
    "display(df_anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img_cat_counts = (\n",
    "    df_anns.groupby([\"image_id\", \"supercategory\", \"category_id\", \"cat_name\"])\n",
    "    .agg(total_anns=(\"id\", \"count\"))\n",
    "    .reset_index()\n",
    ")\n",
    "display(df_img_cat_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_imgs.merge(df_anns, how=\"inner\", left_on=\"id\", right_on=\"image_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_positive_cats = df_img_cat_counts[df_img_cat_counts.total_anns == 1].set_index(\n",
    "    [\"image_id\", \"category_id\"]\n",
    ")\n",
    "# df_positive_cats[\"]\n",
    "display(df_positive_cats)\n",
    "display(df_positive_cats.loc[581317])\n",
    "display(df_positive_cats.loc[581317, 77])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_dataframe_indexing():\n",
    "    # Get one cat when there are many:\n",
    "    cats = df_positive_cats.loc[581317]\n",
    "    display(cats)\n",
    "    print(len(cats))\n",
    "    print(cats.sample(n=1, replace=False))\n",
    "\n",
    "    # When there is one cat\n",
    "    cats = df_positive_cats.loc[581615]\n",
    "    display(cats)\n",
    "    print(len(cats))\n",
    "    print(cats.sample(n=1, replace=False))\n",
    "\n",
    "    # # image_id doesn't exist\n",
    "    # cats = df_positive_cats.loc[58131887]\n",
    "    # display(cats)\n",
    "\n",
    "    # Check when category doesn't exist but img does\n",
    "\n",
    "\n",
    "test_dataframe_indexing()\n",
    "# df_positive_cats[df_positive_cats.index[\"image_id\"]==581317]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_img2cats(df: pd.DataFrame) -> dict[int, dict[str, Any]]:\n",
    "    img_cats = df.reset_index().to_dict(orient=\"records\")\n",
    "    img2cats = defaultdict(dict)\n",
    "    for img_cat in img_cats:\n",
    "        img2cats[img_cat[\"image_id\"]][img_cat[\"category_id\"]] = img_cat\n",
    "\n",
    "    return img2cats\n",
    "\n",
    "\n",
    "cat_counts_pos_samples: dict = get_img2cats(df_positive_cats)\n",
    "cat_counts_all: dict = get_img2cats(df_img_cat_counts)\n",
    "print(len(cat_counts_pos_samples), len(df_img_cat_counts))\n",
    "\n",
    "print(cat_counts_pos_samples[581317])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sibling_lookup(\n",
    "    coco: COCO, img_cat_counts: dict[int, dict[int, dict]]\n",
    ") -> dict[int, set[int]]:\n",
    "    \"\"\"\n",
    "    Return a dictionary allowing lookup of the sibling categories, given a category_id.\n",
    "    \"\"\"\n",
    "    sup_to_children = defaultdict(set)\n",
    "    for cat_id, cat in coco.cats.items():\n",
    "        sup = cat[\"supercategory\"]\n",
    "        sup_to_children[sup].add(cat_id)\n",
    "    cat_to_siblings = defaultdict(set)\n",
    "    for cat_id, cat in coco.cats.items():\n",
    "        super_cat = cat[\"supercategory\"]\n",
    "        children = sup_to_children[super_cat]\n",
    "        # don't count current cat as it's own sibling:\n",
    "        siblings = children - set([cat_id])\n",
    "        if len(siblings) == 0:\n",
    "            print(\n",
    "                f\"Category {cat} has no siblings based on supercategory. Using all other classes as siblings instead.\"\n",
    "            )\n",
    "            siblings = {cat[\"id\"] for cat in coco.cats.values()} - set([cat_id])\n",
    "            print(\"\\tAjusted siblings: \", len(siblings))\n",
    "        cat_to_siblings[cat_id] = cat_to_siblings[cat_id].union(siblings)\n",
    "    return cat_to_siblings\n",
    "\n",
    "\n",
    "sibling_lookup = get_sibling_lookup(coco, cat_counts_all)\n",
    "print(\"Num sibling lookups: \", len(sibling_lookup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_sentence(\n",
    "    sentence_id: int,\n",
    "    cat_id: int,\n",
    "    coco: COCO,\n",
    "    ref: Ref,\n",
    "    exist: bool,\n",
    "    pos_sent: dict[str, Any] = None,\n",
    "    true_cat_id: int = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Add sentence, and sent_ids to the ref object. exist=True/False means it is a positive/negative sample, resp.\n",
    "    \"\"\"\n",
    "    cat = coco.cats[cat_id]\n",
    "    s = f\"The {cat['name'].lower()}\"\n",
    "    sent = {\n",
    "        \"tokens\": s.split(\" \"),\n",
    "        \"raw\": s,\n",
    "        \"sent_id\": sentence_id,\n",
    "        \"sent\": s,  # TODO: what exactly is the dif between \"raw\" and \"sent\"?\n",
    "        \"exist\": exist,\n",
    "    }\n",
    "    if exist == False:\n",
    "        assert pos_sent is not None\n",
    "        assert true_cat_id is not None\n",
    "        sent[\"source_sent\"] = pos_sent[\"sent_id\"]\n",
    "        sent[\"true_cat_id\"] = true_cat_id\n",
    "    ref[\"sent_ids\"].append(sent[\"sent_id\"])\n",
    "    ref[\"sentences\"].append(sent)\n",
    "    return sent\n",
    "\n",
    "\n",
    "def coco_negref(\n",
    "    args,\n",
    "    img_cat_counts_all: dict[int, dict[int, dict[str, Any]]],\n",
    "    img_cat_counts_pos_samples: dict[int, dict[int, dict[str, Any]]],\n",
    "    dataset_name: str,\n",
    "    split_by: str = \"berkeley\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate COCO with negated annotations.\n",
    "\n",
    "    Negated Anns are added to every image in the dataset. Negative classes are detected\n",
    "    as those not appearing in an image ground truth.\n",
    "    \"\"\"\n",
    "    np.random.seed(args.seed)\n",
    "    coco_annotations_file: Path = args.coco_ann_path\n",
    "    output_path: Path = args.output_path.absolute()\n",
    "    output_path.mkdir(exist_ok=True, parents=True)\n",
    "    assert coco_annotations_file.exists(), str(coco_annotations_file)\n",
    "\n",
    "    sibling_lookup = get_sibling_lookup(coco, img_cat_counts_all)\n",
    "    coco_original = COCO(str(coco_annotations_file))\n",
    "    coco_builder = CocoJsonBuilder(\n",
    "        coco_original.dataset[\"categories\"],\n",
    "        dest_path=output_path,\n",
    "        dest_name=f\"instances.json\",\n",
    "        source_coco=coco_original,\n",
    "        is_ref_dataset=True,\n",
    "        dataset_name=dataset_name,\n",
    "        split_by=split_by,\n",
    "    )\n",
    "    current_sent_id = 1\n",
    "\n",
    "    for i, (img_id, img) in tqdm(\n",
    "        enumerate(coco_original.imgs.items()), total=len(coco_original.imgs)\n",
    "    ):\n",
    "        annotations: list[Ann] = deepcopy(coco_original.imgToAnns[img_id])\n",
    "\n",
    "        # For each pos_candidate we will create some negative samples.\n",
    "        # each pos_candidate looks like this: {'image_id': 581317, 'category_id': 77, 'supercategory': 'electronic',\n",
    "        #     'cat_name': 'cell phone', 'total_anns': 1}\n",
    "        pos_candidates = []\n",
    "        # Sample the positive classes. Primarily this means classes with exactly\n",
    "        # one annotation in the image, therefore we can create unambiguous refering expressions for them:\n",
    "        img_cat_counts: list[dict[int, dict]] = list(\n",
    "            img_cat_counts_pos_samples[img_id].values()\n",
    "        )\n",
    "        if img_cat_counts is not None and len(img_cat_counts):\n",
    "            pos_candidates = np.random.choice(\n",
    "                img_cat_counts, args.num_pos_parents_per_image\n",
    "            ).tolist()\n",
    "        refs: list[Ref] = []\n",
    "        # All anns pass through to the new dataset (but won't be used unless a ref points to them).\n",
    "        # Each ref points to an ann_id. Each ref has a list of sentences.\n",
    "\n",
    "        ## What we're building looks like this:\n",
    "        # img: {\"id\": , \"height\": , ...}\n",
    "        #     anns: [{\"id\": , \"category_id\": , bbox: , \"segmentation\": , ...}]\n",
    "        #     refs: [{\"ref_id\": , \"ann_id\": , \"category_id\": , \"image_id\": , sent_ids: ,\n",
    "        #        \"sentences\": [\n",
    "        #             {\n",
    "        #                 'tokens': ['the', 'man', 'in', 'yellow', 'coat'],\n",
    "        #                 'raw': 'the man in yellow coat',\n",
    "        #                 'sent_id': 8,\n",
    "        #                 'sent': 'the man in yellow coat',  # TODO: what exactly is the dif between \"raw\" and \"sent\"?\n",
    "        #                 'exist': True,\n",
    "        #                 ...,\n",
    "        #             }\n",
    "        #     ]}, ...]\n",
    "\n",
    "        for pos_candidate in pos_candidates:\n",
    "            ann = next(ann for ann in annotations if ann[\"category_id\"] == pos_candidate[\"category_id\"])\n",
    "            assert ann is not None\n",
    "            ref = {\n",
    "                \"image_id\": img_id,\n",
    "                \"split\": \"val\",  # TODO: what to use for this, it is just train/val/test?\n",
    "                \"file_name\": img[\"file_name\"],\n",
    "                \"category_id\": ann[\"category_id\"],\n",
    "                \"ann_id\": ann[\"id\"],\n",
    "                \"sent_ids\": [],\n",
    "                \"ref_id\": -1,\n",
    "                \"sentences\": [],\n",
    "            }\n",
    "            pos_sent = add_sentence(\n",
    "                current_sent_id, ann[\"category_id\"], coco, ref, exist=True\n",
    "            )\n",
    "            current_sent_id += 1\n",
    "\n",
    "            # Sample candidate negative classes (siblings)\n",
    "            siblings = sibling_lookup[pos_candidate[\"category_id\"]]\n",
    "            if len(siblings) > 0:\n",
    "                neg_samples = set(\n",
    "                    np.random.choice(list(siblings), args.num_negs_per_pos)\n",
    "                )\n",
    "                for ns in neg_samples:\n",
    "                    neg_sent = add_sentence(\n",
    "                        current_sent_id,\n",
    "                        ann[\"category_id\"],\n",
    "                        coco,\n",
    "                        ref,\n",
    "                        exist=False,\n",
    "                        pos_sent=pos_sent,\n",
    "                        true_cat_id=ann[\"category_id\"],\n",
    "                    )\n",
    "                    current_sent_id += 1\n",
    "            else:\n",
    "                print(\"Empty siblings for cat: \", pos_candidate)\n",
    "            refs.append(ref)\n",
    "\n",
    "        # Add the negated annotations:\n",
    "        new_img: Image = img\n",
    "        coco_builder.add_image(new_img, annotations, refs)\n",
    "    neg_coco_path = coco_builder.save()\n",
    "\n",
    "    # Output a miniature version of the dataset file just for debugging/inspection:\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Building shrunken version\")\n",
    "    num_images = 50\n",
    "    mini_dataset_name = f\"{dataset_name}_mini\"\n",
    "    shrinker = COCOShrinker(\n",
    "        neg_coco_path,\n",
    "        is_ref_dataset=True,\n",
    "        split_by=split_by,\n",
    "        dataset_name=dataset_name,\n",
    "    )\n",
    "    shrink_path = Path(f\"./output/ref_seg/{mini_dataset_name}\")\n",
    "    shrinker.shrink(\n",
    "        \"instances.json\",\n",
    "        size=num_images,\n",
    "        output_dir=shrink_path,\n",
    "        is_ref_dataset=True,\n",
    "        dataset_name=mini_dataset_name,\n",
    "        split_by=\"berkeley\",\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_name = f\"refcoconeg_v001\"\n",
    "    args = argparse.Namespace(\n",
    "        **{\n",
    "            \"coco_ann_path\": dataset_json,\n",
    "            \"output_path\": Path(f\"./output/ref_seg/{dataset_name}\").resolve(),\n",
    "            \"num_pos_parents_per_image\": 1,\n",
    "            \"num_negs_per_pos\": 5,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "    )\n",
    "    print(args)\n",
    "    coco_negref(\n",
    "        args,\n",
    "        cat_counts_all,\n",
    "        cat_counts_pos_samples,\n",
    "        dataset_name=dataset_name,\n",
    "        split_by=\"berkeley\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Creating the new CocoNegRef Dataset, and Output Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VALID_SPLITS = {\n",
    "    \"R-refcoco\": [\"unc\"],\n",
    "    \"R-refcoco+\": [\"unc\"],\n",
    "    \"R-refcocog\": [\"umd\"],\n",
    "    \"refclef\": [\"berkeley\", \"unc\"],\n",
    "    \"refcoco\": [\"google\"],\n",
    "    \"refcoco+\": [\"unc\"],\n",
    "    \"refcocog\": [\"google\", \"umd\"],\n",
    "    \"refcoconeg_v001\": [\"berkeley\"],\n",
    "    \"refcoconeg_v001_mini\": [\"berkeley\"],\n",
    "}\n",
    "\n",
    "\n",
    "def build_refcoco(refseg_path: Path, dataset_name: str, split_by: str = None) -> COCO:\n",
    "    assert dataset_name in VALID_SPLITS, dataset_name\n",
    "    if split_by is None:\n",
    "        split_by = VALID_SPLITS[dataset_name][0]\n",
    "    else:\n",
    "        assert split_by in VALID_SPLITS[dataset_name]\n",
    "    coco = COCO(\n",
    "        refseg_path / dataset_name / \"instances.json\",\n",
    "        is_ref_dataset=True,\n",
    "        dataset_name=dataset_name,\n",
    "        split_by=split_by,\n",
    "    )\n",
    "    return coco\n",
    "\n",
    "\n",
    "coconegref = build_refcoco(\n",
    "    Path(f\"./output/ref_seg/\").resolve(), dataset_name, split_by=\"berkeley\"\n",
    ")\n",
    "coconegref_stats = CocoClassDistHelper(\n",
    "    Path(f\"./output/ref_seg\").resolve(),\n",
    "    is_ref_dataset=True,\n",
    "    dataset_name=dataset_name,\n",
    "    split_by=\"berkeley\",\n",
    ")\n",
    "df, df_agg = coconegref_stats.get_ref_stats()\n",
    "# display(df)\n",
    "display(df_agg)\n",
    "\n",
    "df_agg_2 = df_agg.groupby(lambda x: True).agg(\n",
    "    num_refs=(\"num_refs\", \"sum\"),\n",
    "    sent_count=(\"sent_count\", \"sum\"),\n",
    "    total_pos_sents=(\"total_pos_sents\", \"sum\"),\n",
    "    total_neg_sents=(\"total_neg_sents\", \"sum\"),\n",
    ")\n",
    "display(df_agg_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "print(df.describe())\n",
    "with pd.option_context(\"display.max_rows\", 100, \"display.max_columns\", 10):\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anns = {ann[\"id\"] for ann in coco.anns.values()}\n",
    "print(min(anns), max(anns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "refs = pickle.load(\n",
    "    open(\n",
    "        \"/home/gbiamby/proj/geo-llm-ret/lib/cocobetter/PythonAPI/notebooks/ref_correct/output/ref_seg/refcoconeg_v001/refs(berkeley).p\",\n",
    "        \"rb\",\n",
    "    )\n",
    ")\n",
    "for idx, ref in enumerate(reversed(refs)):\n",
    "    if idx > 1:\n",
    "        break\n",
    "    # print(ref[\"category_id\"], ref[\"ann_id\"])\n",
    "    print(ref)\n",
    "    # if ref[\"sent_ids\"]:\n",
    "    #     print(ref[\"sent_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(list(coco.imgs.values())[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = (\n",
    "    pd.DataFrame(list(coco_dist.get_cat_counts().values()))\n",
    "    .sort_values(\"ann_count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "total_anns = df_counts.ann_count.sum()\n",
    "df_counts[\"ann_count_pdf\"] = df_counts.ann_count / total_anns\n",
    "df_counts[\"ann_count_cdf\"] = df_counts.ann_count_pdf.cumsum()\n",
    "display(df_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add frequency bins based on annotation count Cumulative Distribution Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts[\"freq_bin_2\"] = df_counts.ann_count_cdf.apply(\n",
    "    lambda x: \"high\" if x < 0.5 else \"low\"\n",
    ")\n",
    "df_counts[\"freq_bin_3\"] = df_counts.ann_count_cdf.apply(\n",
    "    lambda x: \"high\" if x < 0.333 else \"medium\" if x <= 0.667 else \"low\"\n",
    ")\n",
    "display(df_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 15))\n",
    "color_map = plt.get_cmap(\"magma\")\n",
    "fig = sns.barplot(\n",
    "    data=df_counts.sort_values([\"img_count\"], ascending=False),\n",
    "    x=\"name\",\n",
    "    y=\"img_count\",\n",
    "    hue=df_counts.freq_bin_3.values,\n",
    ")\n",
    "fig.set_xticklabels(fig.get_xticklabels(), rotation=45, horizontalalignment=\"right\")\n",
    "fig.set_title(f\"Per-category Annotated Image Counts\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 15))\n",
    "color_map = plt.get_cmap(\"magma\")\n",
    "fig = sns.barplot(\n",
    "    data=df_counts.sort_values([\"ann_count\"], ascending=False),\n",
    "    x=\"name\",\n",
    "    y=\"ann_count\",\n",
    "    hue=\"freq_bin_3\",\n",
    ")\n",
    "fig.set_xticklabels(fig.get_xticklabels(), rotation=45, horizontalalignment=\"right\")\n",
    "fig.set_title(f\"Per-category Annotation Counts\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cocobetter",
   "language": "python",
   "name": "cocobetter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
